{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание нейронной сети без использования готовых решений\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "PqC4R7SGseKa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [],
   "source": [
    "torch.set_warn_always(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "## 1. Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f4agkY9WqPwe"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weights: torch.Tensor, bias: torch.Tensor):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(inputs * self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HJRkSkHHsb7u"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = torch.tensor(3.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron.forward: 4.840000152587891\n",
      "functional.linear: 4.840000152587891\n"
     ]
    }
   ],
   "source": [
    "neuron = Neuron(weights, bias)\n",
    "print(f'Neuron.forward: {neuron.forward(inputs)}')\n",
    "print(f'functional.linear: {F.linear(inputs, weights, bias)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJvnwiyty37"
   },
   "source": [
    "1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fVWF3a9vtx90"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, weights: torch.Tensor, biases: torch.Tensor):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(inputs, self.weights.T) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Fo-JFnHPuFCS"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]])  # убрал .T\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear.forward:\n",
      "tensor([ 4.8400,  0.1700, 10.3900])\n",
      "\n",
      "functional.linear:\n",
      "tensor([ 4.8400,  0.1700, 10.3900])\n"
     ]
    }
   ],
   "source": [
    "m = Linear(weights, biases)\n",
    "print(f'Linear.forward:\\n{m.forward(inputs)}')\n",
    "print(f'\\nfunctional.linear:\\n{F.linear(inputs, weights, biases)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtsJzcxuyGd"
   },
   "source": [
    "1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Z8IizmtsuhO1"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear.forward:\n",
      "tensor([[ 3.7900,  0.9200,  9.0850],\n",
      "        [ 6.1400, -2.1000,  6.9000],\n",
      "        [ 2.0400,  0.7610,  6.7260]])\n",
      "\n",
      "functional.linear:\n",
      "tensor([[ 3.7900,  0.9200,  9.0850],\n",
      "        [ 6.1400, -2.1000,  6.9000],\n",
      "        [ 2.0400,  0.7610,  6.7260]])\n"
     ]
    }
   ],
   "source": [
    "m = Linear(weights, biases)\n",
    "print(f'Linear.forward:\\n{m.forward(inputs)}')\n",
    "print(f'\\nfunctional.linear:\\n{F.linear(inputs, weights, biases)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ2OxH4_vBLu"
   },
   "source": [
    "1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "IOv52EdovASs"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = torch.randn(out_features, in_features, requires_grad=True)\n",
    "        self.biases = torch.randn(out_features, requires_grad=True)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(inputs, self.weights.T) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -5.0178,   0.5240,  -3.9319],\n        [  4.0738,  -6.7887,  -3.5657],\n        [-11.6052,  -0.3882,  -3.1959]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "m = Linear(4, 3)\n",
    "m.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -5.0178,   0.5240,  -3.9319],\n        [  4.0738,  -6.7887,  -3.5657],\n        [-11.6052,  -0.3882,  -3.1959]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка\n",
    "nn_m = nn.Linear(4, 3)\n",
    "nn_m.weight = nn.Parameter(m.weights)\n",
    "nn_m.bias = nn.Parameter(m.biases)\n",
    "nn_m.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPG4UqL4wajI"
   },
   "source": [
    "1.5 Используя решение из __1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input = Linear(4, 3)\n",
    "        self.output = Linear(3, 7)\n",
    "\n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        return self.output.forward(self.input.forward(inputs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  5.1553,   1.1916,  -4.7817,   3.6611,  -9.6546,   0.1692,   5.8099],\n        [ -0.8347,   0.0913,   3.1227,   3.5542,   5.4400,   4.9157,   8.2533],\n        [  8.8057,  -1.1342,  -7.8488,   6.7415, -11.1044,   5.1500,   5.6152]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "m = NeuralNet()\n",
    "m.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "## 2. Создание функций активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def randn(*size: int, seed: int = 0) -> torch.Tensor:\n",
    "    torch.manual_seed(seed)\n",
    "    return torch.randn(*size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.maximum(inputs, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.5410, -0.2934, -2.1788],\n        [ 0.5684, -1.0845, -1.3986],\n        [ 0.4033,  0.8380, -0.7193],\n        [-0.4033, -0.5966,  0.1820]])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = randn(4, 3)\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU:\n",
      "tensor([[1.5410, 0.0000, 0.0000],\n",
      "        [0.5684, 0.0000, 0.0000],\n",
      "        [0.4033, 0.8380, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1820]])\n",
      "\n",
      "nn.ReLU:\n",
      "tensor([[1.5410, 0.0000, 0.0000],\n",
      "        [0.5684, 0.0000, 0.0000],\n",
      "        [0.4033, 0.8380, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1820]])\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "nn_relu = nn.ReLU()\n",
    "print(f'ReLU:\\n{relu.forward(inputs)}')\n",
    "print(f'\\nnn.ReLU:\\n{nn_relu.forward(inputs)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def __init__(self, dim: int = 0):\n",
    "        assert dim == 0 or dim == 1\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        exp = torch.exp(inputs)\n",
    "        return exp / torch.sum(exp, dim=self.dim).unsqueeze(self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.5410, -0.2934, -2.1788],\n        [ 0.5684, -1.0845, -1.3986],\n        [ 0.4033,  0.8380, -0.7193],\n        [-0.4033, -0.5966,  0.1820]])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = randn(4, 3)\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:\n",
      "tensor([[0.8446, 0.1349, 0.0205],\n",
      "        [0.7511, 0.1438, 0.1051],\n",
      "        [0.3484, 0.5382, 0.1134],\n",
      "        [0.2762, 0.2277, 0.4961]])\n",
      "\n",
      "nn.Softmax:\n",
      "tensor([[0.8446, 0.1349, 0.0205],\n",
      "        [0.7511, 0.1438, 0.1051],\n",
      "        [0.3484, 0.5382, 0.1134],\n",
      "        [0.2762, 0.2277, 0.4961]])\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax(dim=1)\n",
    "nn_softmax = nn.Softmax(dim=1)\n",
    "print(f'Softmax:\\n{softmax.forward(inputs)}')\n",
    "print(f'\\nnn.Softmax:\\n{nn_softmax.forward(inputs)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "NzMz7HDLySxK"
   },
   "outputs": [],
   "source": [
    "class ELU:\n",
    "\n",
    "    def __init__(self, alpha: float = 1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(inputs > 0, inputs, self.alpha * (torch.exp(inputs) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.5410, -0.2934, -2.1788],\n        [ 0.5684, -1.0845, -1.3986],\n        [ 0.4033,  0.8380, -0.7193],\n        [-0.4033, -0.5966,  0.1820]])"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = randn(4, 3)\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELU:\n",
      "tensor([[ 1.5410, -0.3583, -1.2495],\n",
      "        [ 0.5684, -0.9327, -1.0611],\n",
      "        [ 0.4033,  0.8380, -0.7227],\n",
      "        [-0.4677, -0.6331,  0.1820]])\n",
      "\n",
      "nn.ELU:\n",
      "tensor([[ 1.5410, -0.3583, -1.2495],\n",
      "        [ 0.5684, -0.9327, -1.0611],\n",
      "        [ 0.4033,  0.8380, -0.7227],\n",
      "        [-0.4677, -0.6331,  0.1820]])\n"
     ]
    }
   ],
   "source": [
    "elu = ELU(alpha=1.409)\n",
    "nn_elu = nn.ELU(alpha=1.409)\n",
    "print(f'ELU:\\n{elu.forward(inputs)}')\n",
    "print(f'\\nnn.ELU:\\n{nn_elu.forward(inputs)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "## 3. Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "layer = Linear(4, 1)\n",
    "y_pred = layer.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss:\n",
      "101.30004119873047\n",
      "\n",
      "nn.MSELoss:\n",
      "101.30004119873047\n"
     ]
    }
   ],
   "source": [
    "mse = MSELoss()\n",
    "nn_mse = nn.MSELoss()\n",
    "print(f'MSELoss:\\n{mse.forward(y_pred, y)}')\n",
    "print(f'\\nnn.MSELoss:\\n{nn_mse.forward(y_pred, y)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return -torch.sum(y_true * torch.log(y_pred), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "s7Qoupfo1ZGJ"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "layer = Linear(4, 3)\n",
    "softmax = Softmax()\n",
    "y_pred = softmax.forward(layer.forward(inputs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([9.0918e+00, 1.1272e-04, 1.5679e+01])"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce = CategoricalCrossEntropyLoss()\n",
    "cce.forward(y_pred, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "class MSELossL2:\n",
    "\n",
    "    def __init__(self, lambda_: float, weights: torch.Tensor):\n",
    "        self.lambda_ = lambda_\n",
    "        self.weights = weights\n",
    "\n",
    "    def data_loss(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "    def reg_loss(self) -> torch.Tensor:\n",
    "        return self.lambda_ * torch.sum(self.weights ** 2)\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return self.data_loss(y_pred, y_true) + self.reg_loss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4]).unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "layer = Linear(4, 1)\n",
    "y_pred = layer.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(314.5113)"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_l2 = MSELossL2(lambda_=1.409, weights=layer.weights)\n",
    "mse_l2.forward(y_pred, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Обратное распространение ошибки"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
    "# X.dtype == float64 - нет смысла использовать torch.from_numpy\n",
    "# т.к. последующее приведение типов Tensor.type(torch.float32) приведет к копированию данных\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        e = y_pred - y_true\n",
    "        return 2 * e  # df/dc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "L0KqxPJU9kAN"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.weights = torch.randn(self.in_features)\n",
    "        self.bias = torch.randn(1)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(inputs * self.weights) + self.bias\n",
    "\n",
    "    def backward(self, dvalue: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dweights = dvalue * 1  # df/dw\n",
    "        dbias = dvalue * 1  # df/db\n",
    "        dinputs = dbias * self.weights  # df/dx зачем?\n",
    "        return dweights, dbias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM"
   },
   "source": [
    "4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "id": "_g_FvwvmALJd"
   },
   "outputs": [],
   "source": [
    "n_inputs = X.size(1)  # размерность элемента выборки\n",
    "learning_rate = 0.1  #  скорость обучения\n",
    "n_epoch = 100  #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epoch):\n",
    "    for x, y_true in zip(X, y):  # а это точно нужно делать во внутреннем цикле?\n",
    "        # forward pass\n",
    "        y_pred = neuron.forward(x)\n",
    "        curr_loss = loss.forward(y_pred, y_true)\n",
    "        losses.append(curr_loss)\n",
    "\n",
    "        # backprop\n",
    "        dw, db = neuron.backward(loss.backward(y_pred, y_true))\n",
    "\n",
    "        neuron.weights -= learning_rate * dw\n",
    "        neuron.bias -= learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "data": {
      "text/plain": "([tensor(13452.7949),\n  tensor(3068.4236),\n  tensor(2568.9526),\n  tensor(7889.0342),\n  tensor(2024.4634)],\n [tensor(23572.7812),\n  tensor(38770.8945),\n  tensor(66.2022),\n  tensor(16.8464),\n  tensor(786.2955)],\n tensor([38.4900, 34.9740, 35.9676, 36.3822]),\n array([75.61322997,  0.70372677, 91.67887498, 43.2366121 ]))"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# работает как часы 🕓... (нет)\n",
    "losses[:5], losses[-5:], neuron.weights, coef"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:\n",
      "\tdw = tensor([[-0.7804, -0.1373,  0.7393,  0.0561]])\n",
      "\tw = tensor([[-0.4865, -0.1190,  0.8135, -0.0180]], requires_grad=True)\n",
      "\tloss = 14425.232421875\n",
      "\n",
      "epoch 20:\n",
      "\tdw = tensor([[-0.1754, -0.0499,  0.3540, -0.0156]])\n",
      "\tw = tensor([[-0.1014, -0.0342,  0.2397, -0.0151]], requires_grad=True)\n",
      "\tloss = 14421.5244140625\n",
      "\n",
      "epoch 30:\n",
      "\tdw = tensor([[-0.0354, -0.0130,  0.0902, -0.0064]])\n",
      "\tw = tensor([[-0.0206, -0.0080,  0.0564, -0.0045]], requires_grad=True)\n",
      "\tloss = 14421.390625\n",
      "\n",
      "epoch 40:\n",
      "\tdw = tensor([[-0.0072, -0.0029,  0.0205, -0.0017]])\n",
      "\tw = tensor([[-0.0043, -0.0017,  0.0125, -0.0011]], requires_grad=True)\n",
      "\tloss = 14421.3857421875\n",
      "\n",
      "epoch 50:\n",
      "\tdw = tensor([[-0.0015, -0.0006,  0.0045, -0.0004]])\n",
      "\tw = tensor([[-0.0009, -0.0004,  0.0027, -0.0003]], requires_grad=True)\n",
      "\tloss = 14421.3857421875\n",
      "\n",
      "epoch 60:\n",
      "\tdw = tensor([[-3.1723e-04, -1.2780e-04,  9.7968e-04, -9.0900e-05]])\n",
      "\tw = tensor([[-1.8959e-04, -7.4979e-05,  5.9280e-04, -5.5773e-05]],\n",
      "       requires_grad=True)\n",
      "\tloss = 14421.3837890625\n",
      "\n",
      "epoch 70:\n",
      "\tdw = tensor([[-6.7708e-05, -2.5872e-05,  2.1273e-04, -2.0411e-05]])\n",
      "\tw = tensor([[-4.0413e-05, -1.5615e-05,  1.2835e-04, -1.2236e-05]],\n",
      "       requires_grad=True)\n",
      "\tloss = 14421.3837890625\n",
      "\n",
      "epoch 80:\n",
      "\tdw = tensor([[-1.4980e-05, -5.4001e-06,  4.6206e-05, -4.2587e-06]])\n",
      "\tw = tensor([[-8.4702e-06, -3.3424e-06,  2.7489e-05, -2.6136e-06]],\n",
      "       requires_grad=True)\n",
      "\tloss = 14421.3837890625\n",
      "\n",
      "epoch 90:\n",
      "\tdw = tensor([[-2.5428e-06, -1.3350e-06,  9.7149e-06, -6.7496e-07]])\n",
      "\tw = tensor([[-1.8694e-06, -7.1293e-07,  5.6730e-06, -5.9658e-07]],\n",
      "       requires_grad=True)\n",
      "\tloss = 14421.3837890625\n",
      "\n",
      "epoch 100:\n",
      "\tdw = tensor([[-7.2263e-07, -2.5372e-07,  2.0077e-06, -2.2329e-07]])\n",
      "\tw = tensor([[-4.5288e-07, -4.5142e-08,  1.2632e-06, -1.0822e-07]],\n",
      "       requires_grad=True)\n",
      "\tloss = 14421.3837890625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1  # скорость обучения\n",
    "n_epoch = 100  # количество эпох\n",
    "\n",
    "layer = Linear(X.size(1), 1)\n",
    "mse = MSELoss()\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    y_pred = layer.forward(X)\n",
    "    loss = mse.forward(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        layer.weights -= learning_rate * layer.weights.grad\n",
    "        layer.biases -= learning_rate * layer.biases.grad\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch}:\\n\\tdw = {layer.weights.grad}\\n\\tw = {layer.weights}\\n\\tloss = {loss}\\n')\n",
    "\n",
    "    layer.weights.grad.zero_()\n",
    "    layer.biases.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:\n",
      "\tdw = tensor([[-116.7529,   20.9540, -126.0042,  -52.0535]])\n",
      "\tdb = tensor([-17.5067])\n",
      "\tw = tensor([[19.0080, -2.5065, 22.0046,  8.7914]], requires_grad=True)\n",
      "\tb = tensor([2.2568], requires_grad=True)\n",
      "\tloss = 8773.7451171875\n",
      "\n",
      "epoch 20:\n",
      "\tdw = tensor([[-86.7476,  10.1422, -97.0013, -42.5946]])\n",
      "\tdb = tensor([-9.5114])\n",
      "\tw = tensor([[33.0256, -4.5637, 37.4192, 15.3730]], requires_grad=True)\n",
      "\tb = tensor([4.0662], requires_grad=True)\n",
      "\tloss = 5162.01171875\n",
      "\n",
      "epoch 30:\n",
      "\tdw = tensor([[-64.6843,   3.4048, -74.8942, -34.7179]])\n",
      "\tdb = tensor([-4.2518])\n",
      "\tw = tensor([[43.4603, -5.4323, 49.3041, 20.7471]], requires_grad=True)\n",
      "\tb = tensor([4.9724], requires_grad=True)\n",
      "\tloss = 3074.840576171875\n",
      "\n",
      "epoch 40:\n",
      "\tdw = tensor([[-48.4055,  -0.6268, -57.9959, -28.2039]])\n",
      "\tdb = tensor([-0.8927])\n",
      "\tw = tensor([[51.2556, -5.5744, 58.4948, 25.1195]], requires_grad=True)\n",
      "\tb = tensor([5.2929], requires_grad=True)\n",
      "\tloss = 1852.9224853515625\n",
      "\n",
      "epoch 50:\n",
      "\tdw = tensor([[-36.3526,  -2.8867, -45.0419, -22.8474]])\n",
      "\tdb = tensor([1.1613])\n",
      "\tw = tensor([[57.1001, -5.2945, 65.6228, 28.6660]], requires_grad=True)\n",
      "\tb = tensor([5.2471], requires_grad=True)\n",
      "\tloss = 1128.52099609375\n",
      "\n",
      "epoch 60:\n",
      "\tdw = tensor([[-27.3970,  -4.0081, -35.0824, -18.4633]])\n",
      "\tdb = tensor([2.3320])\n",
      "\tw = tensor([[61.4975, -4.7905, 71.1671, 31.5351]], requires_grad=True)\n",
      "\tb = tensor([4.9845], requires_grad=True)\n",
      "\tloss = 693.9403686523438\n",
      "\n",
      "epoch 70:\n",
      "\tdw = tensor([[-20.7189,  -4.4167, -27.4024, -14.8894]])\n",
      "\tdb = tensor([2.9160])\n",
      "\tw = tensor([[64.8176, -4.1877, 75.4920, 33.8511]], requires_grad=True)\n",
      "\tb = tensor([4.6052], requires_grad=True)\n",
      "\tloss = 430.3497619628906\n",
      "\n",
      "epoch 80:\n",
      "\tdw = tensor([[-15.7214,  -4.3960, -21.4627, -11.9857]])\n",
      "\tdb = tensor([3.1202])\n",
      "\tw = tensor([[67.3329, -3.5632, 78.8751, 35.7169]], requires_grad=True)\n",
      "\tb = tensor([4.1750], requires_grad=True)\n",
      "\tloss = 268.87091064453125\n",
      "\n",
      "epoch 90:\n",
      "\tdw = tensor([[-11.9681,  -4.1319, -16.8553,  -9.6332]])\n",
      "\tdb = tensor([3.0872])\n",
      "\tw = tensor([[69.2448, -2.9623, 81.5286, 37.2176]], requires_grad=True)\n",
      "\tb = tensor([3.7358], requires_grad=True)\n",
      "\tloss = 169.0634002685547\n",
      "\n",
      "epoch 100:\n",
      "\tdw = tensor([[ -9.1394,  -3.7442, -13.2710,  -7.7319]])\n",
      "\tdb = tensor([2.9142])\n",
      "\tw = tensor([[70.7027, -2.4092, 83.6154, 38.4229]], requires_grad=True)\n",
      "\tb = tensor([3.3131], requires_grad=True)\n",
      "\tloss = 106.89018249511719\n",
      "\n",
      "epoch 110:\n",
      "\tdw = tensor([[ -7.0001,  -3.3084, -10.4745,  -6.1986]])\n",
      "\tdb = tensor([2.6660])\n",
      "\tw = tensor([[71.8178, -1.9152, 85.2605, 39.3896]], requires_grad=True)\n",
      "\tb = tensor([2.9211], requires_grad=True)\n",
      "\tloss = 67.89720916748047\n",
      "\n",
      "epoch 120:\n",
      "\tdw = tensor([[-5.3767, -2.8701, -8.2866, -4.9641]])\n",
      "\tdb = tensor([2.3854])\n",
      "\tw = tensor([[72.6731, -1.4832, 86.5606, 40.1642]], requires_grad=True)\n",
      "\tb = tensor([2.5670], requires_grad=True)\n",
      "\tloss = 43.299293518066406\n",
      "\n",
      "epoch 130:\n",
      "\tdw = tensor([[-4.1408, -2.4556, -6.5702, -3.9719]])\n",
      "\tdb = tensor([2.0994])\n",
      "\tw = tensor([[73.3310, -1.1113, 87.5904, 40.7843]], requires_grad=True)\n",
      "\tb = tensor([2.2531], requires_grad=True)\n",
      "\tloss = 27.704872131347656\n",
      "\n",
      "epoch 140:\n",
      "\tdw = tensor([[-3.1970, -2.0783, -5.2202, -3.1755]])\n",
      "\tdb = tensor([1.8244])\n",
      "\tw = tensor([[73.8384, -0.7950, 88.4078, 41.2801]], requires_grad=True)\n",
      "\tb = tensor([1.9788], requires_grad=True)\n",
      "\tloss = 17.77678871154785\n",
      "\n",
      "epoch 150:\n",
      "\tdw = tensor([[-2.4741, -1.7440, -4.1557, -2.5370]])\n",
      "\tdb = tensor([1.5697])\n",
      "\tw = tensor([[74.2306, -0.5286, 89.0579, 41.6765]], requires_grad=True)\n",
      "\tb = tensor([1.7418], requires_grad=True)\n",
      "\tloss = 11.433570861816406\n",
      "\n",
      "epoch 160:\n",
      "\tdw = tensor([[-1.9188, -1.4532, -3.3143, -2.0257]])\n",
      "\tdb = tensor([1.3397])\n",
      "\tw = tensor([[74.5345, -0.3059, 89.5759, 41.9930]], requires_grad=True)\n",
      "\tb = tensor([1.5387], requires_grad=True)\n",
      "\tloss = 7.36865234375\n",
      "\n",
      "epoch 170:\n",
      "\tdw = tensor([[-1.4910, -1.2041, -2.6478, -1.6165]])\n",
      "\tdb = tensor([1.1359])\n",
      "\tw = tensor([[74.7704, -0.1209, 89.9895, 42.2456]], requires_grad=True)\n",
      "\tb = tensor([1.3661], requires_grad=True)\n",
      "\tloss = 4.757040023803711\n",
      "\n",
      "epoch 180:\n",
      "\tdw = tensor([[-1.1607, -0.9929, -2.1186, -1.2894]])\n",
      "\tdb = tensor([0.9580])\n",
      "\tw = tensor([[7.4954e+01, 3.1958e-02, 9.0320e+01, 4.2447e+01]], requires_grad=True)\n",
      "\tb = tensor([1.2201], requires_grad=True)\n",
      "\tloss = 3.0755724906921387\n",
      "\n",
      "epoch 190:\n",
      "\tdw = tensor([[-0.9051, -0.8154, -1.6977, -1.0280]])\n",
      "\tdb = tensor([0.8042])\n",
      "\tw = tensor([[75.0969,  0.1577, 90.5849, 42.6079]], requires_grad=True)\n",
      "\tb = tensor([1.0973], requires_grad=True)\n",
      "\tloss = 1.990999698638916\n",
      "\n",
      "epoch 200:\n",
      "\tdw = tensor([[-0.7068, -0.6674, -1.3622, -0.8193]])\n",
      "\tdb = tensor([0.6726])\n",
      "\tw = tensor([[75.2085,  0.2608, 90.7972, 42.7361]], requires_grad=True)\n",
      "\tb = tensor([0.9944], requires_grad=True)\n",
      "\tloss = 1.2903239727020264\n",
      "\n",
      "epoch 210:\n",
      "\tdw = tensor([[-0.5527, -0.5447, -1.0944, -0.6528]])\n",
      "\tdb = tensor([0.5607])\n",
      "\tw = tensor([[75.2957,  0.3451, 90.9677, 42.8382]], requires_grad=True)\n",
      "\tb = tensor([0.9085], requires_grad=True)\n",
      "\tloss = 0.8370597958564758\n",
      "\n",
      "epoch 220:\n",
      "\tdw = tensor([[-0.4327, -0.4434, -0.8803, -0.5200]])\n",
      "\tdb = tensor([0.4661])\n",
      "\tw = tensor([[75.3639,  0.4138, 91.1048, 42.9195]], requires_grad=True)\n",
      "\tb = tensor([0.8371], requires_grad=True)\n",
      "\tloss = 0.5435166954994202\n",
      "\n",
      "epoch 230:\n",
      "\tdw = tensor([[-0.3391, -0.3602, -0.7088, -0.4140]])\n",
      "\tdb = tensor([0.3866])\n",
      "\tw = tensor([[75.4173,  0.4696, 91.2151, 42.9843]], requires_grad=True)\n",
      "\tb = tensor([0.7777], requires_grad=True)\n",
      "\tloss = 0.35317569971084595\n",
      "\n",
      "epoch 240:\n",
      "\tdw = tensor([[-0.2661, -0.2920, -0.5712, -0.3296]])\n",
      "\tdb = tensor([0.3200])\n",
      "\tw = tensor([[75.4592,  0.5149, 91.3040, 43.0358]], requires_grad=True)\n",
      "\tb = tensor([0.7285], requires_grad=True)\n",
      "\tloss = 0.22966942191123962\n",
      "\n",
      "epoch 250:\n",
      "\tdw = tensor([[-0.2089, -0.2364, -0.4608, -0.2624]])\n",
      "\tdb = tensor([0.2643])\n",
      "\tw = tensor([[75.4921,  0.5516, 91.3756, 43.0769]], requires_grad=True)\n",
      "\tb = tensor([0.6879], requires_grad=True)\n",
      "\tloss = 0.14946097135543823\n",
      "\n",
      "epoch 260:\n",
      "\tdw = tensor([[-0.1641, -0.1910, -0.3720, -0.2088]])\n",
      "\tdb = tensor([0.2181])\n",
      "\tw = tensor([[75.5180,  0.5813, 91.4334, 43.1096]], requires_grad=True)\n",
      "\tb = tensor([0.6543], requires_grad=True)\n",
      "\tloss = 0.0973225086927414\n",
      "\n",
      "epoch 270:\n",
      "\tdw = tensor([[-0.1290, -0.1542, -0.3005, -0.1661]])\n",
      "\tdb = tensor([0.1796])\n",
      "\tw = tensor([[75.5383,  0.6053, 91.4801, 43.1356]], requires_grad=True)\n",
      "\tb = tensor([0.6267], requires_grad=True)\n",
      "\tloss = 0.06341453641653061\n",
      "\n",
      "epoch 280:\n",
      "\tdw = tensor([[-0.1015, -0.1243, -0.2430, -0.1322]])\n",
      "\tdb = tensor([0.1478])\n",
      "\tw = tensor([[75.5542,  0.6246, 91.5178, 43.1563]], requires_grad=True)\n",
      "\tb = tensor([0.6039], requires_grad=True)\n",
      "\tloss = 0.041340865194797516\n",
      "\n",
      "epoch 290:\n",
      "\tdw = tensor([[-0.0799, -0.1001, -0.1965, -0.1051]])\n",
      "\tdb = tensor([0.1215])\n",
      "\tw = tensor([[75.5668,  0.6402, 91.5484, 43.1727]], requires_grad=True)\n",
      "\tb = tensor([0.5852], requires_grad=True)\n",
      "\tloss = 0.026964416727423668\n",
      "\n",
      "epoch 300:\n",
      "\tdw = tensor([[-0.0628, -0.0806, -0.1591, -0.0836]])\n",
      "\tdb = tensor([0.0998])\n",
      "\tw = tensor([[75.5767,  0.6527, 91.5730, 43.1858]], requires_grad=True)\n",
      "\tb = tensor([0.5698], requires_grad=True)\n",
      "\tloss = 0.017597895115613937\n",
      "\n",
      "epoch 310:\n",
      "\tdw = tensor([[-0.0495, -0.0648, -0.1288, -0.0665]])\n",
      "\tdb = tensor([0.0819])\n",
      "\tw = tensor([[75.5845,  0.6628, 91.5930, 43.1962]], requires_grad=True)\n",
      "\tb = tensor([0.5571], requires_grad=True)\n",
      "\tloss = 0.011489888653159142\n",
      "\n",
      "epoch 320:\n",
      "\tdw = tensor([[-0.0390, -0.0521, -0.1043, -0.0529]])\n",
      "\tdb = tensor([0.0672])\n",
      "\tw = tensor([[75.5906,  0.6709, 91.6092, 43.2045]], requires_grad=True)\n",
      "\tb = tensor([0.5468], requires_grad=True)\n",
      "\tloss = 0.007506451569497585\n",
      "\n",
      "epoch 330:\n",
      "\tdw = tensor([[-0.0307, -0.0418, -0.0846, -0.0421]])\n",
      "\tdb = tensor([0.0551])\n",
      "\tw = tensor([[75.5954,  0.6774, 91.6224, 43.2111]], requires_grad=True)\n",
      "\tb = tensor([0.5383], requires_grad=True)\n",
      "\tloss = 0.004906574264168739\n",
      "\n",
      "epoch 340:\n",
      "\tdw = tensor([[-0.0242, -0.0336, -0.0685, -0.0335]])\n",
      "\tdb = tensor([0.0451])\n",
      "\tw = tensor([[75.5992,  0.6827, 91.6330, 43.2163]], requires_grad=True)\n",
      "\tb = tensor([0.5313], requires_grad=True)\n",
      "\tloss = 0.0032078861258924007\n",
      "\n",
      "epoch 350:\n",
      "\tdw = tensor([[-0.0190, -0.0269, -0.0556, -0.0266]])\n",
      "\tdb = tensor([0.0369])\n",
      "\tw = tensor([[75.6022,  0.6869, 91.6416, 43.2205]], requires_grad=True)\n",
      "\tb = tensor([0.5256], requires_grad=True)\n",
      "\tloss = 0.0020984048023819923\n",
      "\n",
      "epoch 360:\n",
      "\tdw = tensor([[-0.0150, -0.0216, -0.0451, -0.0211]])\n",
      "\tdb = tensor([0.0302])\n",
      "\tw = tensor([[75.6046,  0.6902, 91.6486, 43.2238]], requires_grad=True)\n",
      "\tb = tensor([0.5209], requires_grad=True)\n",
      "\tloss = 0.0013731898507103324\n",
      "\n",
      "epoch 370:\n",
      "\tdw = tensor([[-0.0118, -0.0173, -0.0366, -0.0168]])\n",
      "\tdb = tensor([0.0247])\n",
      "\tw = tensor([[75.6064,  0.6929, 91.6543, 43.2264]], requires_grad=True)\n",
      "\tb = tensor([0.5171], requires_grad=True)\n",
      "\tloss = 0.0008992408402264118\n",
      "\n",
      "epoch 380:\n",
      "\tdw = tensor([[-0.0093, -0.0138, -0.0297, -0.0134]])\n",
      "\tdb = tensor([0.0202])\n",
      "\tw = tensor([[75.6079,  0.6951, 91.6589, 43.2285]], requires_grad=True)\n",
      "\tb = tensor([0.5140], requires_grad=True)\n",
      "\tloss = 0.0005891238106414676\n",
      "\n",
      "epoch 390:\n",
      "\tdw = tensor([[-0.0073, -0.0111, -0.0241, -0.0106]])\n",
      "\tdb = tensor([0.0165])\n",
      "\tw = tensor([[75.6090,  0.6968, 91.6626, 43.2302]], requires_grad=True)\n",
      "\tb = tensor([0.5114], requires_grad=True)\n",
      "\tloss = 0.00038618731196038425\n",
      "\n",
      "epoch 400:\n",
      "\tdw = tensor([[-0.0057, -0.0089, -0.0196, -0.0084]])\n",
      "\tdb = tensor([0.0135])\n",
      "\tw = tensor([[75.6099,  0.6982, 91.6657, 43.2315]], requires_grad=True)\n",
      "\tb = tensor([0.5093], requires_grad=True)\n",
      "\tloss = 0.00025306607130914927\n",
      "\n",
      "epoch 410:\n",
      "\tdw = tensor([[-0.0045, -0.0071, -0.0159, -0.0067]])\n",
      "\tdb = tensor([0.0111])\n",
      "\tw = tensor([[75.6107,  0.6993, 91.6682, 43.2325]], requires_grad=True)\n",
      "\tb = tensor([0.5076], requires_grad=True)\n",
      "\tloss = 0.0001660077687120065\n",
      "\n",
      "epoch 420:\n",
      "\tdw = tensor([[-0.0035, -0.0057, -0.0129, -0.0053]])\n",
      "\tdb = tensor([0.0090])\n",
      "\tw = tensor([[75.6112,  0.7002, 91.6702, 43.2334]], requires_grad=True)\n",
      "\tb = tensor([0.5062], requires_grad=True)\n",
      "\tloss = 0.00010903594375122339\n",
      "\n",
      "epoch 430:\n",
      "\tdw = tensor([[-0.0028, -0.0045, -0.0105, -0.0042]])\n",
      "\tdb = tensor([0.0074])\n",
      "\tw = tensor([[75.6116,  0.7009, 91.6718, 43.2341]], requires_grad=True)\n",
      "\tb = tensor([0.5051], requires_grad=True)\n",
      "\tloss = 7.161880057537928e-05\n",
      "\n",
      "epoch 440:\n",
      "\tdw = tensor([[-0.0022, -0.0036, -0.0086, -0.0034]])\n",
      "\tdb = tensor([0.0060])\n",
      "\tw = tensor([[75.6120,  0.7015, 91.6731, 43.2346]], requires_grad=True)\n",
      "\tb = tensor([0.5042], requires_grad=True)\n",
      "\tloss = 4.7068529966054484e-05\n",
      "\n",
      "epoch 450:\n",
      "\tdw = tensor([[-0.0017, -0.0029, -0.0070, -0.0027]])\n",
      "\tdb = tensor([0.0049])\n",
      "\tw = tensor([[75.6123,  0.7019, 91.6742, 43.2350]], requires_grad=True)\n",
      "\tb = tensor([0.5034], requires_grad=True)\n",
      "\tloss = 3.09243805531878e-05\n",
      "\n",
      "epoch 460:\n",
      "\tdw = tensor([[-0.0013, -0.0023, -0.0056, -0.0021]])\n",
      "\tdb = tensor([0.0040])\n",
      "\tw = tensor([[75.6125,  0.7023, 91.6751, 43.2353]], requires_grad=True)\n",
      "\tb = tensor([0.5028], requires_grad=True)\n",
      "\tloss = 2.028380913543515e-05\n",
      "\n",
      "epoch 470:\n",
      "\tdw = tensor([[-0.0011, -0.0018, -0.0046, -0.0017]])\n",
      "\tdb = tensor([0.0033])\n",
      "\tw = tensor([[75.6126,  0.7026, 91.6758, 43.2356]], requires_grad=True)\n",
      "\tb = tensor([0.5023], requires_grad=True)\n",
      "\tloss = 1.3369003681873437e-05\n",
      "\n",
      "epoch 480:\n",
      "\tdw = tensor([[-0.0008, -0.0015, -0.0037, -0.0013]])\n",
      "\tdb = tensor([0.0027])\n",
      "\tw = tensor([[75.6128,  0.7028, 91.6763, 43.2358]], requires_grad=True)\n",
      "\tb = tensor([0.5018], requires_grad=True)\n",
      "\tloss = 8.801173862593714e-06\n",
      "\n",
      "epoch 490:\n",
      "\tdw = tensor([[-0.0007, -0.0012, -0.0030, -0.0011]])\n",
      "\tdb = tensor([0.0022])\n",
      "\tw = tensor([[75.6129,  0.7030, 91.6768, 43.2360]], requires_grad=True)\n",
      "\tb = tensor([0.5015], requires_grad=True)\n",
      "\tloss = 5.814920314151095e-06\n",
      "\n",
      "epoch 500:\n",
      "\tdw = tensor([[-0.0005, -0.0009, -0.0025, -0.0009]])\n",
      "\tdb = tensor([0.0018])\n",
      "\tw = tensor([[75.6129,  0.7031, 91.6772, 43.2361]], requires_grad=True)\n",
      "\tb = tensor([0.5012], requires_grad=True)\n",
      "\tloss = 3.835100869764574e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# это вот работает\n",
    "Y = y.unsqueeze(1)\n",
    "\n",
    "layer = Linear(X.size(1), 1)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.01409\n",
    "optimizer = torch.optim.SGD([layer.weights, layer.biases], lr=learning_rate)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(1, epochs + 1):\n",
    "    y_pred = layer.forward(X)\n",
    "    loss = mse.forward(y_pred, Y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch}:'\n",
    "              f'\\n\\tdw = {layer.weights.grad}\\n\\tdb = {layer.biases.grad}\\n\\t'\n",
    "              f'w = {layer.weights}\\n\\tb = {layer.biases}\\n\\t'\n",
    "              f'loss = {loss}\\n')\n",
    "\n",
    "    optimizer.zero_grad()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7"
   },
   "source": [
    "4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd"
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt"
   },
   "source": [
    "4.2.1 Модифицируйте класс `MSELoss` из __3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8wjk9iPMQ4x"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return  # <реализовать логику MSE>\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        self.dinput =  # df/dy^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8"
   },
   "source": [
    "4.2.2. Модифицируйте класс `Neuron` из __4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_OpuAP0Jpz1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs):\n",
    "        # <создать атрибуты объекта weights и bias>\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return  # <реализовать логику нейрона>\n",
    "\n",
    "    def backward(self, dvalue):\n",
    "        # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "        # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
    "        self.dweights =  # df/dW\n",
    "        self.dbias =  # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx"
   },
   "source": [
    "4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zqwm_7eqJim1"
   },
   "outputs": [],
   "source": [
    "n_inputs =  # <размерность элемента выборки >\n",
    "learning_rate = 0.1  #  скорость обучения\n",
    "n_epoch = 100  #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "    y_pred =  # <прогон через нейрон>\n",
    "    curr_loss =  # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk"
   },
   "source": [
    "4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7"
   },
   "source": [
    "4.3.1 Модифицируйте класс `Linear` из __1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zWuhaLdSB2_"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_features, n_neurons):\n",
    "        # <создать атрибуты объекта weights и biases>\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return  # <реализовать логику слоя>\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights =  # df/dW\n",
    "        self.dbiases =  # df/db\n",
    "        self.dinputs =  # df/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt"
   },
   "source": [
    "4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN"
   },
   "source": [
    "4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axUjpPz-SvS1"
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXoiNxkpTziV"
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs.clip(min=0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.clone()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXhspwW6T44T"
   },
   "outputs": [],
   "source": [
    "# создание компонентов сети\n",
    "# fc1 = \n",
    "# relu1 = \n",
    "# fc2 = \n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "    # <forward pass>\n",
    "    # fc1 > relu1 > fc2 > loss\n",
    "\n",
    "    data_loss =  # <прогон через функцию потерь>\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'epoch {epoch} mean loss {data_loss}')\n",
    "        ys.append(out)\n",
    "\n",
    "    # <backprop>\n",
    "    # loss > fc2 > relu1 > fc1\n",
    "\n",
    "    # <шаг оптимизации для fc1>\n",
    "\n",
    "    # <шаг оптимизации для fc2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKi0OfoUkwk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "    ax.scatter(X.numpy(), y.numpy(), color=\"orange\")\n",
    "    ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
    "    ax.set_xlim(-1.05, 1.5)\n",
    "    ax.set_ylim(-0.25, 1.25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
