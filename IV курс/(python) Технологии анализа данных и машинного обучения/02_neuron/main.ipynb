{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание нейронной сети без использования готовых решений\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "PqC4R7SGseKa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "torch.set_warn_always(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "## 1. Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "f4agkY9WqPwe"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weights: torch.Tensor, bias: torch.Tensor):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(inputs * self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "HJRkSkHHsb7u"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = torch.tensor(3.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron.forward: 4.840000152587891\n",
      "functional.linear: 4.840000152587891\n"
     ]
    }
   ],
   "source": [
    "neuron = Neuron(weights, bias)\n",
    "print(f'Neuron.forward: {neuron.forward(inputs)}')\n",
    "print(f'functional.linear: {F.linear(inputs, weights, bias)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJvnwiyty37"
   },
   "source": [
    "1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "fVWF3a9vtx90"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, weights: torch.Tensor, biases: torch.Tensor):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(inputs, self.weights.T) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "Fo-JFnHPuFCS"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]])  # убрал .T\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear.forward:\n",
      "tensor([ 4.8400,  0.1700, 10.3900])\n",
      "\n",
      "functional.linear:\n",
      "tensor([ 4.8400,  0.1700, 10.3900])\n"
     ]
    }
   ],
   "source": [
    "m = Linear(weights, biases)\n",
    "print(f'Linear.forward:\\n{m.forward(inputs)}')\n",
    "print(f'\\nfunctional.linear:\\n{F.linear(inputs, weights, biases)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtsJzcxuyGd"
   },
   "source": [
    "1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "Z8IizmtsuhO1"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear.forward:\n",
      "tensor([[ 3.7900,  0.9200,  9.0850],\n",
      "        [ 6.1400, -2.1000,  6.9000],\n",
      "        [ 2.0400,  0.7610,  6.7260]])\n",
      "\n",
      "functional.linear:\n",
      "tensor([[ 3.7900,  0.9200,  9.0850],\n",
      "        [ 6.1400, -2.1000,  6.9000],\n",
      "        [ 2.0400,  0.7610,  6.7260]])\n"
     ]
    }
   ],
   "source": [
    "m = Linear(weights, biases)\n",
    "print(f'Linear.forward:\\n{m.forward(inputs)}')\n",
    "print(f'\\nfunctional.linear:\\n{F.linear(inputs, weights, biases)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ2OxH4_vBLu"
   },
   "source": [
    "1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "IOv52EdovASs"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = torch.randn(out_features, in_features, requires_grad=True)\n",
    "        self.biases = torch.randn(out_features, requires_grad=True)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(inputs, self.weights.T) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -5.0178,   0.5240,  -3.9319],\n        [  4.0738,  -6.7887,  -3.5657],\n        [-11.6052,  -0.3882,  -3.1959]], grad_fn=<AddBackward0>)"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "m = Linear(4, 3)\n",
    "m.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -5.0178,   0.5240,  -3.9319],\n        [  4.0738,  -6.7887,  -3.5657],\n        [-11.6052,  -0.3882,  -3.1959]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка\n",
    "nn_m = nn.Linear(4, 3)\n",
    "nn_m.weight = nn.Parameter(m.weights)\n",
    "nn_m.bias = nn.Parameter(m.biases)\n",
    "nn_m.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPG4UqL4wajI"
   },
   "source": [
    "1.5 Используя решение из __1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input = Linear(4, 3)\n",
    "        self.output = Linear(3, 7)\n",
    "\n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        return self.output.forward(self.input.forward(inputs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  5.1553,   1.1916,  -4.7817,   3.6611,  -9.6546,   0.1692,   5.8099],\n        [ -0.8347,   0.0913,   3.1227,   3.5542,   5.4400,   4.9157,   8.2533],\n        [  8.8057,  -1.1342,  -7.8488,   6.7415, -11.1044,   5.1500,   5.6152]],\n       grad_fn=<AddBackward0>)"
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "m = NeuralNet()\n",
    "m.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "## 2. Создание функций активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "def randn(*size: int, seed: int = 0) -> torch.Tensor:\n",
    "    torch.manual_seed(seed)\n",
    "    return torch.randn(*size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.maximum(inputs, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.5410, -0.2934, -2.1788],\n        [ 0.5684, -1.0845, -1.3986],\n        [ 0.4033,  0.8380, -0.7193],\n        [-0.4033, -0.5966,  0.1820]])"
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = randn(4, 3)\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU:\n",
      "tensor([[1.5410, 0.0000, 0.0000],\n",
      "        [0.5684, 0.0000, 0.0000],\n",
      "        [0.4033, 0.8380, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1820]])\n",
      "\n",
      "nn.ReLU:\n",
      "tensor([[1.5410, 0.0000, 0.0000],\n",
      "        [0.5684, 0.0000, 0.0000],\n",
      "        [0.4033, 0.8380, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1820]])\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "nn_relu = nn.ReLU()\n",
    "print(f'ReLU:\\n{relu.forward(inputs)}')\n",
    "print(f'\\nnn.ReLU:\\n{nn_relu.forward(inputs)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def __init__(self, dim: int = 0):\n",
    "        assert dim == 0 or dim == 1\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        exp = torch.exp(inputs)\n",
    "        return exp / torch.sum(exp, dim=self.dim).unsqueeze(self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.5410, -0.2934, -2.1788],\n        [ 0.5684, -1.0845, -1.3986],\n        [ 0.4033,  0.8380, -0.7193],\n        [-0.4033, -0.5966,  0.1820]])"
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = randn(4, 3)\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:\n",
      "tensor([[0.8446, 0.1349, 0.0205],\n",
      "        [0.7511, 0.1438, 0.1051],\n",
      "        [0.3484, 0.5382, 0.1134],\n",
      "        [0.2762, 0.2277, 0.4961]])\n",
      "\n",
      "nn.Softmax:\n",
      "tensor([[0.8446, 0.1349, 0.0205],\n",
      "        [0.7511, 0.1438, 0.1051],\n",
      "        [0.3484, 0.5382, 0.1134],\n",
      "        [0.2762, 0.2277, 0.4961]])\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax(dim=1)\n",
    "nn_softmax = nn.Softmax(dim=1)\n",
    "print(f'Softmax:\\n{softmax.forward(inputs)}')\n",
    "print(f'\\nnn.Softmax:\\n{nn_softmax.forward(inputs)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "NzMz7HDLySxK"
   },
   "outputs": [],
   "source": [
    "class ELU:\n",
    "\n",
    "    def __init__(self, alpha: float = 1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(inputs > 0, inputs, self.alpha * (torch.exp(inputs) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.5410, -0.2934, -2.1788],\n        [ 0.5684, -1.0845, -1.3986],\n        [ 0.4033,  0.8380, -0.7193],\n        [-0.4033, -0.5966,  0.1820]])"
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = randn(4, 3)\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELU:\n",
      "tensor([[ 1.5410, -0.3583, -1.2495],\n",
      "        [ 0.5684, -0.9327, -1.0611],\n",
      "        [ 0.4033,  0.8380, -0.7227],\n",
      "        [-0.4677, -0.6331,  0.1820]])\n",
      "\n",
      "nn.ELU:\n",
      "tensor([[ 1.5410, -0.3583, -1.2495],\n",
      "        [ 0.5684, -0.9327, -1.0611],\n",
      "        [ 0.4033,  0.8380, -0.7227],\n",
      "        [-0.4677, -0.6331,  0.1820]])\n"
     ]
    }
   ],
   "source": [
    "elu = ELU(alpha=1.409)\n",
    "nn_elu = nn.ELU(alpha=1.409)\n",
    "print(f'ELU:\\n{elu.forward(inputs)}')\n",
    "print(f'\\nnn.ELU:\\n{nn_elu.forward(inputs)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "## 3. Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "layer = Linear(4, 1)\n",
    "y_pred = layer.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss:\n",
      "101.30004119873047\n",
      "\n",
      "nn.MSELoss:\n",
      "101.30004119873047\n"
     ]
    }
   ],
   "source": [
    "mse = MSELoss()\n",
    "nn_mse = nn.MSELoss()\n",
    "print(f'MSELoss:\\n{mse.forward(y_pred, y)}')\n",
    "print(f'\\nnn.MSELoss:\\n{nn_mse.forward(y_pred, y)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return -torch.sum(y_true * torch.log(y_pred), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "s7Qoupfo1ZGJ"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "layer = Linear(4, 3)\n",
    "softmax = Softmax()\n",
    "y_pred = softmax.forward(layer.forward(inputs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([9.0918e+00, 1.1272e-04, 1.5679e+01], grad_fn=<NegBackward0>)"
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce = CategoricalCrossEntropyLoss()\n",
    "cce.forward(y_pred, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "class MSELossL2:\n",
    "\n",
    "    def __init__(self, lambda_: float, weights: torch.Tensor):\n",
    "        self.lambda_ = lambda_\n",
    "        self.weights = weights\n",
    "\n",
    "    def data_loss(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "    def reg_loss(self) -> torch.Tensor:\n",
    "        return self.lambda_ * torch.sum(self.weights ** 2)\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return self.data_loss(y_pred, y_true) + self.reg_loss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4]).unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "layer = Linear(4, 1)\n",
    "y_pred = layer.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(314.5113, grad_fn=<AddBackward0>)"
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_l2 = MSELossL2(lambda_=1.409, weights=layer.weights)\n",
    "mse_l2.forward(y_pred, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Обратное распространение ошибки"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5, random_state=0)\n",
    "# X.dtype == float64 - нет смысла использовать torch.from_numpy\n",
    "# т.к. последующее приведение типов Tensor.type(torch.float32) приведет к копированию данных\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return (y_pred - y_true) ** 2  # когда MSE (mean squared error) без mean\n",
    "\n",
    "    def backward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        # здравая мысль - похоже на производную MSE, только mean потерялась,\n",
    "        # т.к. обработка по одному ответу за шаг\n",
    "        return 2 * (y_pred - y_true)  # df/dc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "L0KqxPJU9kAN"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.weights = torch.randn(self.in_features)\n",
    "        self.bias = torch.randn(1)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(inputs * self.weights) + self.bias\n",
    "\n",
    "    def backward(self, dvalue: torch.Tensor, inputs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dweights = dvalue * 1  # df/dw\n",
    "        dbias = dvalue * 1  # df/db\n",
    "        dinputs = dbias * self.weights  # df/dx\n",
    "        return dvalue * inputs, dvalue  # считаем градиент (типа на предшествующем слое)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM"
   },
   "source": [
    "4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "def pretty_train_log(epoch: int, **kwargs) -> None:\n",
    "    params = '\\n\\t'.join([f'{k} = {v}' for k, v in kwargs.items()])\n",
    "    print(f'epoch {epoch:03} |' + '-' * 50 + f'\\n\\t{params}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "_g_FvwvmALJd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([ 5.8937,  3.4480,  8.5208, -6.0718])\n",
      "\tdw = tensor([ -43.5271,  -37.4140, -106.9956,   66.4020])\n",
      "\tbias = tensor([-9.6251])\n",
      "\tdb = tensor([85.4055])\n",
      "\tloss = tensor([1823.5259])\n",
      "\n",
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([21.6376, 36.3937, 65.8168, 88.4170])\n",
      "\tdw = tensor([9.4374, 2.5177, 7.3150, 2.5319])\n",
      "\tbias = tensor([1.1639])\n",
      "\tdb = tensor([7.9437])\n",
      "\tloss = tensor([15.7756])\n",
      "\n",
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.5037, 34.1982, 67.6044, 87.9179])\n",
      "\tdw = tensor([ 0.0004,  0.0003, -0.0005,  0.0003])\n",
      "\tbias = tensor([0.4687])\n",
      "\tdb = tensor([0.0006])\n",
      "\tloss = tensor([1.0514e-07])\n",
      "\n",
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4878, 34.1696, 67.6197, 87.9236])\n",
      "\tdw = tensor([-0.0058,  0.0145,  0.0002,  0.0066])\n",
      "\tbias = tensor([0.5022])\n",
      "\tdb = tensor([-0.0090])\n",
      "\tloss = tensor([2.0193e-05])\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10])\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235])\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11])\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12])\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([0., -0., -0., -0.])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.])\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10])\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235])\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11])\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12])\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([0., -0., -0., -0.])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.])\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10])\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235])\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11])\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12])\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([0., -0., -0., -0.])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.])\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10])\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235])\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11])\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12])\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235])\n",
      "\tdw = tensor([0., -0., -0., -0.])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([20.4924, 34.1698, 67.6242, 87.9235]),\n array([20.4923687 , 34.16981149, 67.62424823, 87.9234763 ]))"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# работает как часы 🕓...\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_inputs = X.size(1)  # размерность элемента выборки\n",
    "learning_rate = 0.1  #  скорость обучения\n",
    "n_epoch = 100  #  количество эпох\n",
    "\n",
    "y_true = y.unsqueeze(1)\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    for i, (x_example, y_example) in enumerate(zip(X, y_true), start=1):\n",
    "        # forward pass\n",
    "        y_pred = neuron.forward(x_example)  # прогон через нейрон\n",
    "        curr_loss = loss.forward(y_pred, y_example)  # прогон через функцию потерь\n",
    "\n",
    "        # backprop\n",
    "        # вызов методов backward\n",
    "        dw, db = neuron.backward(loss.backward(y_pred, y_example), x_example)\n",
    "        # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "        # шаг оптимизации для весов (weights и bias) нейрона\n",
    "        neuron.weights -= learning_rate * dw\n",
    "        neuron.bias -= learning_rate * db\n",
    "\n",
    "        if (epoch - 1) % 20 == 0 and (i - 1) % 30 == 0:\n",
    "            pretty_train_log(epoch, i=i, weights=neuron.weights, dw=dw, bias=neuron.bias, db=db, loss=curr_loss)\n",
    "\n",
    "neuron.weights, coef"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Судя по выводу задача регрессии была решена еще в конце 1-ой эпохи (первобытности).\n",
    "Зачем тогда пакетный градиентный спуск, если этот работает быстрее?\n",
    "\n",
    "Зачем мы это все делали и почему называем обратным распространением, если уже знаем всю производную?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Проверка**\n",
    "\n",
    "Сделаем то же самое, но пусть производную посчитает pytorch (честно, будто не знаем производную сложной функции)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._last_forward: torch.Tensor | None = None\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        self._last_forward = (y_pred - y_true) ** 2\n",
    "        return self._last_forward\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        self._last_forward.backward()\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.weights = torch.randn(self.in_features, requires_grad=True)\n",
    "        self.bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(inputs * self.weights) + self.bias\n",
    "\n",
    "    def backward(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dw = self.weights.grad.clone().detach()\n",
    "        db = self.bias.grad.clone().detach()\n",
    "        self.weights.grad.zero_()\n",
    "        self.bias.grad.zero_()\n",
    "        return dw, db\n",
    "\n",
    "    def update_weights(self, lr: float, dw: torch.Tensor, db: torch.Tensor) -> None:\n",
    "        with torch.no_grad():\n",
    "            self.weights -= lr * dw\n",
    "            self.bias -= lr * db"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([ 5.8937,  3.4480,  8.5208, -6.0718], requires_grad=True)\n",
      "\tdw = tensor([ -43.5271,  -37.4140, -106.9956,   66.4020])\n",
      "\tbias = tensor([-9.6251], requires_grad=True)\n",
      "\tdb = tensor([85.4055])\n",
      "\tloss = tensor([1823.5259], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([21.6376, 36.3937, 65.8168, 88.4170], requires_grad=True)\n",
      "\tdw = tensor([9.4374, 2.5177, 7.3150, 2.5319])\n",
      "\tbias = tensor([1.1639], requires_grad=True)\n",
      "\tdb = tensor([7.9437])\n",
      "\tloss = tensor([15.7756], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.5037, 34.1982, 67.6044, 87.9179], requires_grad=True)\n",
      "\tdw = tensor([ 0.0004,  0.0003, -0.0005,  0.0003])\n",
      "\tbias = tensor([0.4687], requires_grad=True)\n",
      "\tdb = tensor([0.0006])\n",
      "\tloss = tensor([1.0514e-07], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 001 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4878, 34.1696, 67.6197, 87.9236], requires_grad=True)\n",
      "\tdw = tensor([-0.0058,  0.0145,  0.0002,  0.0066])\n",
      "\tbias = tensor([0.5022], requires_grad=True)\n",
      "\tdb = tensor([-0.0090])\n",
      "\tloss = tensor([2.0193e-05], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 021 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([0., 0., 0., 0.])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 041 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([0., 0., 0., 0.])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 061 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([0., 0., 0., 0.])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 1\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.3330e-05, -2.0053e-05, -5.7348e-05,  3.5591e-05])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([4.5776e-05])\n",
      "\tloss = tensor([5.2387e-10], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 31\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6242, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-1.8128e-05, -4.8362e-06, -1.4051e-05, -4.8634e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-1.5259e-05])\n",
      "\tloss = tensor([5.8208e-11], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 61\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([-2.5646e-06, -1.5543e-06,  2.9370e-06, -2.0571e-06])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([-3.8147e-06])\n",
      "\tloss = tensor([3.6380e-12], grad_fn=<PowBackward0>)\n",
      "\n",
      "epoch 081 |--------------------------------------------------\n",
      "\ti = 91\n",
      "\tweights = tensor([20.4924, 34.1698, 67.6243, 87.9235], requires_grad=True)\n",
      "\tdw = tensor([0., 0., 0., 0.])\n",
      "\tbias = tensor([0.5000], requires_grad=True)\n",
      "\tdb = tensor([0.])\n",
      "\tloss = tensor([0.], grad_fn=<PowBackward0>)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([20.4924, 34.1698, 67.6242, 87.9235], requires_grad=True),\n array([20.4923687 , 34.16981149, 67.62424823, 87.9234763 ]))"
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "y_true = y.unsqueeze(1)\n",
    "\n",
    "neuron = Neuron(X.size(1))\n",
    "mse = MSELoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, (x_example, y_example) in enumerate(zip(X, y_true), start=1):\n",
    "        # feed forward\n",
    "        y_pred = neuron.forward(x_example)\n",
    "        loss = mse.forward(y_pred, y_example)\n",
    "\n",
    "        # backward\n",
    "        mse.backward()\n",
    "        dw, db = neuron.backward()\n",
    "\n",
    "        # optimize step\n",
    "        neuron.update_weights(learning_rate, dw, db)\n",
    "\n",
    "        if (epoch - 1) % 20 == 0 and (i - 1) % 30 == 0:\n",
    "            pretty_train_log(epoch, i=i, weights=neuron.weights, dw=dw, bias=neuron.bias, db=db, loss=loss)\n",
    "\n",
    "neuron.weights, coef"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._last_forward: torch.Tensor | None = None\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        self._last_forward = torch.mean((y_pred - y_true) ** 2)\n",
    "        return self._last_forward\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        self._last_forward.backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.weights = torch.randn(self.in_features, requires_grad=True)\n",
    "        self.bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(inputs * self.weights) + self.bias\n",
    "\n",
    "    def backward(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dw = self.weights.grad.clone().detach()\n",
    "        db = self.bias.grad.clone().detach()\n",
    "        self.weights.grad.zero_()\n",
    "        self.bias.grad.zero_()\n",
    "        return dw, db\n",
    "\n",
    "    def update_weights(self, lr: float, dw: torch.Tensor, db: torch.Tensor) -> None:\n",
    "        with torch.no_grad():\n",
    "            self.weights -= lr * dw\n",
    "            self.bias -= lr * db"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:\n",
      "\tdw = tensor([  0.0946,  -5.0925, -19.5831, -23.7725])\n",
      "\tw = tensor([22.8361, 35.1808, 56.8774, 77.7312], requires_grad=True)\n",
      "\tdb = tensor([-1.1937])\n",
      "\tb = tensor([-0.3279], requires_grad=True)\n",
      "\tloss = 273.2061462402344\n",
      "\n",
      "epoch 20:\n",
      "\tdw = tensor([ 1.2022,  1.0444, -3.6096, -2.9211])\n",
      "\tw = tensor([21.3865, 35.2819, 65.2723, 86.5805], requires_grad=True)\n",
      "\tdb = tensor([-0.2879])\n",
      "\tb = tensor([0.2734], requires_grad=True)\n",
      "\tloss = 8.566176414489746\n",
      "\n",
      "epoch 30:\n",
      "\tdw = tensor([ 0.3433,  0.4741, -0.8571, -0.4070])\n",
      "\tw = tensor([20.7266, 34.5308, 67.0295, 87.7218], requires_grad=True)\n",
      "\tdb = tensor([-0.0849])\n",
      "\tb = tensor([0.4255], requires_grad=True)\n",
      "\tloss = 0.48739707469940186\n",
      "\n",
      "epoch 40:\n",
      "\tdw = tensor([ 0.0871,  0.1401, -0.2223, -0.0644])\n",
      "\tw = tensor([20.5520, 34.2695, 67.4672, 87.8893], requires_grad=True)\n",
      "\tdb = tensor([-0.0294])\n",
      "\tb = tensor([0.4750], requires_grad=True)\n",
      "\tloss = 0.032591093331575394\n",
      "\n",
      "epoch 50:\n",
      "\tdw = tensor([ 0.0222,  0.0377, -0.0591, -0.0114])\n",
      "\tw = tensor([20.5077, 34.1962, 67.5823, 87.9171], requires_grad=True)\n",
      "\tdb = tensor([-0.0099])\n",
      "\tb = tensor([0.4920], requires_grad=True)\n",
      "\tloss = 0.002265702001750469\n",
      "\n",
      "epoch 60:\n",
      "\tdw = tensor([ 0.0057,  0.0099, -0.0158, -0.0022])\n",
      "\tw = tensor([20.4964, 34.1767, 67.6130, 87.9222], requires_grad=True)\n",
      "\tdb = tensor([-0.0031])\n",
      "\tb = tensor([0.4976], requires_grad=True)\n",
      "\tloss = 0.00015940135926939547\n",
      "\n",
      "epoch 70:\n",
      "\tdw = tensor([ 0.0015,  0.0026, -0.0042, -0.0005])\n",
      "\tw = tensor([20.4934, 34.1716, 67.6212, 87.9232], requires_grad=True)\n",
      "\tdb = tensor([-0.0009])\n",
      "\tb = tensor([0.4993], requires_grad=True)\n",
      "\tloss = 1.1334186638123356e-05\n",
      "\n",
      "epoch 80:\n",
      "\tdw = tensor([ 0.0004,  0.0007, -0.0011, -0.0001])\n",
      "\tw = tensor([20.4926, 34.1703, 67.6234, 87.9234], requires_grad=True)\n",
      "\tdb = tensor([-0.0003])\n",
      "\tb = tensor([0.4998], requires_grad=True)\n",
      "\tloss = 8.034064649109496e-07\n",
      "\n",
      "epoch 90:\n",
      "\tdw = tensor([ 1.0860e-04,  1.7978e-04, -3.0411e-04, -2.9911e-05])\n",
      "\tw = tensor([20.4924, 34.1699, 67.6240, 87.9235], requires_grad=True)\n",
      "\tdb = tensor([-7.3667e-05])\n",
      "\tb = tensor([0.4999], requires_grad=True)\n",
      "\tloss = 5.771409661292637e-08\n",
      "\n",
      "epoch 100:\n",
      "\tdw = tensor([ 2.4684e-05,  4.2315e-05, -9.1560e-05, -2.5875e-05])\n",
      "\tw = tensor([20.4924, 34.1698, 67.6242, 87.9235], requires_grad=True)\n",
      "\tdb = tensor([-2.1472e-05])\n",
      "\tb = tensor([0.5000], requires_grad=True)\n",
      "\tloss = 4.774629580595047e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "y_true = y.unsqueeze(1)\n",
    "\n",
    "neuron = Neuron(X.size(1))\n",
    "mse = MSELoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    y_pred = torch.empty(y_true.size())\n",
    "    for i, x in enumerate(X):\n",
    "        y_pred[i, 0] = neuron.forward(x)\n",
    "\n",
    "    loss = mse.forward(y_pred, y_true)\n",
    "\n",
    "    mse.backward()\n",
    "    dw, db = neuron.backward()\n",
    "    neuron.update_weights(learning_rate, dw, db)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch}:\\n'\n",
    "              f'\\tdw = {dw}\\n'\n",
    "              f'\\tw = {neuron.weights}\\n'\n",
    "              f'\\tdb = {db}\\n'\n",
    "              f'\\tb = {neuron.bias}\\n'\n",
    "              f'\\tloss = {loss}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Используем слой из 1 нейрона и почти все отдаем pytorch кроме обновления весов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50:\n",
      "\tdw = tensor([[ -6.0558, -15.0842, -33.4915, -42.9164]])\n",
      "\tw = tensor([[21.5726, 31.4347, 47.8225, 66.2012]], requires_grad=True)\n",
      "\tdb = tensor([-1.3325])\n",
      "\tb = tensor([-0.7389], requires_grad=True)\n",
      "\tloss = 838.6507568359375\n",
      "\n",
      "epoch 100:\n",
      "\tdw = tensor([[  1.1853,  -0.8189,  -9.8127, -10.7610]])\n",
      "\tw = tensor([[22.2123, 35.4245, 61.0621, 82.3018]], requires_grad=True)\n",
      "\tdb = tensor([-0.6651])\n",
      "\tb = tensor([-0.0315], requires_grad=True)\n",
      "\tloss = 64.63803100585938\n",
      "\n",
      "epoch 150:\n",
      "\tdw = tensor([[ 0.9964,  0.7865, -3.3318, -2.8093]])\n",
      "\tw = tensor([[21.3588, 35.1799, 65.2005, 86.4030]], requires_grad=True)\n",
      "\tdb = tensor([-0.2642])\n",
      "\tb = tensor([0.2715], requires_grad=True)\n",
      "\tloss = 7.178253650665283\n",
      "\n",
      "epoch 200:\n",
      "\tdw = tensor([[ 0.4676,  0.5566, -1.2485, -0.7666]])\n",
      "\tw = tensor([[20.8615, 34.6863, 66.6720, 87.4926]], requires_grad=True)\n",
      "\tdb = tensor([-0.1130])\n",
      "\tb = tensor([0.3947], requires_grad=True)\n",
      "\tloss = 1.0145002603530884\n",
      "\n",
      "epoch 250:\n",
      "\tdw = tensor([[ 0.1950,  0.2764, -0.4942, -0.2191]])\n",
      "\tw = tensor([[20.6428, 34.4022, 67.2383, 87.7956]], requires_grad=True)\n",
      "\tdb = tensor([-0.0529])\n",
      "\tb = tensor([0.4498], requires_grad=True)\n",
      "\tloss = 0.16036461293697357\n",
      "\n",
      "epoch 300:\n",
      "\tdw = tensor([[ 0.0789,  0.1229, -0.2010, -0.0655]])\n",
      "\tw = tensor([[20.5531, 34.2691, 67.4655, 87.8838]], requires_grad=True)\n",
      "\tdb = tensor([-0.0256])\n",
      "\tb = tensor([0.4760], requires_grad=True)\n",
      "\tloss = 0.026530513539910316\n",
      "\n",
      "epoch 350:\n",
      "\tdw = tensor([[ 0.0318,  0.0522, -0.0828, -0.0205]])\n",
      "\tw = tensor([[20.5170, 34.2113, 67.5586, 87.9107]], requires_grad=True)\n",
      "\tdb = tensor([-0.0123])\n",
      "\tb = tensor([0.4888], requires_grad=True)\n",
      "\tloss = 0.00447244755923748\n",
      "\n",
      "epoch 400:\n",
      "\tdw = tensor([[ 0.0129,  0.0217, -0.0343, -0.0066]])\n",
      "\tw = tensor([[20.5024, 34.1869, 67.5970, 87.9192]], requires_grad=True)\n",
      "\tdb = tensor([-0.0058])\n",
      "\tb = tensor([0.4948], requires_grad=True)\n",
      "\tloss = 0.0007608865853399038\n",
      "\n",
      "epoch 450:\n",
      "\tdw = tensor([[ 0.0052,  0.0090, -0.0143, -0.0022]])\n",
      "\tw = tensor([[20.4965, 34.1768, 67.6129, 87.9220]], requires_grad=True)\n",
      "\tdb = tensor([-0.0027])\n",
      "\tb = tensor([0.4977], requires_grad=True)\n",
      "\tloss = 0.00013008964015170932\n",
      "\n",
      "epoch 500:\n",
      "\tdw = tensor([[ 0.0021,  0.0037, -0.0059, -0.0008]])\n",
      "\tw = tensor([[20.4940, 34.1727, 67.6195, 87.9230]], requires_grad=True)\n",
      "\tdb = tensor([-0.0012])\n",
      "\tb = tensor([0.4990], requires_grad=True)\n",
      "\tloss = 2.2297730538411997e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "Y = y.unsqueeze(1)\n",
    "\n",
    "layer = Linear(X.size(1), 1)\n",
    "mse = MSELoss()\n",
    "\n",
    "learning_rate = 0.01409\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(1, epochs + 1):\n",
    "    y_pred = layer.forward(X)\n",
    "    loss = mse.forward(y_pred, Y)\n",
    "\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        layer.weights -= learning_rate * layer.weights.grad\n",
    "        layer.biases -= learning_rate * layer.biases.grad\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch {epoch}:\\n'\n",
    "              f'\\tdw = {layer.weights.grad}\\n'\n",
    "              f'\\tw = {layer.weights}\\n'\n",
    "              f'\\tdb = {layer.biases.grad}\\n'\n",
    "              f'\\tb = {layer.biases}\\n'\n",
    "              f'\\tloss = {loss}\\n')\n",
    "\n",
    "    layer.weights.grad.zero_()\n",
    "    layer.biases.grad.zero_()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7"
   },
   "source": [
    "4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd"
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt"
   },
   "source": [
    "4.2.1 Модифицируйте класс `MSELoss` из __3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "L8wjk9iPMQ4x"
   },
   "outputs": [],
   "source": [
    "# использование mean в методах вызывает доверие к имени класса\n",
    "class MSELoss:\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return 2 * (y_pred - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8"
   },
   "source": [
    "4.2.2. Модифицируйте класс `Neuron` из __4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "# а это точно еще нейрон, а не полносвязный слой из одного нейрона с in_features входами?\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.weights = torch.randn(1, in_features)\n",
    "        self.bias = torch.randn(1)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(inputs, self.weights.T) + self.bias\n",
    "\n",
    "    def backward(self, dvalue: torch.Tensor, inputs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return torch.mean(dvalue * inputs, dim=0), torch.mean(dvalue)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx"
   },
   "source": [
    "4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 010 |--------------------------------------------------\n",
      "\tweights = tensor([[22.8361, 35.1808, 56.8774, 77.7312]])\n",
      "\tdw = tensor([  0.0946,  -5.0925, -19.5831, -23.7725])\n",
      "\tbias = tensor([-0.3279])\n",
      "\tdb = -1.1936815977096558\n",
      "\tloss = 273.2061462402344\n",
      "\n",
      "epoch 020 |--------------------------------------------------\n",
      "\tweights = tensor([[21.3865, 35.2819, 65.2723, 86.5805]])\n",
      "\tdw = tensor([ 1.2022,  1.0444, -3.6096, -2.9211])\n",
      "\tbias = tensor([0.2734])\n",
      "\tdb = -0.28787899017333984\n",
      "\tloss = 8.566173553466797\n",
      "\n",
      "epoch 030 |--------------------------------------------------\n",
      "\tweights = tensor([[20.7266, 34.5308, 67.0295, 87.7218]])\n",
      "\tdw = tensor([ 0.3433,  0.4741, -0.8571, -0.4070])\n",
      "\tbias = tensor([0.4255])\n",
      "\tdb = -0.08487050980329514\n",
      "\tloss = 0.4873894155025482\n",
      "\n",
      "epoch 040 |--------------------------------------------------\n",
      "\tweights = tensor([[20.5520, 34.2695, 67.4672, 87.8893]])\n",
      "\tdw = tensor([ 0.0871,  0.1401, -0.2223, -0.0644])\n",
      "\tbias = tensor([0.4750])\n",
      "\tdb = -0.029402075335383415\n",
      "\tloss = 0.03259115293622017\n",
      "\n",
      "epoch 050 |--------------------------------------------------\n",
      "\tweights = tensor([[20.5077, 34.1962, 67.5823, 87.9171]])\n",
      "\tdw = tensor([ 0.0222,  0.0377, -0.0591, -0.0114])\n",
      "\tbias = tensor([0.4920])\n",
      "\tdb = -0.00987914577126503\n",
      "\tloss = 0.0022657406516373158\n",
      "\n",
      "epoch 060 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4964, 34.1767, 67.6130, 87.9222]])\n",
      "\tdw = tensor([ 0.0057,  0.0099, -0.0158, -0.0022])\n",
      "\tbias = tensor([0.4976])\n",
      "\tdb = -0.0031090069096535444\n",
      "\tloss = 0.00015939045988488942\n",
      "\n",
      "epoch 070 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4934, 34.1716, 67.6212, 87.9232]])\n",
      "\tdw = tensor([ 0.0015,  0.0026, -0.0042, -0.0005])\n",
      "\tbias = tensor([0.4993])\n",
      "\tdb = -0.0009288120199926198\n",
      "\tloss = 1.1334587725286838e-05\n",
      "\n",
      "epoch 080 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4926, 34.1703, 67.6234, 87.9234]])\n",
      "\tdw = tensor([ 0.0004,  0.0007, -0.0011, -0.0001])\n",
      "\tbias = tensor([0.4998])\n",
      "\tdb = -0.0002657222794368863\n",
      "\tloss = 8.040325951697014e-07\n",
      "\n",
      "epoch 090 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4924, 34.1699, 67.6240, 87.9235]])\n",
      "\tdw = tensor([ 1.0864e-04,  1.7909e-04, -3.0577e-04, -3.1768e-05])\n",
      "\tbias = tensor([0.4999])\n",
      "\tdb = -7.373333210125566e-05\n",
      "\tloss = 5.807940084423535e-08\n",
      "\n",
      "epoch 100 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4924, 34.1698, 67.6242, 87.9235]])\n",
      "\tdw = tensor([ 2.8378e-05,  4.4850e-05, -7.9438e-05, -2.4782e-05])\n",
      "\tbias = tensor([0.5000])\n",
      "\tdb = -2.0513534764177166e-05\n",
      "\tloss = 4.089672600571248e-09\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[20.4924, 34.1698, 67.6242, 87.9235]]),\n array([20.4923687 , 34.16981149, 67.62424823, 87.9234763 ]))"
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "y_true = y.unsqueeze(1)\n",
    "neuron = Neuron(X.size(1))\n",
    "mse = MSELoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    y_pred = neuron.forward(X)\n",
    "    loss = mse.forward(y_pred, y_true)\n",
    "\n",
    "    dw, db = neuron.backward(mse.backward(y_pred, y_true), X)\n",
    "\n",
    "    neuron.weights -= learning_rate * dw\n",
    "    neuron.bias -= learning_rate * db\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        pretty_train_log(epoch, weights=neuron.weights, dw=dw, bias=neuron.bias, db=db, loss=loss)\n",
    "\n",
    "neuron.weights, coef"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk"
   },
   "source": [
    "4.3 Используя один полносвязный слой и пакетный градиентный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7"
   },
   "source": [
    "4.3.1 Модифицируйте класс `Linear` из __1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "# подозрительно 🧐 похоже на предыдущий нейрон (просто совпадение)\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = torch.randn(out_features, in_features)\n",
    "        self.biases = torch.randn(out_features)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(inputs, self.weights.T) + self.biases\n",
    "\n",
    "    def backward(self, dvalue: torch.Tensor, inputs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return torch.mean(dvalue * inputs, dim=0), torch.mean(dvalue)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt"
   },
   "source": [
    "4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 010 |--------------------------------------------------\n",
      "\tweights = tensor([[22.8361, 35.1808, 56.8774, 77.7312]])\n",
      "\tdw = tensor([  0.0946,  -5.0925, -19.5831, -23.7725])\n",
      "\tbiases = tensor([-0.3279])\n",
      "\tdb = -1.1936815977096558\n",
      "\tloss = 273.2061462402344\n",
      "\n",
      "epoch 020 |--------------------------------------------------\n",
      "\tweights = tensor([[21.3865, 35.2819, 65.2723, 86.5805]])\n",
      "\tdw = tensor([ 1.2022,  1.0444, -3.6096, -2.9211])\n",
      "\tbiases = tensor([0.2734])\n",
      "\tdb = -0.28787899017333984\n",
      "\tloss = 8.566173553466797\n",
      "\n",
      "epoch 030 |--------------------------------------------------\n",
      "\tweights = tensor([[20.7266, 34.5308, 67.0295, 87.7218]])\n",
      "\tdw = tensor([ 0.3433,  0.4741, -0.8571, -0.4070])\n",
      "\tbiases = tensor([0.4255])\n",
      "\tdb = -0.08487050980329514\n",
      "\tloss = 0.4873894155025482\n",
      "\n",
      "epoch 040 |--------------------------------------------------\n",
      "\tweights = tensor([[20.5520, 34.2695, 67.4672, 87.8893]])\n",
      "\tdw = tensor([ 0.0871,  0.1401, -0.2223, -0.0644])\n",
      "\tbiases = tensor([0.4750])\n",
      "\tdb = -0.029402075335383415\n",
      "\tloss = 0.03259115293622017\n",
      "\n",
      "epoch 050 |--------------------------------------------------\n",
      "\tweights = tensor([[20.5077, 34.1962, 67.5823, 87.9171]])\n",
      "\tdw = tensor([ 0.0222,  0.0377, -0.0591, -0.0114])\n",
      "\tbiases = tensor([0.4920])\n",
      "\tdb = -0.00987914577126503\n",
      "\tloss = 0.0022657406516373158\n",
      "\n",
      "epoch 060 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4964, 34.1767, 67.6130, 87.9222]])\n",
      "\tdw = tensor([ 0.0057,  0.0099, -0.0158, -0.0022])\n",
      "\tbiases = tensor([0.4976])\n",
      "\tdb = -0.0031090069096535444\n",
      "\tloss = 0.00015939045988488942\n",
      "\n",
      "epoch 070 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4934, 34.1716, 67.6212, 87.9232]])\n",
      "\tdw = tensor([ 0.0015,  0.0026, -0.0042, -0.0005])\n",
      "\tbiases = tensor([0.4993])\n",
      "\tdb = -0.0009288120199926198\n",
      "\tloss = 1.1334587725286838e-05\n",
      "\n",
      "epoch 080 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4926, 34.1703, 67.6234, 87.9234]])\n",
      "\tdw = tensor([ 0.0004,  0.0007, -0.0011, -0.0001])\n",
      "\tbiases = tensor([0.4998])\n",
      "\tdb = -0.0002657222794368863\n",
      "\tloss = 8.040325951697014e-07\n",
      "\n",
      "epoch 090 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4924, 34.1699, 67.6240, 87.9235]])\n",
      "\tdw = tensor([ 1.0864e-04,  1.7909e-04, -3.0577e-04, -3.1768e-05])\n",
      "\tbiases = tensor([0.4999])\n",
      "\tdb = -7.373333210125566e-05\n",
      "\tloss = 5.807940084423535e-08\n",
      "\n",
      "epoch 100 |--------------------------------------------------\n",
      "\tweights = tensor([[20.4924, 34.1698, 67.6242, 87.9235]])\n",
      "\tdw = tensor([ 2.8378e-05,  4.4850e-05, -7.9438e-05, -2.4782e-05])\n",
      "\tbiases = tensor([0.5000])\n",
      "\tdb = -2.0513534764177166e-05\n",
      "\tloss = 4.089672600571248e-09\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[20.4924, 34.1698, 67.6242, 87.9235]]),\n array([20.4923687 , 34.16981149, 67.62424823, 87.9234763 ]))"
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "y_true = y.unsqueeze(1)\n",
    "layer = Linear(X.size(1), 1)\n",
    "mse = MSELoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    y_pred = layer.forward(X)\n",
    "    loss = mse.forward(y_pred, y_true)\n",
    "\n",
    "    dw, db = layer.backward(mse.backward(y_pred, y_true), X)\n",
    "\n",
    "    layer.weights -= learning_rate * dw\n",
    "    layer.biases -= learning_rate * db\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        pretty_train_log(epoch, weights=layer.weights, dw=dw, biases=layer.biases, db=db, loss=loss)\n",
    "\n",
    "neuron.weights, coef"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Проверка**\n",
    "\n",
    "А pytorch точно правильно работает?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 010 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[22.8361, 35.1808, 56.8774, 77.7312]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([-0.3279], requires_grad=True)\n",
      "\tloss = 273.20623779296875\n",
      "\n",
      "epoch 020 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[21.3865, 35.2819, 65.2723, 86.5805]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.2734], requires_grad=True)\n",
      "\tloss = 8.566173553466797\n",
      "\n",
      "epoch 030 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.7266, 34.5308, 67.0295, 87.7218]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.4255], requires_grad=True)\n",
      "\tloss = 0.4873894155025482\n",
      "\n",
      "epoch 040 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.5520, 34.2695, 67.4672, 87.8893]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.4750], requires_grad=True)\n",
      "\tloss = 0.03259115293622017\n",
      "\n",
      "epoch 050 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.5077, 34.1962, 67.5823, 87.9171]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.4920], requires_grad=True)\n",
      "\tloss = 0.0022657406516373158\n",
      "\n",
      "epoch 060 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.4964, 34.1767, 67.6130, 87.9222]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.4976], requires_grad=True)\n",
      "\tloss = 0.00015939045988488942\n",
      "\n",
      "epoch 070 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.4934, 34.1716, 67.6212, 87.9232]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.4993], requires_grad=True)\n",
      "\tloss = 1.1334587725286838e-05\n",
      "\n",
      "epoch 080 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.4926, 34.1703, 67.6234, 87.9234]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.4998], requires_grad=True)\n",
      "\tloss = 8.040325951697014e-07\n",
      "\n",
      "epoch 090 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.4924, 34.1699, 67.6240, 87.9235]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.4999], requires_grad=True)\n",
      "\tloss = 5.807940084423535e-08\n",
      "\n",
      "epoch 100 |--------------------------------------------------\n",
      "\tweight = Parameter containing:\n",
      "tensor([[20.4924, 34.1698, 67.6242, 87.9235]], requires_grad=True)\n",
      "\tbias = Parameter containing:\n",
      "tensor([0.5000], requires_grad=True)\n",
      "\tloss = 4.089672600571248e-09\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[20.4924, 34.1698, 67.6242, 87.9235]]),\n array([20.4923687 , 34.16981149, 67.62424823, 87.9234763 ]))"
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "weights = torch.randn(1, 4)\n",
    "bias = torch.randn(1)\n",
    "\n",
    "y_true = y.unsqueeze(1)\n",
    "layer = nn.Linear(X.size(1), 1)\n",
    "layer.weight = nn.Parameter(weights, requires_grad=True)\n",
    "layer.bias = nn.Parameter(bias, requires_grad=True)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD([layer.weight, layer.bias], lr=learning_rate)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    y_pred = layer.forward(X)\n",
    "    loss = mse.forward(y_pred, y_true)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        pretty_train_log(epoch, weight=layer.weight, bias=layer.bias, loss=loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "neuron.weights, coef"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN"
   },
   "source": [
    "4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "axUjpPz-SvS1"
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "LXoiNxkpTziV"
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs.clip(min=0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.clone()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "tXhspwW6T44T"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1143161818.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [289]\u001B[1;36m\u001B[0m\n\u001B[1;33m    data_loss =  # <прогон через функцию потерь>\u001B[0m\n\u001B[1;37m                 ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# создание компонентов сети\n",
    "# fc1 = \n",
    "# relu1 = \n",
    "# fc2 = \n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "    # <forward pass>\n",
    "    # fc1 > relu1 > fc2 > loss\n",
    "\n",
    "    data_loss =  # <прогон через функцию потерь>\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'epoch {epoch} mean loss {data_loss}')\n",
    "        ys.append(out)\n",
    "\n",
    "    # <backprop>\n",
    "    # loss > fc2 > relu1 > fc1\n",
    "\n",
    "    # <шаг оптимизации для fc1>\n",
    "\n",
    "    # <шаг оптимизации для fc2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKi0OfoUkwk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "    ax.scatter(X.numpy(), y.numpy(), color=\"orange\")\n",
    "    ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
    "    ax.set_xlim(-1.05, 1.5)\n",
    "    ax.set_ylim(-0.25, 1.25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
