{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 2: Обучение искуственных нейронных сетей в PyTorch\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: SVMakrushin@fa.ru \n",
    "\n",
    "Финансовый универсиет, 2021 г. \n",
    "\n",
    "При подготовке лекции использованы материалы:\n",
    "* ...\n",
    "\n",
    "* v0.5 старое название: TCN20_lNNp2_2_v5\n",
    "* v0.6 18.02.2021 \n",
    "* v0.7 22.02.2021 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделы: <a class=\"anchor\" id=\"разделы\"></a>\n",
    "* [Современные методы обучения нейронной сети и обратное распространение ошибки](#современные-методы)\n",
    "* [Обучение модели нейронной сети, алгоритм обратного распространения ошибки](#обратное-распространение)\n",
    "    * [Проблема обучения модели нейронной сети](#проблема-обучения)\n",
    "    * [Проблема поиска градиента](#проблема-поиска)\n",
    "* [Дифференцируемое программирование и реализация обратного распространения ошибки](#дифференцируемое)\n",
    "    * [Автоматическое дифференциирование в PyTorch](#автоматическое-PyTorch)\n",
    "\n",
    "-\n",
    "\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "\r\n",
       "#cssTableCenter td, th \r\n",
       "{\r\n",
       "    text-align: center; \r\n",
       "    vertical-align: middle;\r\n",
       "}\r\n",
       "\r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v2.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Современные методы обучения нейронной сети и обратное распространение ошибки <a class=\"anchor\" id=\"современные-методы\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Применение тензоров:  прямое распространение сигналов и оценка ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Постановка задачи__ \n",
    "\n",
    "* У нас есть набор данных $D$, состоящий из пар $(\\pmb{x}, \\pmb{y})$, где $\\pmb{x}$ - признаки, а $\\pmb{y}$ - правильный ответ. \n",
    "* Модель сети $f_L$, имеющей $L$ слоев с весами $\\pmb{\\theta}$ (совокупность весов нейронов из всех слоев) на этих данных делает некоторые предсказания $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})$\n",
    "* Задана функция ошибки $E$, которую можно подсчитать на каждом примере: $E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$ (например, это может быть квадрат или модуль отклонения $\\hat{\\pmb{y}}$ от $\\pmb{y}$ в случае регрессии или перекрестная энтропия в случае классификации)\n",
    "* Тогда суммарная ошибка на наборе данных $D$ будет функцией от параметров модели: $E(\\pmb{\\theta})$ и определяется как $E(\\pmb{\\theta})=\\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямое распространение сигналов__\n",
    "\n",
    "* Модель нейронной сети это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев:\n",
    "    * т.е. модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_11.png\" alt=\"многослойный перцептрон с двумя скрытыми слоями\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример модели сети: многослойный перцептрон с двумя скрытыми слоями</b>    \n",
    "</center> \n",
    "\n",
    "* Прямое распространение сигналов по модели (в частности: нейронной сети) реализуется с помощощью __прямого прохода (forward pass)__: входящая информация (вектор $\\pmb{x}$) распространяется через сеть $f_L$ с учетом весов связей $\\pmb{\\theta}$, расчитывается выходной вектор $\\hat{\\pmb{y}}=f_L(\\pmb{x}, \\pmb{\\theta})$ .\n",
    "    * Каждый слой нейронной сети - это последовательно применяемая функция слоя, которая рассчитывается при помощи операций с тензорами.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Последовательность операций с тензормаи используется для расчета результата:\n",
    "    * Прямой проход (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "    * Оценки ошибки $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em class=\"qs\"></em> Почему для реализации модели ИНС используют тензоры?\n",
    "\n",
    "Рассмотрим примеры моделей (веса):\n",
    "* 1 персептрон (4 входа + константа(смещение, baias) ): \n",
    "```python\n",
    "inputs = torch.tensor([1.0, 2.0, 3.0, 2.5])\n",
    "weights = torch.tensor([0.2, 0.8, -0.5, 1.0, 2.0])```\n",
    "* 1 слой персептронов из 3 нейронов (ось 0), каждый с 4 входами и константой (ось 1):\n",
    "```python\n",
    "inputs = torch.tensor([1.0, 2.0, 3.0, 2.5])\n",
    "weights = torch.tensor([[0.2, 0.8, -0.5, 1.0, 2.0],\n",
    "                        [0.5, -0.91, 0.26, -0.5, 3.0],\n",
    "                        [-0.26, -0.27, 0.17, 0.87, 0.5]])\n",
    "```\n",
    "* 2 слоя персептронов из 3 нйронов и из 2 нейронов\n",
    "```python\n",
    "inputs = torch.tensor([1.0, 2.0, 3.0, 2.5])\n",
    "weights_layer1 = torch.tensor([[0.2, 0.8, -0.5, 1.0, 2.0],\n",
    "                               [0.5, -0.91, 0.26, -0.5, 3.0],\n",
    "                               [-0.26, -0.27, 0.17, 0.87, 0.5]])\n",
    "# выход слоя 1 - вход слоя 2\n",
    "weights2_layer2 = torch.tensor([[0.8, 0.5, 1.1, 2.0],\n",
    "                                [0.4, 0.26, -0.4, 3.0],\n",
    "                                [-0.2, 0.27, 0.17, 0.5]])\n",
    "```\n",
    "* набор входных вектров (batch):\n",
    "```python\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0, 2.5],\n",
    "                       [-1.1, 3.0, 2.1, 0.8],\n",
    "                       [-2.0, 1.3, 0.1, -1.8],\n",
    "                       [-1.3, 3.2, 1.1, 0.6]])\n",
    "weights = torch.tensor([[0.2, 0.8, -0.5, 1.0, 2.0],\n",
    "           [0.5, -0.91, 0.26, -0.5, 3.0],\n",
    "           [-0.26, -0.27, 0.17, 0.87, 0.5]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица X: \n",
      "tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])\n",
      "X.size = torch.Size([4, 2])\n",
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "# вариант исходных данны #1 (2й признак всегда равен 0):\n",
    "# X = torch.tensor([[1., 0.],\n",
    "#                   [2., 0.],\n",
    "#                   [3., 0.],\n",
    "#                   [4., 0.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# вариант исходных данны #2 (2й признак используется и существенно больше 1го):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'Матрица X: \\n{X}')\n",
    "print(f'X.size = {X.size()}') \n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "\n",
    "# вариант #1:\n",
    "# w_ans = torch.tensor([2., 0.], dtype=torch.float32)\n",
    "\n",
    "# вариант #2:\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "print(f'w:\\n{w}')\n",
    "\n",
    "# прямое распространение (тут фактически описывается модель):\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Обучение модели нейронной сети, алгоритм обратного распространения ошибки <a class=\"anchor\" id=\"обратное-распространение\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема обучения модели нейронной сети <a class=\"anchor\" id=\"проблема-обучения\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проблема обучения модели нейронной сети: общий взгляд__\n",
    "\n",
    "* <em class=\"nt\"></em> __основная проблема__ это не применение модели к входным данным $\\pmb{x}$ и оцнка ошибки на правильных ответах $\\pmb{y}$, а __обучение модели__ (опредление наилучших параметров модели $\\pmb{\\theta}$). \n",
    "     * В случае нейронной сети обучение сводится к поиску весов слоев сети $\\pmb{\\theta}=(\\pmb{w}_1, \\ldots, \\pmb{w}_L)$, которые в совокупности являются параметрами модели $\\pmb{\\theta}$.\n",
    "\n",
    "* Формально: цель обучения - найти оптимальное значение параметров $\\theta^{*}$, минимизирующих ошибку на обучающией выборке $D$: \n",
    "$$\\theta^{*} = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ E(\\pmb{\\theta}) = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "* Т.е. задача обучения сводится к задаче оптимизации.\n",
    "    * <em class=\"nt\"></em> На самом деле __все сложнее__: хороший результат на $D$ может плохо обобщаться (модель может давать низкое качество на другой выборке из той же генеральной совокупности) - __проблема переобучения__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p1_v1.png\" alt=\"Приниципиальная логика обучения нейронной сети\" style=\"width: 800px;\"/><br/>\n",
    "    <b>Приниципиальная логика обучения нейронной сети</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход и оценка ошибки__\n",
    "\n",
    "* __Прямой проход__ (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 300px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> \n",
    "\n",
    "* __Оценки ошибки__ $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки в общей логике обучения нейронной сети</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача оптимизации__\n",
    "\n",
    "* Задача: корректировка весов сети (параметров модели $\\pmb{\\theta}$) на основе информации об ошибке на обучающих примерах $E(\\hat{\\pmb{y}}, \\pmb{y})$.\n",
    "    * Решение: использовать методы оптимизации, основанные на __методе градиентного спуска__.\n",
    "    \n",
    "\n",
    "* __Метод градиентныого спуска__ - метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента. В нашем случае шаг метода градиентного спуска выглядит следующим образом:\n",
    "$$\\pmb{\\theta}_t = \\pmb{\\theta}_{t-1}-\\gamma\\nabla_\\theta E(\\pmb{\\theta}_{t-1}) = \\pmb{\\theta}_{t-1}-\\gamma \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} \\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "\n",
    "* <em class=\"nt\"></em> Выполнение на каждом шаге градиентого спуска суммирование по всем $(\\pmb{x}, \\pmb{y}) \\in D$ __обычно слшиком неэффективно__\n",
    "\n",
    "\n",
    "* Для выпуклых функций __задача локальной оптимизации__ - найти локальный минимум (максимум) автоматически превращается в __задачу глобальной оптимизации__ - найти точку, в которой достигается наименьшее (наибольшее) значение функции, то есть самую низкую (высокую) точку среди всех.\n",
    "* Оптимизировать веса одного перцептрона - выпуклая задача, но __для большой нейронной сети  целевая функция не является выпуклой__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_15.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 500px;\"/><br/>\n",
    "    <b>Пример работы градиентного спуска для функции двух переменных</b>    \n",
    "</center>\n",
    "\n",
    "* У нейронных сетей функция ошибки может задавать __очень сложный ландшафт__ с огромным числом локальных максимумов и минимумов. Это свойство необходимо для обеспечения __выразительности нейронных сетей__, позволяющей им решать так много разных задач.\n",
    "\n",
    "\n",
    "* <em class=\"nt\"></em> для использования методов, основанных на методе градиентного спуска __необходимо знать градиент функции потерь по параметрам модели__: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$. Этот градиент определяет вектор (\"направление\") изменения параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "print(f'w:\\n{w}')\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "# градиент: \n",
    "# рассчитан аналитически по модели и функции потерь:\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2 * (w*x - y) * x\n",
    "def gradient(x, y, y_pred):   \n",
    "#     print(f'''y = {y},\n",
    "#     y_pred = {y_pred},\n",
    "#     (2* (y_pred - y)).unsqueeze(1) = {(2* (y_pred - y)).unsqueeze(1)},\n",
    "#     x = {x},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x) = {((2* (y_pred - y)).unsqueeze(1) * x)},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0) = {((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)}''')\n",
    "    return ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0013\n",
    "\n",
    "# predict = forward pass\n",
    "y_pred = forward(X)\n",
    "\n",
    "# loss\n",
    "l = loss(Y, y_pred)\n",
    "\n",
    "# calculate gradients\n",
    "dw = gradient(X, Y, y_pred)\n",
    "\n",
    "# update weights\n",
    "w -= learning_rate * dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: w = tensor([0.04740, 0.08853]), y_pred = tensor([88.56900, 66.63800, 44.70700, 22.77600]), loss = 901.66833496\n",
      "gradient = tensor([  93.53500, 1631.90002])\n",
      "epoch 5: w = tensor([0.27269, 1.96004]), y_pred = tensor([9.90674, 7.60272, 5.29869, 2.99467]), loss = 595.12457275\n",
      "gradient = tensor([ -103.50652, -1319.86426])\n",
      "epoch 10: w = tensor([0.26484, 0.43254]), y_pred = tensor([73.32084, 55.41520, 37.50956, 19.60392]), loss = 393.66198730\n",
      "gradient = tensor([  57.54780, 1070.75989])\n",
      "epoch 15: w = tensor([0.43660, 1.65862]), y_pred = tensor([21.67870, 16.68858, 11.69846,  6.70834]), loss = 261.16906738\n",
      "gradient = tensor([ -71.50771, -865.57104])\n",
      "epoch 20: w = tensor([0.45417, 0.65508]), y_pred = tensor([63.23888, 48.05265, 32.86644, 17.68021]), loss = 173.95428467\n",
      "gradient = tensor([ 34.33218, 702.63275])\n",
      "epoch 25: w = tensor([0.58835, 1.45793]), y_pred = tensor([29.32578, 22.64829, 15.97079,  9.29329]), loss = 116.47224426\n",
      "gradient = tensor([ -50.14605, -567.58569])\n",
      "epoch 30: w = tensor([0.62025, 0.79825]), y_pred = tensor([56.55405, 43.22246, 29.89087, 16.55927]), loss = 78.52218628\n",
      "gradient = tensor([ 19.45434, 461.12296])\n",
      "epoch 35: w = tensor([0.72757, 1.32362]), y_pred = tensor([34.27473, 26.55733, 18.83993, 11.12253]), loss = 53.40980911\n",
      "gradient = tensor([ -35.80034, -372.13342])\n",
      "epoch 40: w = tensor([0.76677, 0.88965]), y_pred = tensor([52.10501, 40.05349, 28.00197, 15.95045]), loss = 36.74113083\n",
      "gradient = tensor([ 10.00985, 302.67453])\n",
      "epoch 45: w = tensor([0.85449, 1.23312]), y_pred = tensor([37.46066, 29.12121, 20.78176, 12.44231]), loss = 25.63151741\n",
      "gradient = tensor([ -26.09119, -243.93945])\n",
      "epoch 50: w = tensor([0.89661, 0.94735]), y_pred = tensor([49.12919, 37.97432, 26.81944, 15.66457]), loss = 18.18657875\n",
      "gradient = tensor([  4.09722, 198.71594])\n",
      "epoch 55: w = tensor([0.96969, 1.17162]), y_pred = tensor([39.49628, 30.80272, 22.10915, 13.41558]), loss = 13.16171455\n",
      "gradient = tensor([ -19.45426, -159.86418])\n",
      "epoch 60: w = tensor([1.01204, 0.98317]), y_pred = tensor([47.12565, 36.61006, 26.09446, 15.57887]), loss = 9.73888302\n",
      "gradient = tensor([  0.47232, 130.50296])\n",
      "epoch 65: w = tensor([1.07392, 1.12935]), y_pred = tensor([40.78293, 31.90545, 23.02797, 14.15049]), loss = 7.38002539\n",
      "gradient = tensor([ -14.86015, -104.72755])\n",
      "epoch 70: w = tensor([1.11491, 1.00485]), y_pred = tensor([45.76522, 35.71482, 25.66442, 15.61402]), loss = 5.73075104\n",
      "gradient = tensor([-1.67789, 85.74104])\n",
      "epoch 75: w = tensor([1.16801, 1.09990]), y_pred = tensor([41.58322, 32.62852, 23.67382, 14.71913]), loss = 4.55746794\n",
      "gradient = tensor([-11.63087, -68.57382])\n",
      "epoch 80: w = tensor([1.20674, 1.01744]), y_pred = tensor([44.83142, 35.12730, 25.42318, 15.71907]), loss = 3.70577574\n",
      "gradient = tensor([-2.88408, 56.36501])\n",
      "epoch 85: w = tensor([1.25279, 1.07904]), y_pred = tensor([42.06898, 33.10259, 24.13619, 15.16980]), loss = 3.07348490\n",
      "gradient = tensor([ -9.31904, -44.87057])\n",
      "epoch 90: w = tensor([1.28883, 1.02423]), y_pred = tensor([44.18171, 34.74165, 25.30159, 15.86153]), loss = 2.59267926\n",
      "gradient = tensor([-3.49206, 37.08232])\n",
      "epoch 95: w = tensor([1.32912, 1.06396]), y_pred = tensor([42.35246, 33.41334, 24.47422, 15.53510]), loss = 2.21802735\n",
      "gradient = tensor([ -7.62889, -29.33311])\n",
      "epoch 100: w = tensor([1.36229, 1.02737]), y_pred = tensor([43.72217, 34.48844, 25.25471, 16.02098]), loss = 1.91910768\n",
      "gradient = tensor([-3.72645, 24.42206])\n",
      "epoch 105: w = tensor([1.39775, 1.05283]), y_pred = tensor([42.50684, 33.61698, 24.72712, 15.83726]), loss = 1.67532277\n",
      "gradient = tensor([ -6.36442, -19.15111])\n",
      "epoch 110: w = tensor([1.42808, 1.02825]), y_pred = tensor([43.39080, 34.32213, 25.25346, 16.18479]), loss = 1.47260582\n",
      "gradient = tensor([-3.73270, 16.10661])\n",
      "epoch 115: w = tensor([1.45944, 1.04441]), y_pred = tensor([42.57979, 33.75038, 24.92097, 16.09155]), loss = 1.30123818\n",
      "gradient = tensor([ -5.39517, -12.48116])\n",
      "epoch 120: w = tensor([1.48701, 1.02777]), y_pred = tensor([43.14663, 34.21288, 25.27913, 16.34538]), loss = 1.15437365\n",
      "gradient = tensor([-3.60436, 10.64381])\n",
      "epoch 125: w = tensor([1.51486, 1.03788]), y_pred = tensor([42.60222, 33.83771, 25.07320, 16.30869]), loss = 1.02712226\n",
      "gradient = tensor([-4.63400, -8.11450])\n",
      "epoch 130: w = tensor([1.53984, 1.02651]), y_pred = tensor([42.96235, 34.14103, 25.31971, 16.49839]), loss = 0.91591144\n",
      "gradient = tensor([-3.40145,  7.05159])\n",
      "epoch 135: w = tensor([1.56463, 1.03270]), y_pred = tensor([42.59415, 33.89486, 25.19558, 16.49629]), loss = 0.81807786\n",
      "gradient = tensor([-4.02212, -5.25692])\n",
      "epoch 140: w = tensor([1.58719, 1.02483]), y_pred = tensor([42.81988, 34.09376, 25.36765, 16.64153]), loss = 0.73157334\n",
      "gradient = tensor([-3.16176,  4.68811])\n",
      "epoch 145: w = tensor([1.60931, 1.02850]), y_pred = tensor([42.56838, 33.93221, 25.29604, 16.65986]), loss = 0.65479702\n",
      "gradient = tensor([-3.51982, -3.38955])\n",
      "epoch 150: w = tensor([1.62966, 1.02296]), y_pred = tensor([42.70705, 34.06263, 25.41822, 16.77381]), loss = 0.58646035\n",
      "gradient = tensor([-2.90889,  3.13171])\n",
      "epoch 155: w = tensor([1.64943, 1.02502]), y_pred = tensor([42.53312, 33.95658, 25.38004, 16.80351]), loss = 0.52550709\n",
      "gradient = tensor([-3.09977, -2.17097])\n",
      "epoch 160: w = tensor([1.66775, 1.02105]), y_pred = tensor([42.61564, 34.04210, 25.46856, 16.89502]), loss = 0.47105190\n",
      "gradient = tensor([-2.65721,  2.10501])\n",
      "epoch 165: w = tensor([1.68543, 1.02208]), y_pred = tensor([42.49351, 33.97246, 25.45141, 16.93036]), loss = 0.42234868\n",
      "gradient = tensor([-2.74295, -1.37701])\n",
      "epoch 170: w = tensor([1.70191, 1.01918]), y_pred = tensor([42.54008, 34.02853, 25.51697, 17.00542]), loss = 0.37875193\n",
      "gradient = tensor([-2.41514,  1.42634])\n",
      "epoch 175: w = tensor([1.71774, 1.01957]), y_pred = tensor([42.45276, 33.98278, 25.51281, 17.04283]), loss = 0.33970279\n",
      "gradient = tensor([-2.43596, -0.86082])\n",
      "epoch 180: w = tensor([1.73256, 1.01740]), y_pred = tensor([42.47651, 34.01952, 25.56254, 17.10556]), loss = 0.30470935\n",
      "gradient = tensor([-2.18729,  0.97625])\n",
      "epoch 185: w = tensor([1.74673, 1.01741]), y_pred = tensor([42.41277, 33.98947, 25.56617, 17.14287]), loss = 0.27334169\n",
      "gradient = tensor([-2.16915, -0.52649])\n",
      "epoch 190: w = tensor([1.76005, 1.01574]), y_pred = tensor([42.42224, 34.01353, 25.60482, 17.19612]), loss = 0.24521472\n",
      "gradient = tensor([-1.97588,  0.67659])\n",
      "epoch 195: w = tensor([1.77275, 1.01551]), y_pred = tensor([42.37465, 33.99379, 25.61292, 17.23206]), loss = 0.21999198\n",
      "gradient = tensor([-1.93540, -0.31084])\n",
      "epoch 200: w = tensor([1.78471, 1.01421]), y_pred = tensor([42.37537, 34.00953, 25.64368, 17.27784]), loss = 0.19736950\n",
      "gradient = tensor([-1.78160,  0.47626])\n",
      "epoch 205: w = tensor([1.79610, 1.01385]), y_pred = tensor([42.33895, 33.99653, 25.65411, 17.31169]), loss = 0.17707650\n",
      "gradient = tensor([-1.72943, -0.17331])\n",
      "epoch 210: w = tensor([1.80684, 1.01280]), y_pred = tensor([42.33454, 34.00685, 25.67916, 17.35147]), loss = 0.15887275\n",
      "gradient = tensor([-1.60420,  0.34241])\n",
      "epoch 215: w = tensor([1.81705, 1.01239]), y_pred = tensor([42.30596, 33.99827, 25.69058, 17.38290]), loss = 0.14254203\n",
      "gradient = tensor([-1.54708, -0.08644])\n",
      "epoch 220: w = tensor([1.82669, 1.01152]), y_pred = tensor([42.29866, 34.00502, 25.71138, 17.41773]), loss = 0.12789123\n",
      "gradient = tensor([-1.44312,  0.25101])\n",
      "epoch 225: w = tensor([1.83585, 1.01108]), y_pred = tensor([42.27573, 33.99937, 25.72300, 17.44662]), loss = 0.11474628\n",
      "gradient = tensor([-1.38502, -0.03174])\n",
      "epoch 230: w = tensor([1.84451, 1.01036]), y_pred = tensor([42.26698, 34.00375, 25.74053, 17.47731]), loss = 0.10295333\n",
      "gradient = tensor([-1.29733,  0.18772])\n",
      "epoch 235: w = tensor([1.85272, 1.00993]), y_pred = tensor([42.24820, 34.00003, 25.75186, 17.50370]), loss = 0.09237251\n",
      "gradient = tensor([-1.24068,  0.00149])\n",
      "epoch 240: w = tensor([1.86049, 1.00931]), y_pred = tensor([42.23890, 34.00289, 25.76688, 17.53086]), loss = 0.08287939\n",
      "gradient = tensor([-1.16562,  0.14438])\n",
      "epoch 245: w = tensor([1.86785, 1.00889]), y_pred = tensor([42.22322, 34.00042, 25.77763, 17.55483]), loss = 0.07436194\n",
      "gradient = tensor([-1.11186,  0.02109])\n",
      "epoch 250: w = tensor([1.87482, 1.00836]), y_pred = tensor([42.21392, 34.00227, 25.79062, 17.57896]), loss = 0.06672009\n",
      "gradient = tensor([-1.04691,  0.11354])\n",
      "epoch 255: w = tensor([1.88143, 1.00797]), y_pred = tensor([42.20061, 34.00063, 25.80065, 17.60067]), loss = 0.05986354\n",
      "gradient = tensor([-0.99676,  0.03147])\n",
      "epoch 260: w = tensor([1.88769, 1.00751]), y_pred = tensor([42.19168, 34.00184, 25.81200, 17.62217]), loss = 0.05371136\n",
      "gradient = tensor([-0.93999,  0.09197])\n",
      "epoch 265: w = tensor([1.89362, 1.00715]), y_pred = tensor([42.18022, 34.00073, 25.82125, 17.64176]), loss = 0.04819155\n",
      "gradient = tensor([-0.89376,  0.03662])\n",
      "epoch 270: w = tensor([1.89923, 1.00674]), y_pred = tensor([42.17178, 34.00150, 25.83122, 17.66094]), loss = 0.04323899\n",
      "gradient = tensor([-0.84388,  0.07518])\n",
      "epoch 275: w = tensor([1.90455, 1.00641]), y_pred = tensor([42.16184, 34.00077, 25.83969, 17.67861]), loss = 0.03879556\n",
      "gradient = tensor([-0.80155,  0.03831])\n",
      "epoch 280: w = tensor([1.90959, 1.00605]), y_pred = tensor([42.15401, 34.00126, 25.84851, 17.69576]), loss = 0.03480874\n",
      "gradient = tensor([-0.75745,  0.06312])\n",
      "epoch 285: w = tensor([1.91436, 1.00575]), y_pred = tensor([42.14531, 34.00076, 25.85621, 17.71167]), loss = 0.03123153\n",
      "gradient = tensor([-0.71893,  0.03805])\n",
      "epoch 290: w = tensor([1.91888, 1.00543]), y_pred = tensor([42.13810, 34.00107, 25.86404, 17.72700]), loss = 0.02802203\n",
      "gradient = tensor([-0.67982,  0.05352])\n",
      "epoch 295: w = tensor([1.92316, 1.00516]), y_pred = tensor([42.13045, 34.00074, 25.87103, 17.74131]), loss = 0.02514252\n",
      "gradient = tensor([-0.64487,  0.03681])\n",
      "epoch 300: w = tensor([1.92721, 1.00488]), y_pred = tensor([42.12387, 34.00092, 25.87798, 17.75504]), loss = 0.02255898\n",
      "gradient = tensor([-0.61008,  0.04623])\n",
      "epoch 305: w = tensor([1.93106, 1.00462]), y_pred = tensor([42.11708, 34.00069, 25.88430, 17.76791]), loss = 0.02024037\n",
      "gradient = tensor([-0.57849,  0.03466])\n",
      "epoch 310: w = tensor([1.93469, 1.00438]), y_pred = tensor([42.11110, 34.00080, 25.89050, 17.78021]), loss = 0.01816060\n",
      "gradient = tensor([-0.54748,  0.04006])\n",
      "epoch 315: w = tensor([1.93814, 1.00415]), y_pred = tensor([42.10508, 34.00064, 25.89621, 17.79177]), loss = 0.01629427\n",
      "gradient = tensor([-0.51897,  0.03223])\n",
      "epoch 320: w = tensor([1.94140, 1.00393]), y_pred = tensor([42.09965, 34.00069, 25.90174, 17.80278]), loss = 0.01461987\n",
      "gradient = tensor([-0.49131,  0.03468])\n",
      "epoch 325: w = tensor([1.94450, 1.00372]), y_pred = tensor([42.09431, 34.00060, 25.90688, 17.81317]), loss = 0.01311745\n",
      "gradient = tensor([-0.46558,  0.02975])\n",
      "epoch 330: w = tensor([1.94743, 1.00352]), y_pred = tensor([42.08939, 34.00061, 25.91183, 17.82305]), loss = 0.01176946\n",
      "gradient = tensor([-0.44085,  0.03056])\n",
      "epoch 335: w = tensor([1.95020, 1.00334]), y_pred = tensor([42.08464, 34.00055, 25.91647, 17.83238]), loss = 0.01055982\n",
      "gradient = tensor([-0.41767,  0.02766])\n",
      "epoch 340: w = tensor([1.95283, 1.00316]), y_pred = tensor([42.08020, 34.00054, 25.92089, 17.84123]), loss = 0.00947476\n",
      "gradient = tensor([-0.39557,  0.02715])\n",
      "epoch 345: w = tensor([1.95532, 1.00300]), y_pred = tensor([42.07594, 34.00049, 25.92505, 17.84960]), loss = 0.00850094\n",
      "gradient = tensor([-0.37476,  0.02467])\n",
      "epoch 350: w = tensor([1.95768, 1.00284]), y_pred = tensor([42.07196, 34.00048, 25.92902, 17.85755]), loss = 0.00762747\n",
      "gradient = tensor([-0.35492,  0.02429])\n",
      "epoch 355: w = tensor([1.95991, 1.00269]), y_pred = tensor([42.06813, 34.00044, 25.93275, 17.86506]), loss = 0.00684364\n",
      "gradient = tensor([-0.33625,  0.02212])\n",
      "epoch 360: w = tensor([1.96203, 1.00255]), y_pred = tensor([42.06456, 34.00043, 25.93631, 17.87218]), loss = 0.00614040\n",
      "gradient = tensor([-0.31845,  0.02184])\n",
      "epoch 365: w = tensor([1.96403, 1.00241]), y_pred = tensor([42.06114, 34.00040, 25.93966, 17.87893]), loss = 0.00550942\n",
      "gradient = tensor([-0.30169,  0.02003])\n",
      "epoch 370: w = tensor([1.96593, 1.00228]), y_pred = tensor([42.05793, 34.00039, 25.94285, 17.88532]), loss = 0.00494326\n",
      "gradient = tensor([-0.28573,  0.01949])\n",
      "epoch 375: w = tensor([1.96773, 1.00216]), y_pred = tensor([42.05485, 34.00036, 25.94586, 17.89137]), loss = 0.00443522\n",
      "gradient = tensor([-0.27069,  0.01786])\n",
      "epoch 380: w = tensor([1.96943, 1.00205]), y_pred = tensor([42.05197, 34.00035, 25.94873, 17.89710]), loss = 0.00397941\n",
      "gradient = tensor([-0.25636,  0.01748])\n",
      "epoch 385: w = tensor([1.97104, 1.00194]), y_pred = tensor([42.04921, 34.00032, 25.95142, 17.90253]), loss = 0.00357040\n",
      "gradient = tensor([-0.24288,  0.01583])\n",
      "epoch 390: w = tensor([1.97257, 1.00184]), y_pred = tensor([42.04663, 34.00032, 25.95400, 17.90768]), loss = 0.00320348\n",
      "gradient = tensor([-0.23001,  0.01583])\n",
      "epoch 395: w = tensor([1.97402, 1.00174]), y_pred = tensor([42.04416, 34.00029, 25.95642, 17.91255]), loss = 0.00287427\n",
      "gradient = tensor([-0.21791,  0.01431])\n",
      "epoch 400: w = tensor([1.97539, 1.00165]), y_pred = tensor([42.04183, 34.00027, 25.95872, 17.91716]), loss = 0.00257894\n",
      "gradient = tensor([-0.20640,  0.01379])\n",
      "epoch 405: w = tensor([1.97669, 1.00156]), y_pred = tensor([42.03962, 34.00026, 25.96090, 17.92154]), loss = 0.00231390\n",
      "gradient = tensor([-0.19551,  0.01308])\n",
      "epoch 410: w = tensor([1.97792, 1.00148]), y_pred = tensor([42.03754, 34.00025, 25.96296, 17.92568]), loss = 0.00207613\n",
      "gradient = tensor([-0.18518,  0.01253])\n",
      "epoch 415: w = tensor([1.97908, 1.00140]), y_pred = tensor([42.03555, 34.00024, 25.96492, 17.92960]), loss = 0.00186272\n",
      "gradient = tensor([-0.17542,  0.01169])\n",
      "epoch 420: w = tensor([1.98019, 1.00133]), y_pred = tensor([42.03368, 34.00022, 25.96677, 17.93332]), loss = 0.00167133\n",
      "gradient = tensor([-0.16616,  0.01110])\n",
      "epoch 425: w = tensor([1.98123, 1.00126]), y_pred = tensor([42.03189, 34.00021, 25.96852, 17.93683]), loss = 0.00149959\n",
      "gradient = tensor([-0.15740,  0.01036])\n",
      "epoch 430: w = tensor([1.98222, 1.00119]), y_pred = tensor([42.03022, 34.00020, 25.97018, 17.94017]), loss = 0.00134546\n",
      "gradient = tensor([-0.14908,  0.01004])\n",
      "epoch 435: w = tensor([1.98316, 1.00113]), y_pred = tensor([42.02862, 34.00019, 25.97176, 17.94333]), loss = 0.00120723\n",
      "gradient = tensor([-0.14121,  0.00950])\n",
      "epoch 440: w = tensor([1.98405, 1.00107]), y_pred = tensor([42.02710, 34.00017, 25.97324, 17.94631]), loss = 0.00108313\n",
      "gradient = tensor([-0.13379,  0.00857])\n",
      "epoch 445: w = tensor([1.98489, 1.00101]), y_pred = tensor([42.02570, 34.00018, 25.97467, 17.94915]), loss = 0.00097188\n",
      "gradient = tensor([-0.12666,  0.00919])\n",
      "epoch 450: w = tensor([1.98569, 1.00096]), y_pred = tensor([42.02432, 34.00016, 25.97600, 17.95183]), loss = 0.00087199\n",
      "gradient = tensor([-0.12002,  0.00792])\n",
      "epoch 455: w = tensor([1.98645, 1.00091]), y_pred = tensor([42.02304, 34.00015, 25.97726, 17.95438]), loss = 0.00078233\n",
      "gradient = tensor([-0.11368,  0.00763])\n",
      "epoch 460: w = tensor([1.98716, 1.00086]), y_pred = tensor([42.02184, 34.00015, 25.97847, 17.95679]), loss = 0.00070199\n",
      "gradient = tensor([-0.10766,  0.00760])\n",
      "epoch 465: w = tensor([1.98784, 1.00082]), y_pred = tensor([42.02066, 34.00012, 25.97959, 17.95906]), loss = 0.00062981\n",
      "gradient = tensor([-0.10204,  0.00618])\n",
      "epoch 470: w = tensor([1.98848, 1.00077]), y_pred = tensor([42.01960, 34.00014, 25.98069, 17.96123]), loss = 0.00056509\n",
      "gradient = tensor([-0.09657,  0.00718])\n",
      "epoch 475: w = tensor([1.98909, 1.00073]), y_pred = tensor([42.01853, 34.00011, 25.98169, 17.96327]), loss = 0.00050698\n",
      "gradient = tensor([-0.09155,  0.00553])\n",
      "epoch 480: w = tensor([1.98966, 1.00069]), y_pred = tensor([42.01758, 34.00012, 25.98267, 17.96521]), loss = 0.00045489\n",
      "gradient = tensor([-0.08665,  0.00624])\n",
      "epoch 485: w = tensor([1.99021, 1.00066]), y_pred = tensor([42.01663, 34.00010, 25.98357, 17.96704]), loss = 0.00040816\n",
      "gradient = tensor([-0.08213,  0.00514])\n",
      "epoch 490: w = tensor([1.99073, 1.00062]), y_pred = tensor([42.01578, 34.00011, 25.98445, 17.96879]), loss = 0.00036622\n",
      "gradient = tensor([-0.07774,  0.00572])\n",
      "epoch 495: w = tensor([1.99122, 1.00059]), y_pred = tensor([42.01493, 34.00010, 25.98526, 17.97043]), loss = 0.00032858\n",
      "gradient = tensor([-0.07368,  0.00476])\n",
      "epoch 500: w = tensor([1.99168, 1.00056]), y_pred = tensor([42.01414, 34.00009, 25.98604, 17.97199]), loss = 0.00029482\n",
      "gradient = tensor([-0.06980,  0.00442])\n",
      "epoch 505: w = tensor([1.99212, 1.00053]), y_pred = tensor([42.01340, 34.00010, 25.98678, 17.97347]), loss = 0.00026450\n",
      "gradient = tensor([-0.06609,  0.00463])\n",
      "epoch 510: w = tensor([1.99253, 1.00050]), y_pred = tensor([42.01269, 34.00008, 25.98748, 17.97487]), loss = 0.00023735\n",
      "gradient = tensor([-0.06262,  0.00406])\n",
      "epoch 515: w = tensor([1.99293, 1.00047]), y_pred = tensor([42.01202, 34.00008, 25.98814, 17.97620]), loss = 0.00021294\n",
      "gradient = tensor([-0.05930,  0.00407])\n",
      "epoch 520: w = tensor([1.99330, 1.00045]), y_pred = tensor([42.01139, 34.00008, 25.98876, 17.97745]), loss = 0.00019107\n",
      "gradient = tensor([-0.05618,  0.00379])\n",
      "epoch 525: w = tensor([1.99365, 1.00043]), y_pred = tensor([42.01079, 34.00007, 25.98936, 17.97864]), loss = 0.00017144\n",
      "gradient = tensor([-0.05321,  0.00363])\n",
      "epoch 530: w = tensor([1.99399, 1.00040]), y_pred = tensor([42.01021, 34.00006, 25.98992, 17.97977]), loss = 0.00015379\n",
      "gradient = tensor([-0.05041,  0.00324])\n",
      "epoch 535: w = tensor([1.99431, 1.00038]), y_pred = tensor([42.00968, 34.00006, 25.99045, 17.98084]), loss = 0.00013800\n",
      "gradient = tensor([-0.04774,  0.00324])\n",
      "epoch 540: w = tensor([1.99461, 1.00036]), y_pred = tensor([42.00916, 34.00006, 25.99095, 17.98185]), loss = 0.00012383\n",
      "gradient = tensor([-0.04524,  0.00289])\n",
      "epoch 545: w = tensor([1.99489, 1.00034]), y_pred = tensor([42.00868, 34.00005, 25.99143, 17.98281]), loss = 0.00011109\n",
      "gradient = tensor([-0.04285,  0.00271])\n",
      "epoch 550: w = tensor([1.99516, 1.00032]), y_pred = tensor([42.00823, 34.00006, 25.99189, 17.98372]), loss = 0.00009969\n",
      "gradient = tensor([-0.04057,  0.00286])\n",
      "epoch 555: w = tensor([1.99542, 1.00031]), y_pred = tensor([42.00779, 34.00005, 25.99231, 17.98457]), loss = 0.00008945\n",
      "gradient = tensor([-0.03843,  0.00267])\n",
      "epoch 560: w = tensor([1.99566, 1.00029]), y_pred = tensor([42.00737, 34.00004, 25.99271, 17.98538]), loss = 0.00008027\n",
      "gradient = tensor([-0.03644,  0.00207])\n",
      "epoch 565: w = tensor([1.99589, 1.00028]), y_pred = tensor([42.00700, 34.00006, 25.99311, 17.98616]), loss = 0.00007200\n",
      "gradient = tensor([-0.03445,  0.00285])\n",
      "epoch 570: w = tensor([1.99610, 1.00026]), y_pred = tensor([42.00661, 34.00004, 25.99346, 17.98689]), loss = 0.00006461\n",
      "gradient = tensor([-0.03269,  0.00194])\n",
      "epoch 575: w = tensor([1.99631, 1.00025]), y_pred = tensor([42.00628, 34.00005, 25.99381, 17.98758]), loss = 0.00005797\n",
      "gradient = tensor([-0.03093,  0.00233])\n",
      "epoch 580: w = tensor([1.99651, 1.00023]), y_pred = tensor([42.00594, 34.00004, 25.99413, 17.98824]), loss = 0.00005201\n",
      "gradient = tensor([-0.02932,  0.00181])\n",
      "epoch 585: w = tensor([1.99669, 1.00022]), y_pred = tensor([42.00563, 34.00004, 25.99445, 17.98886]), loss = 0.00004667\n",
      "gradient = tensor([-0.02777,  0.00185])\n",
      "epoch 590: w = tensor([1.99686, 1.00021]), y_pred = tensor([42.00534, 34.00004, 25.99474, 17.98945]), loss = 0.00004187\n",
      "gradient = tensor([-0.02629,  0.00197])\n",
      "epoch 595: w = tensor([1.99703, 1.00020]), y_pred = tensor([42.00505, 34.00003, 25.99502, 17.99000]), loss = 0.00003757\n",
      "gradient = tensor([-0.02492,  0.00156])\n",
      "epoch 600: w = tensor([1.99719, 1.00019]), y_pred = tensor([42.00478, 34.00003, 25.99528, 17.99053]), loss = 0.00003371\n",
      "gradient = tensor([-0.02359,  0.00165])\n",
      "epoch 605: w = tensor([1.99733, 1.00018]), y_pred = tensor([42.00453, 34.00003, 25.99553, 17.99103]), loss = 0.00003025\n",
      "gradient = tensor([-0.02235,  0.00154])\n",
      "epoch 610: w = tensor([1.99748, 1.00017]), y_pred = tensor([42.00428, 34.00002, 25.99576, 17.99150]), loss = 0.00002714\n",
      "gradient = tensor([-0.02120,  0.00098])\n",
      "epoch 615: w = tensor([1.99761, 1.00016]), y_pred = tensor([42.00407, 34.00004, 25.99599, 17.99195]), loss = 0.00002435\n",
      "gradient = tensor([-0.02003,  0.00176])\n",
      "epoch 620: w = tensor([1.99773, 1.00015]), y_pred = tensor([42.00385, 34.00002, 25.99620, 17.99237]), loss = 0.00002185\n",
      "gradient = tensor([-0.01901,  0.00103])\n",
      "epoch 625: w = tensor([1.99785, 1.00014]), y_pred = tensor([42.00365, 34.00003, 25.99640, 17.99278]), loss = 0.00001960\n",
      "gradient = tensor([-0.01799,  0.00134])\n",
      "epoch 630: w = tensor([1.99797, 1.00014]), y_pred = tensor([42.00345, 34.00002, 25.99659, 17.99316]), loss = 0.00001759\n",
      "gradient = tensor([-0.01705,  0.00108])\n",
      "epoch 635: w = tensor([1.99807, 1.00013]), y_pred = tensor([42.00328, 34.00002, 25.99677, 17.99352]), loss = 0.00001578\n",
      "gradient = tensor([-0.01614,  0.00121])\n",
      "epoch 640: w = tensor([1.99818, 1.00012]), y_pred = tensor([42.00311, 34.00002, 25.99694, 17.99386]), loss = 0.00001416\n",
      "gradient = tensor([-0.01528,  0.00129])\n",
      "epoch 645: w = tensor([1.99827, 1.00012]), y_pred = tensor([42.00292, 34.00001, 25.99710, 17.99418]), loss = 0.00001270\n",
      "gradient = tensor([-0.01452,  0.00042])\n",
      "epoch 650: w = tensor([1.99836, 1.00011]), y_pred = tensor([42.00279, 34.00003, 25.99726, 17.99450]), loss = 0.00001139\n",
      "gradient = tensor([-0.01370,  0.00126])\n",
      "epoch 655: w = tensor([1.99845, 1.00010]), y_pred = tensor([42.00263, 34.00002, 25.99740, 17.99478]), loss = 0.00001023\n",
      "gradient = tensor([-0.01301,  0.00069])\n",
      "epoch 660: w = tensor([1.99853, 1.00010]), y_pred = tensor([42.00250, 34.00002, 25.99754, 17.99506]), loss = 0.00000918\n",
      "gradient = tensor([-0.01230,  0.00109])\n",
      "epoch 665: w = tensor([1.99861, 1.00009]), y_pred = tensor([42.00236, 34.00001, 25.99767, 17.99532]), loss = 0.00000823\n",
      "gradient = tensor([-0.01167,  0.00065])\n",
      "epoch 670: w = tensor([1.99868, 1.00009]), y_pred = tensor([42.00225, 34.00002, 25.99780, 17.99557]), loss = 0.00000739\n",
      "gradient = tensor([-0.01102,  0.00115])\n",
      "epoch 675: w = tensor([1.99875, 1.00008]), y_pred = tensor([42.00210, 34.00000, 25.99790, 17.99580]), loss = 0.00000663\n",
      "gradient = tensor([-1.05057e-02,  5.72205e-05])\n",
      "epoch 680: w = tensor([1.99882, 1.00008]), y_pred = tensor([42.00203, 34.00003, 25.99803, 17.99603]), loss = 0.00000595\n",
      "gradient = tensor([-0.00987,  0.00130])\n",
      "epoch 685: w = tensor([1.99888, 1.00008]), y_pred = tensor([42.00189, 34.00000, 25.99812, 17.99623]), loss = 0.00000534\n",
      "gradient = tensor([-0.00942,  0.00011])\n",
      "epoch 690: w = tensor([1.99894, 1.00007]), y_pred = tensor([42.00181, 34.00002, 25.99823, 17.99643]), loss = 0.00000479\n",
      "gradient = tensor([-0.00887,  0.00095])\n",
      "epoch 695: w = tensor([1.99900, 1.00007]), y_pred = tensor([42.00171, 34.00001, 25.99832, 17.99662]), loss = 0.00000430\n",
      "gradient = tensor([-0.00842,  0.00061])\n",
      "epoch 700: w = tensor([1.99905, 1.00006]), y_pred = tensor([42.00161, 34.00001, 25.99840, 17.99680]), loss = 0.00000386\n",
      "gradient = tensor([-0.00799,  0.00038])\n",
      "epoch 705: w = tensor([1.99910, 1.00006]), y_pred = tensor([42.00153, 34.00001, 25.99849, 17.99697]), loss = 0.00000346\n",
      "gradient = tensor([-0.00755,  0.00057])\n",
      "epoch 710: w = tensor([1.99915, 1.00006]), y_pred = tensor([42.00146, 34.00002, 25.99857, 17.99713]), loss = 0.00000311\n",
      "gradient = tensor([-0.00714,  0.00082])\n",
      "epoch 715: w = tensor([1.99919, 1.00005]), y_pred = tensor([42.00137, 34.00001, 25.99864, 17.99728]), loss = 0.00000278\n",
      "gradient = tensor([-0.00679,  0.00039])\n",
      "epoch 720: w = tensor([1.99923, 1.00005]), y_pred = tensor([42.00131, 34.00002, 25.99872, 17.99742]), loss = 0.00000250\n",
      "gradient = tensor([-0.00641,  0.00070])\n",
      "epoch 725: w = tensor([1.99927, 1.00005]), y_pred = tensor([42.00122, 34.00000, 25.99878, 17.99755]), loss = 0.00000224\n",
      "gradient = tensor([-6.11687e-03, -5.72205e-05])\n",
      "epoch 730: w = tensor([1.99931, 1.00005]), y_pred = tensor([42.00118, 34.00002, 25.99885, 17.99769]), loss = 0.00000201\n",
      "gradient = tensor([-0.00574,  0.00076])\n",
      "epoch 735: w = tensor([1.99935, 1.00004]), y_pred = tensor([42.00109, 34.00000, 25.99890, 17.99781]), loss = 0.00000180\n",
      "gradient = tensor([-5.48458e-03, -3.81470e-05])\n",
      "epoch 740: w = tensor([1.99938, 1.00004]), y_pred = tensor([42.00106, 34.00001, 25.99897, 17.99793]), loss = 0.00000162\n",
      "gradient = tensor([-0.00515,  0.00064])\n",
      "epoch 745: w = tensor([1.99942, 1.00004]), y_pred = tensor([42.00099, 34.00000, 25.99902, 17.99803]), loss = 0.00000145\n",
      "gradient = tensor([-0.00491,  0.00016])\n",
      "epoch 750: w = tensor([1.99945, 1.00004]), y_pred = tensor([42.00093, 34.00000, 25.99907, 17.99814]), loss = 0.00000130\n",
      "gradient = tensor([-4.65775e-03, -9.53674e-06])\n",
      "epoch 755: w = tensor([1.99948, 1.00003]), y_pred = tensor([42.00090, 34.00001, 25.99912, 17.99824]), loss = 0.00000117\n",
      "gradient = tensor([-0.00438,  0.00053])\n",
      "epoch 760: w = tensor([1.99950, 1.00003]), y_pred = tensor([42.00084, 34.00000, 25.99916, 17.99833]), loss = 0.00000105\n",
      "gradient = tensor([-0.00417,  0.00013])\n",
      "epoch 765: w = tensor([1.99953, 1.00003]), y_pred = tensor([42.00080, 34.00000, 25.99921, 17.99842]), loss = 0.00000094\n",
      "gradient = tensor([-0.00395,  0.00019])\n",
      "epoch 770: w = tensor([1.99955, 1.00003]), y_pred = tensor([42.00076, 34.00001, 25.99926, 17.99850]), loss = 0.00000084\n",
      "gradient = tensor([-0.00372,  0.00045])\n",
      "epoch 775: w = tensor([1.99958, 1.00003]), y_pred = tensor([42.00071, 34.00000, 25.99929, 17.99858]), loss = 0.00000076\n",
      "gradient = tensor([-3.54958e-03,  6.67572e-05])\n",
      "epoch 780: w = tensor([1.99960, 1.00003]), y_pred = tensor([42.00068, 34.00001, 25.99933, 17.99866]), loss = 0.00000068\n",
      "gradient = tensor([-0.00334,  0.00038])\n",
      "epoch 785: w = tensor([1.99962, 1.00003]), y_pred = tensor([42.00064, 34.00000, 25.99936, 17.99873]), loss = 0.00000061\n",
      "gradient = tensor([-0.00317,  0.00023])\n",
      "epoch 790: w = tensor([1.99964, 1.00002]), y_pred = tensor([42.00061, 34.00000, 25.99940, 17.99879]), loss = 0.00000055\n",
      "gradient = tensor([-3.01552e-03,  6.67572e-05])\n",
      "epoch 795: w = tensor([1.99966, 1.00002]), y_pred = tensor([42.00057, 34.00000, 25.99943, 17.99886]), loss = 0.00000049\n",
      "gradient = tensor([-0.00286,  0.00000])\n",
      "epoch 800: w = tensor([1.99968, 1.00002]), y_pred = tensor([42.00055, 34.00001, 25.99946, 17.99892]), loss = 0.00000044\n",
      "gradient = tensor([-0.00268,  0.00041])\n",
      "epoch 805: w = tensor([1.99970, 1.00002]), y_pred = tensor([42.00051, 34.00000, 25.99949, 17.99897]), loss = 0.00000039\n",
      "gradient = tensor([-0.00256,  0.00011])\n",
      "epoch 810: w = tensor([1.99971, 1.00002]), y_pred = tensor([42.00050, 34.00001, 25.99952, 17.99903]), loss = 0.00000036\n",
      "gradient = tensor([-0.00241,  0.00035])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 815: w = tensor([1.99973, 1.00002]), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99908]), loss = 0.00000032\n",
      "gradient = tensor([-0.00229,  0.00019])\n",
      "epoch 820: w = tensor([1.99974, 1.00002]), y_pred = tensor([42.00044, 34.00000, 25.99957, 17.99913]), loss = 0.00000029\n",
      "gradient = tensor([-0.00217,  0.00012])\n",
      "epoch 825: w = tensor([1.99975, 1.00002]), y_pred = tensor([42.00042, 34.00000, 25.99959, 17.99918]), loss = 0.00000026\n",
      "gradient = tensor([-0.00205,  0.00023])\n",
      "epoch 830: w = tensor([1.99977, 1.00002]), y_pred = tensor([42.00040, 34.00000, 25.99961, 17.99922]), loss = 0.00000023\n",
      "gradient = tensor([-0.00195,  0.00019])\n",
      "epoch 835: w = tensor([1.99978, 1.00001]), y_pred = tensor([42.00037, 34.00000, 25.99963, 17.99926]), loss = 0.00000021\n",
      "gradient = tensor([-1.86062e-03, -8.58307e-05])\n",
      "epoch 840: w = tensor([1.99979, 1.00001]), y_pred = tensor([42.00036, 34.00000, 25.99965, 17.99930]), loss = 0.00000019\n",
      "gradient = tensor([-0.00174,  0.00025])\n",
      "epoch 845: w = tensor([1.99980, 1.00001]), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99934]), loss = 0.00000017\n",
      "gradient = tensor([-1.65749e-03,  7.62939e-05])\n",
      "epoch 850: w = tensor([1.99981, 1.00001]), y_pred = tensor([42.00031, 34.00000, 25.99969, 17.99937]), loss = 0.00000015\n",
      "gradient = tensor([-1.57452e-03, -3.81470e-05])\n",
      "epoch 855: w = tensor([1.99982, 1.00001]), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940]), loss = 0.00000013\n",
      "gradient = tensor([-0.00148,  0.00013])\n",
      "epoch 860: w = tensor([1.99983, 1.00001]), y_pred = tensor([42.00029, 34.00001, 25.99972, 17.99944]), loss = 0.00000012\n",
      "gradient = tensor([-0.00139,  0.00039])\n",
      "epoch 865: w = tensor([1.99984, 1.00001]), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946]), loss = 0.00000011\n",
      "gradient = tensor([-1.34182e-03, -2.86102e-05])\n",
      "epoch 870: w = tensor([1.99985, 1.00001]), y_pred = tensor([42.00025, 34.00000, 25.99974, 17.99949]), loss = 0.00000010\n",
      "gradient = tensor([-1.27220e-03, -5.72205e-05])\n",
      "epoch 875: w = tensor([1.99986, 1.00001]), y_pred = tensor([42.00025, 34.00001, 25.99977, 17.99952]), loss = 0.00000009\n",
      "gradient = tensor([-0.00117,  0.00042])\n",
      "epoch 880: w = tensor([1.99987, 1.00001]), y_pred = tensor([42.00021, 33.99999, 25.99977, 17.99954]), loss = 0.00000008\n",
      "gradient = tensor([-0.00117, -0.00048])\n",
      "epoch 885: w = tensor([1.99987, 1.00001]), y_pred = tensor([42.00023, 34.00001, 25.99979, 17.99957]), loss = 0.00000007\n",
      "gradient = tensor([-0.00104,  0.00053])\n",
      "epoch 890: w = tensor([1.99988, 1.00001]), y_pred = tensor([42.00018, 33.99998, 25.99978, 17.99958]), loss = 0.00000006\n",
      "gradient = tensor([-0.00109, -0.00095])\n",
      "epoch 895: w = tensor([1.99989, 1.00001]), y_pred = tensor([42.00021, 34.00002, 25.99982, 17.99962]), loss = 0.00000006\n",
      "gradient = tensor([-0.00092,  0.00076])\n",
      "epoch 900: w = tensor([1.99989, 1.00001]), y_pred = tensor([42.00017, 33.99999, 25.99981, 17.99963]), loss = 0.00000005\n",
      "gradient = tensor([-0.00095, -0.00042])\n",
      "epoch 905: w = tensor([1.99990, 1.00001]), y_pred = tensor([42.00019, 34.00001, 25.99984, 17.99966]), loss = 0.00000005\n",
      "gradient = tensor([-0.00082,  0.00071])\n",
      "epoch 910: w = tensor([1.99990, 1.00001]), y_pred = tensor([42.00016, 34.00000, 25.99983, 17.99967]), loss = 0.00000004\n",
      "gradient = tensor([-0.00083, -0.00016])\n",
      "epoch 915: w = tensor([1.99991, 1.00001]), y_pred = tensor([42.00015, 34.00000, 25.99984, 17.99969]), loss = 0.00000004\n",
      "gradient = tensor([-0.00079, -0.00015])\n",
      "epoch 920: w = tensor([1.99991, 1.00001]), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970]), loss = 0.00000003\n",
      "gradient = tensor([-7.33376e-04,  8.58307e-05])\n",
      "epoch 925: w = tensor([1.99992, 1.00001]), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972]), loss = 0.00000003\n",
      "gradient = tensor([-0.00071, -0.00012])\n",
      "epoch 930: w = tensor([1.99992, 1.00001]), y_pred = tensor([42.00014, 34.00000, 25.99987, 17.99974]), loss = 0.00000003\n",
      "gradient = tensor([-0.00064,  0.00030])\n",
      "epoch 935: w = tensor([1.99993, 1.00001]), y_pred = tensor([42.00012, 34.00000, 25.99987, 17.99975]), loss = 0.00000002\n",
      "gradient = tensor([-0.00064, -0.00023])\n",
      "epoch 940: w = tensor([1.99993, 1.00000]), y_pred = tensor([42.00013, 34.00001, 25.99989, 17.99977]), loss = 0.00000002\n",
      "gradient = tensor([-0.00057,  0.00031])\n",
      "epoch 945: w = tensor([1.99993, 1.00000]), y_pred = tensor([42.00011, 34.00000, 25.99989, 17.99977]), loss = 0.00000002\n",
      "gradient = tensor([-0.00057, -0.00019])\n",
      "epoch 950: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99979]), loss = 0.00000002\n",
      "gradient = tensor([-0.00052,  0.00018])\n",
      "epoch 955: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99980]), loss = 0.00000002\n",
      "gradient = tensor([-0.00051, -0.00019])\n",
      "epoch 960: w = tensor([1.99994, 1.00000]), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981]), loss = 0.00000001\n",
      "gradient = tensor([-0.00048,  0.00000])\n",
      "epoch 965: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00009, 34.00000, 25.99991, 17.99982]), loss = 0.00000001\n",
      "gradient = tensor([-4.51088e-04,  2.86102e-05])\n",
      "epoch 970: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00010, 34.00001, 25.99992, 17.99983]), loss = 0.00000001\n",
      "gradient = tensor([-0.00040,  0.00046])\n",
      "epoch 975: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00006, 33.99999, 25.99991, 17.99983]), loss = 0.00000001\n",
      "gradient = tensor([-0.00044, -0.00058])\n",
      "epoch 980: w = tensor([1.99995, 1.00000]), y_pred = tensor([42.00008, 34.00000, 25.99993, 17.99985]), loss = 0.00000001\n",
      "gradient = tensor([-0.00037,  0.00025])\n",
      "epoch 985: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99985]), loss = 0.00000001\n",
      "gradient = tensor([-0.00038, -0.00018])\n",
      "epoch 990: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986]), loss = 0.00000001\n",
      "gradient = tensor([-0.00034,  0.00000])\n",
      "epoch 995: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00007, 34.00000, 25.99994, 17.99987]), loss = 0.00000001\n",
      "gradient = tensor([-0.00031,  0.00026])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000]), y_pred = tensor([42.00005, 34.00000, 25.99994, 17.99988]), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# вариант #1:\n",
    "# learning_rate = 0.05\n",
    "# n_iters = 20 + 1\n",
    "\n",
    "# вариант #2:\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема поиска градиента <a class=\"anchor\" id=\"проблема-поиска\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <em class=\"qs\"></em> Проблема: как найти градиент для нейронной сети: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$?\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p3_v1.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Проблема поиска градиента в общей логике обучения нейронной сети</b>    \n",
    "</center> \n",
    "\n",
    "* Для решения этой задачи и используется __алгоритм обратного распространения ошибки__ (backpropagation). Суть алгоритма:\n",
    "    * рассчитывается ошибка между выходным вектором сети $\\hat{\\pmb{y}}$ и правильным ответом обучающего примера $\\pmb{y}$\n",
    "    * ошибка распростаняется от результата к источнику (в обратную сторону) для корректировки весов\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_13.png\" alt=\"Пример обратного распространения ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em>Пример обратного распространения ошибки</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Рассчет градиента суперпозиции двух функций нескольких переменных__\n",
    "\n",
    "* Сначала рассмотрим подзадачу: как рассчитать градиент для $f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L)$? \n",
    "\n",
    "* Для этого нам нужно будет __рассчитывать градиент суперпозиции (сложной функции)__ состоящей из последовательного применения функций слоев $h^i$.\n",
    "\n",
    "* Вспомним, как рассчитать производную (градиент) суперпозиции нескольких функций.\n",
    "    * Пусть $z=f(y)$, $y=g(x)$\n",
    "    * Тогда производная суперпозиции функций (правило дифференцирования сложной функции (chain rule)): $\\frac{\\mathrm{d} z}{\\mathrm{d} x}=\\frac{\\mathrm{d} z}{\\mathrm{d} y}\\frac{\\mathrm{d} y}{\\mathrm{d} x}$\n",
    "    * Если $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{y} \\in \\mathbb{R}^m$, а $\\mathbf{z} \\in \\mathbb{R}$, то: $\\frac{\\partial z }{\\partial x_i} = \\sum_j \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}$\n",
    "\n",
    "<center> \n",
    "    \n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача поиска градиента: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})$__\n",
    "\n",
    "* Перейдем от $f_L$ к последовательному рассчету функций слоев $h^i$:\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "\n",
    "* Обозначим через $\\mathbf{a}^l$ результат рассчета функции активации на слое $l$: $\\mathbf{a}^l=h^l(\\mathbf{x}_l,\\mathbf{w}_l)$. Тогда: $\\mathbf{x}_{l+1}=\\mathbf{a}_l$ (вход следущего слоя является результатом рассчета функции активации предыдущего слоя)\n",
    "\n",
    "* Тогда можно записать: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_\\theta E(\\mathbf{a}^L, \\mathbf{y})$. Функция потерь $E(\\mathbf{a}^L, \\mathbf{y})$ зависит от $\\mathbf{a}^L$, $\\mathbf{a}^L$ от $\\mathbf{a}^{L-1}$, ..., $\\mathbf{a}^{l+1}$ от $\\mathbf{a}^{l}$\n",
    "\n",
    "* Исходя из этого представления можно градиенты весов $l$-го слоя можно записать как: \n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\color{blue}{ \\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} \\cdot \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$$\n",
    "\n",
    "* Произведение всех сомножетелей кроме последнего является градиентом функции потерь по результатам рассчета функции активации слоя $l$:\n",
    "$$\\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} = \\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$\n",
    "\n",
    "* Тогда:\n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$ для рассчета $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ нам нужен только якобиан функции активации $l$-го слоя по параметрам слоя $\\mathbf{w}_{l}$. \n",
    "\n",
    "* Градиент функции потерь по результатам рассчета функции активации слоя  $l$ может быть рассчитан рекурсивно по результатам слоя $l$, собственно тут и происходит __обратное распространение__:\n",
    "$\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}=\\left ( \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}} \\right )^T \\cdot \\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}}$ для рассчета $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}}$ нам нужен только якобиан функции активации $l+1$-го слоя по входным значениям слоя $\\mathbf{x}_{l+1}$\n",
    "\n",
    "* Т.е. чтобы проводить обратное распространение ошибки, нам на каждом слое (например $l$-м) нужно рассчитывать два якобиана:\n",
    "    * якобиан функции активации $l$-го слоя по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ - он позволит рассчитать градиент $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}$ и сделать очередной шаг градиентного спуска для параметров этого слоя: $\\mathbf{w}_l^{t+1} = \\mathbf{w}_l^{t}-\\gamma\\nabla_{w_l} E(\\mathbf{w}^{t})=\\mathbf{w}_l^{t}-\\gamma \\dfrac{\\partial E(\\mathbf{w}^{t})}{\\partial \\mathbf{w}_l}$\n",
    "    * якобиан функции активации $l$-го слоя по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ - он позволит распространить ошибку на низлежащие слои.\n",
    "\n",
    "Такми образом при обучении нам нужна только __очень локальная информация, содержащаяся в самом слое__. Т.е.:\n",
    "* __нет нобходимости знать как устроены сосоедние слои__: между слоями __очень простой интерфейс__ \n",
    "* т.е. __можно создать модульную архитектуру для слоев нейронной сети__: каждый модуль рассчитывает значение функции активации на основе выходов на прямом проходе и распространяет ошибку пришедшую на выходы на обратном проходе; все модули станадртным образом стыкуются друг с другом\n",
    "* при модульной архитектуре граф нейронной сети может быть очень сложным, но его __рассчет выполняется по одной простой и универсальной схеме__\n",
    "* внутри __модули могут быть сложно устроены, это никак не меняет логику остальных модулей и всего процесса обучения__, главное чтобы модуль корректно выполнял прямой и обратный проход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "##  Дифференцируемое программирование и реализация обратного распространения ошибки <a class=\"anchor\" id=\"дифференцируемое\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Почему Tensor *Flow*?__\n",
    "\n",
    "<em class=\"qs\"></em> Как реализовать __алгоритм обратного распространения ошибки__ удобно для использования в задачах моделирования ИНС?\n",
    "\n",
    "Основная абстракция TensorFlow, PyTorch и других аналогичных библиотеках - __граф потока вычислений__.\n",
    "\n",
    "* Рассматриваемые библиотеки обычно:\n",
    "    1. задают __граф потока вычислений__ (формирует объект отложенных вычислений)\n",
    "    2. запускают __процедуру выполненния отложенных вычислений__ и получает __результаты__ вычислений (в т.ч. ошибку модели).\n",
    "* Возможность в явном виде работать с графом потока вычислений дает большое приемущество для __автоматического решения задачи обратного распространения ошибки__, являющейся составляющей адачей обучения модели ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ker_6.png\" alt=\"Принцип устройства графа потока вычислений в TensorFlow\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Принцип устройства графа потока вычислений в TensorFlow</b>    \n",
    "</center>\n",
    "\n",
    "* Нейронная сеть это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев ИНС. Модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "* Вычисление функций слоев и взаимосвязи между слоями формируют граф потока вычислений в библиотеке моделирования ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_16.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Примеры иерархий в нейронных сетях</b>    \n",
    "</center>\n",
    "\n",
    "\n",
    "* По сути, ИНС это композиция модулей, представляющих собой слои нейронной сети:\n",
    "    * если сеть прямого распространения (feedforward), то все просто\n",
    "    * если сеть является направленным ациклическим графом, то существует правильный порядок применения функций\n",
    "    * в случае, если есть циклы, образующие рекуррентные связи, то существуют специальные подходы (будут рассмотрены позднее)\n",
    "\n",
    "\n",
    "* На обратном проходе (при обратном распространении ошибки) нам необходимо __дифференциировать сложную функцию__ многослойной ИНС\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "* алгоритм обратного распространения ошибки позволяет свести эту задачу к дифференциированию составляющих функций, но для этого необходимо __храниить информацию о виде и взаимосвязях функций задействованных в расчете модели ИНС__, именно эта информация и хранится в графе потока вычислений. Это позволяет организовать __автоматическое дифференциирование__ сложной функци многослойной ИНС."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Дифференциируемое программирование__\n",
    "\n",
    "<em class=\"df\"></em> __Дифференциируемое программирование__ (differentiable programming) - парадигма программирования при которой программа (функция рассчета значения) может быть продифференциирована в любой точке, обычно с помощью __автоматического диффиренциирования__. \n",
    "\n",
    "Это свойство позволяет использовать к программе __методы оптимизации основанные на рассчете градиента__, обычно - __методы градиентного спуска__.\n",
    "\n",
    "Дифференциируемое программирование используется в:\n",
    "* глубоком обучении\n",
    "* глубоком обучении комбинированном с физическими моделями в робототехнике\n",
    "* специализированных методах трассировки лучей\n",
    "* обработке изображений\n",
    "\n",
    "Большинство фреймоврков для дифференциируемого программирования использует граф потока вычислений определяющий выполнение программы и ее структуры данных.\n",
    "\n",
    "Основные классы фреймворков для дифференциируемого программирования:\n",
    "* __статические__ - они компилируют граф потока вычислений. Типичные представители: TensorFlow, Theano и др. Плюсы и минусы\n",
    "    * <em class=\"pl\"></em> могут использовать оптимизацию при компиляции\n",
    "    * <em class=\"pl\"></em> легче масштабирются на большие системы\n",
    "    * <em class=\"mn\"></em> статичность ограничевает интерактивность\n",
    "    * <em class=\"mn\"></em> многие программы не могут реализовываться легко (в частности: циклы, рекурсия)\n",
    "* __динамические__ - динамически исполняют граф потока вычислений. Используют перегрузку операторов для записи. Типичные представители: PyTorch, AutoGradrFlow. Плюсы и минусы:\n",
    "    * <em class=\"pl\"></em> более простая и понятная запись программы\n",
    "    * <em class=\"mn\"></em> накладные расходы интерпретатора\n",
    "    * <em class=\"mn\"></em> невозможно использовать оптимизацию компилятора\n",
    "    * <em class=\"mn\"></em> хуже масштабируемость\n",
    "* статическая на основе разбора промежуточного представления синтаксического разбора исходной программы. Пример фрэймоврк Zygote (язык программирования Julia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход__:\n",
    "* Модули из графа обходятся один за одним начиная с узла входных данных и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден\n",
    "* Рассчет функций активации для каждого модуля по входным данным: $a_l=h_l(x_l, w_l)$\n",
    "* Промежуточные значения кэшируются, чтобы не рассчитывать их повторно (в сложном графе сети и при обратном проходе)\n",
    "* Выходы одних модулей становятся входами других модулей: $x_{l+1}=a_l$\n",
    "* Последним модулем рассчитывается сумма потерь для входных данных\n",
    "<center> \n",
    "    \n",
    "__Прямой и обратный проход процедуры обучения многослойной ИНС:__\n",
    "\n",
    "<img src=\"./img/ann_19.png\" alt=\"Прямой и обратный проход \" style=\"width: 300px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "__Обратный проход__:\n",
    "* Сначала должен быть произведен прямой проход. На входе обратного прохода известна сумма потерь.\n",
    "* Строится обратный порядок обхода графа зависимостей модулей.\n",
    "* Модули из графа обходятся один за одним начиная с узла рассчета функции потерь и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден \n",
    "* Для каждого модуля рассчитыватся якобиан функции активации по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ и якобиан функции активации по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ \n",
    "* По пришедшму в модуль градиенту ошибки (полученному из модулей использовавших результаты данного модуля на прямом проходе) $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$ рассчитывается:\n",
    "    * Градиент для шага градиентного спуска по параметрам модуля $w_l$: $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$\n",
    "    * Градиент ошбки, который передается в модули, поставившие данные в этот модуль во время прямого прохода: $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l-1}}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_2.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_3.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_4.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_5.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_6.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Производные популярных функций активации__\n",
    "\n",
    "<img src=\"./img/ann_17.png\" alt=\"Пример\" style=\"width: 500px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_7.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":\n",
    "##  Автоматическое дифференциирование в PyTorch <a class=\"anchor\" id=\"автоматическое-PyTorch\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.08521,  0.25389, -0.85220], requires_grad=True)\n",
      "tensor([1.91479, 2.25389, 1.14780], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x00000221B5628908>\n"
     ]
    }
   ],
   "source": [
    "# The autograd package provides automatic differentiation \n",
    "# for all operations on Tensors\n",
    "\n",
    "# requires_grad = True -> tracks all operations on the tensor. \n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.99930, 15.24001,  3.95233], grad_fn=<MulBackward0>)\n",
      "tensor(10.06388, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.82959, 4.50777, 2.29560])\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n",
    "\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derivates while applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2596.08423,  -432.97888, -2130.46240], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Model with non-scalar output:\n",
    "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n",
    "# specify a gradient argument that is a tensor of matching shape.\n",
    "# needed for vector-Jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "for _ in range(10):\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.04800e+02, 2.04800e+03, 2.04800e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Stop a tensor from tracking history:\n",
    "For example during our training loop when we want to update our weights\n",
    "then this update operation should not be part of the gradient computation\n",
    "* `x.requires_grad_(False)`\n",
    "* `x.detach()`\n",
    "* wrap in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad = False\n",
      "b.grad_fn = None\n",
      "a.requires_grad = True\n",
      "b.grad_fn = <SumBackward0 object at 0x00000221B56D26C8>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(f'b.grad_fn = {b.grad_fn}')\n",
    "      \n",
    "a.requires_grad_(True)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(f'b.grad_fn = {b.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = a.detach()\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([0.10000, 0.10000, 0.10000, 0.10000], requires_grad=True)\n",
      "tensor(4.80000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    # 'forward pass'\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)\n",
    "\n",
    "# Optimizer has zero_grad() method\n",
    "# optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# During training:\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Автоматическое выполнение обратного прохода с помощью `l.backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "epoch 0: w = tensor([0.16900, 2.21000], requires_grad=True), y_pred = tensor([0., 0., 0., 0.], grad_fn=<MvBackward>), loss = 980.00000000\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 5: w = tensor([0.13813, 0.24422], requires_grad=True), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021], grad_fn=<MvBackward>), loss = 646.58898926\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 10: w = tensor([0.33966, 1.82453], requires_grad=True), y_pred = tensor([15.22920, 11.70164,  8.17407,  4.64651], grad_fn=<MvBackward>), loss = 427.49307251\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 15: w = tensor([0.34364, 0.53338], requires_grad=True), y_pred = tensor([68.78386, 52.09385, 35.40385, 18.71384], grad_fn=<MvBackward>), loss = 283.42614746\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 20: w = tensor([0.49880, 1.56850], requires_grad=True), y_pred = tensor([25.13912, 19.37721, 13.61531,  7.85340], grad_fn=<MvBackward>), loss = 188.61228943\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 25: w = tensor([0.52316, 0.72007], requires_grad=True), y_pred = tensor([60.23326, 45.87370, 31.51413, 17.15457], grad_fn=<MvBackward>), loss = 126.13926697\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 30: w = tensor([0.64554, 1.39771], requires_grad=True), y_pred = tensor([31.56792, 24.41182, 17.25571, 10.09960], grad_fn=<MvBackward>), loss = 84.91012573\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 35: w = tensor([0.68103, 0.83984], requires_grad=True), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671], grad_fn=<MvBackward>), loss = 57.64197159\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 40: w = tensor([0.77979, 1.28313], requires_grad=True), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095], grad_fn=<MvBackward>), loss = 39.55477905\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 45: w = tensor([0.82057, 0.91600], requires_grad=True), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541], grad_fn=<MvBackward>), loss = 27.51074600\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 50: w = tensor([0.90194, 1.20568], requires_grad=True), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615], grad_fn=<MvBackward>), loss = 19.44943237\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 55: w = tensor([0.94440, 0.96380], requires_grad=True), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149], grad_fn=<MvBackward>), loss = 14.01713943\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 60: w = tensor([1.01265, 1.15282], requires_grad=True), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346], grad_fn=<MvBackward>), loss = 10.32425213\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 65: w = tensor([1.05460, 0.99321], requires_grad=True), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179], grad_fn=<MvBackward>), loss = 7.78575325\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 70: w = tensor([1.11272, 1.11631], requires_grad=True), y_pred = tensor([41.15015, 32.23169, 23.31322, 14.39476], grad_fn=<MvBackward>), loss = 6.01642036\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 75: w = tensor([1.15289, 1.01076], requires_grad=True), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980], grad_fn=<MvBackward>), loss = 4.76234579\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 80: w = tensor([1.20299, 1.09071], requires_grad=True), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144], grad_fn=<MvBackward>), loss = 3.85587597\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 85: w = tensor([1.24068, 1.02071], requires_grad=True), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292], grad_fn=<MvBackward>), loss = 3.18603969\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 90: w = tensor([1.28429, 1.07244], requires_grad=True), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473], grad_fn=<MvBackward>), loss = 2.67915201\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 95: w = tensor([1.31920, 1.02583], requires_grad=True), y_pred = tensor([43.97913, 34.62745, 25.27579, 15.92412], grad_fn=<MvBackward>), loss = 2.28610182\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 100: w = tensor([1.35745, 1.05912], requires_grad=True), y_pred = tensor([42.42656, 33.50523, 24.58389, 15.66256], grad_fn=<MvBackward>), loss = 1.97393394\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 105: w = tensor([1.38948, 1.02793], requires_grad=True), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649], grad_fn=<MvBackward>), loss = 1.72041774\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 110: w = tensor([1.42322, 1.04919], requires_grad=True), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94403], grad_fn=<MvBackward>), loss = 1.51038170\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 115: w = tensor([1.45243, 1.02818], requires_grad=True), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971], grad_fn=<MvBackward>), loss = 1.33336782\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 120: w = tensor([1.48233, 1.04161], requires_grad=True), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238], grad_fn=<MvBackward>), loss = 1.18204451\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 125: w = tensor([1.50884, 1.02733], requires_grad=True), y_pred = tensor([43.06691, 34.18049, 25.29407, 16.40766], grad_fn=<MvBackward>), loss = 1.05119181\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 130: w = tensor([1.53541, 1.03567], requires_grad=True), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693], grad_fn=<MvBackward>), loss = 0.93700927\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 135: w = tensor([1.55940, 1.02587], requires_grad=True), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691], grad_fn=<MvBackward>), loss = 0.83668095\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 140: w = tensor([1.58308, 1.03092], requires_grad=True), y_pred = tensor([42.58541, 33.91172, 25.23803, 16.56434], grad_fn=<MvBackward>), loss = 0.74805164\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 145: w = tensor([1.60474, 1.02409], requires_grad=True), y_pred = tensor([42.77173, 34.07975, 25.38776, 16.69578], grad_fn=<MvBackward>), loss = 0.66944206\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 150: w = tensor([1.62588, 1.02703], requires_grad=True), y_pred = tensor([42.55506, 33.94321, 25.33137, 16.71952], grad_fn=<MvBackward>), loss = 0.59950674\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 155: w = tensor([1.64540, 1.02219], requires_grad=True), y_pred = tensor([42.66828, 34.05339, 25.43850, 16.82362], grad_fn=<MvBackward>), loss = 0.53715217\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 160: w = tensor([1.66429, 1.02378], requires_grad=True), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612], grad_fn=<MvBackward>), loss = 0.48146015\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 165: w = tensor([1.68186, 1.02029], requires_grad=True), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045], grad_fn=<MvBackward>), loss = 0.43166173\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 170: w = tensor([1.69877, 1.02103], requires_grad=True), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695], grad_fn=<MvBackward>), loss = 0.38709140\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 175: w = tensor([1.71457, 1.01845], requires_grad=True), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667], grad_fn=<MvBackward>), loss = 0.34717295\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 180: w = tensor([1.72971, 1.01867], requires_grad=True), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08424], grad_fn=<MvBackward>), loss = 0.31140596\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 185: w = tensor([1.74391, 1.01672], requires_grad=True), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289], grad_fn=<MvBackward>), loss = 0.27934441\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 190: w = tensor([1.75748, 1.01662], requires_grad=True), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977], grad_fn=<MvBackward>), loss = 0.25059801\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 195: w = tensor([1.77024, 1.01511], requires_grad=True), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982], grad_fn=<MvBackward>), loss = 0.22481927\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 200: w = tensor([1.78240, 1.01482], requires_grad=True), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499], grad_fn=<MvBackward>), loss = 0.20169993\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 205: w = tensor([1.79385, 1.01363], requires_grad=True), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822], grad_fn=<MvBackward>), loss = 0.18096074\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 210: w = tensor([1.80476, 1.01324], requires_grad=True), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113], grad_fn=<MvBackward>), loss = 0.16235729\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 215: w = tensor([1.81504, 1.01227], requires_grad=True), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882], grad_fn=<MvBackward>), loss = 0.14566770\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 220: w = tensor([1.82482, 1.01185], requires_grad=True), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924], grad_fn=<MvBackward>), loss = 0.13069558\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 225: w = tensor([1.83405, 1.01104], requires_grad=True), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233], grad_fn=<MvBackward>), loss = 0.11726224\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 230: w = tensor([1.84282, 1.01060], requires_grad=True), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021], grad_fn=<MvBackward>), loss = 0.10521053\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 235: w = tensor([1.85111, 1.00993], requires_grad=True), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942], grad_fn=<MvBackward>), loss = 0.09439759\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 240: w = tensor([1.85897, 1.00950], requires_grad=True), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483], grad_fn=<MvBackward>), loss = 0.08469677\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 245: w = tensor([1.86641, 1.00892], requires_grad=True), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073], grad_fn=<MvBackward>), loss = 0.07599217\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 250: w = tensor([1.87346, 1.00851], requires_grad=True), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377], grad_fn=<MvBackward>), loss = 0.06818256\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 255: w = tensor([1.88014, 1.00801], requires_grad=True), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681], grad_fn=<MvBackward>), loss = 0.06117591\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 260: w = tensor([1.88647, 1.00763], requires_grad=True), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764], grad_fn=<MvBackward>), loss = 0.05488885\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 265: w = tensor([1.89246, 1.00719], requires_grad=True), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818], grad_fn=<MvBackward>), loss = 0.04924808\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 270: w = tensor([1.89813, 1.00684], requires_grad=True), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699], grad_fn=<MvBackward>), loss = 0.04418735\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 275: w = tensor([1.90351, 1.00646], requires_grad=True), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532], grad_fn=<MvBackward>), loss = 0.03964623\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 280: w = tensor([1.90860, 1.00614], requires_grad=True), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227], grad_fn=<MvBackward>), loss = 0.03557184\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 285: w = tensor([1.91342, 1.00580], requires_grad=True), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866], grad_fn=<MvBackward>), loss = 0.03191639\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 290: w = tensor([1.91799, 1.00550], requires_grad=True), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391], grad_fn=<MvBackward>), loss = 0.02863660\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 295: w = tensor([1.92232, 1.00520], requires_grad=True), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858], grad_fn=<MvBackward>), loss = 0.02569385\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 300: w = tensor([1.92642, 1.00494], requires_grad=True), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230], grad_fn=<MvBackward>), loss = 0.02305340\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 305: w = tensor([1.93030, 1.00467], requires_grad=True), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544], grad_fn=<MvBackward>), loss = 0.02068412\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 310: w = tensor([1.93398, 1.00443], requires_grad=True), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776], grad_fn=<MvBackward>), loss = 0.01855873\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 315: w = tensor([1.93747, 1.00419], requires_grad=True), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953], grad_fn=<MvBackward>), loss = 0.01665138\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 320: w = tensor([1.94077, 1.00397], requires_grad=True), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061], grad_fn=<MvBackward>), loss = 0.01494033\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 325: w = tensor([1.94389, 1.00376], requires_grad=True), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116], grad_fn=<MvBackward>), loss = 0.01340491\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 330: w = tensor([1.94685, 1.00356], requires_grad=True), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110], grad_fn=<MvBackward>), loss = 0.01202754\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 335: w = tensor([1.94966, 1.00337], requires_grad=True), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055], grad_fn=<MvBackward>), loss = 0.01079139\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 340: w = tensor([1.95231, 1.00320], requires_grad=True), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949], grad_fn=<MvBackward>), loss = 0.00968240\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 345: w = tensor([1.95483, 1.00303], requires_grad=True), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797], grad_fn=<MvBackward>), loss = 0.00868748\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 350: w = tensor([1.95722, 1.00287], requires_grad=True), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599], grad_fn=<MvBackward>), loss = 0.00779471\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 355: w = tensor([1.95947, 1.00272], requires_grad=True), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359], grad_fn=<MvBackward>), loss = 0.00699359\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 360: w = tensor([1.96161, 1.00257], requires_grad=True), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078], grad_fn=<MvBackward>), loss = 0.00627505\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 365: w = tensor([1.96364, 1.00244], requires_grad=True), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761], grad_fn=<MvBackward>), loss = 0.00563018\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 370: w = tensor([1.96556, 1.00231], requires_grad=True), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406], grad_fn=<MvBackward>), loss = 0.00505164\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 375: w = tensor([1.96737, 1.00219], requires_grad=True), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019], grad_fn=<MvBackward>), loss = 0.00453248\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 380: w = tensor([1.96910, 1.00207], requires_grad=True), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598], grad_fn=<MvBackward>), loss = 0.00406663\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 385: w = tensor([1.97073, 1.00196], requires_grad=True), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147], grad_fn=<MvBackward>), loss = 0.00364873\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 390: w = tensor([1.97227, 1.00186], requires_grad=True), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667], grad_fn=<MvBackward>), loss = 0.00327375\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 395: w = tensor([1.97374, 1.00176], requires_grad=True), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160], grad_fn=<MvBackward>), loss = 0.00293732\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 400: w = tensor([1.97512, 1.00167], requires_grad=True), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626], grad_fn=<MvBackward>), loss = 0.00263548\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 405: w = tensor([1.97643, 1.00158], requires_grad=True), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068], grad_fn=<MvBackward>), loss = 0.00236459\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 410: w = tensor([1.97768, 1.00150], requires_grad=True), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487], grad_fn=<MvBackward>), loss = 0.00212172\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 415: w = tensor([1.97886, 1.00142], requires_grad=True), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883], grad_fn=<MvBackward>), loss = 0.00190360\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 420: w = tensor([1.97997, 1.00134], requires_grad=True), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259], grad_fn=<MvBackward>), loss = 0.00170798\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 425: w = tensor([1.98103, 1.00127], requires_grad=True), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615], grad_fn=<MvBackward>), loss = 0.00153252\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 430: w = tensor([1.98203, 1.00120], requires_grad=True), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951], grad_fn=<MvBackward>), loss = 0.00137501\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 435: w = tensor([1.98298, 1.00114], requires_grad=True), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271], grad_fn=<MvBackward>), loss = 0.00123367\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 440: w = tensor([1.98388, 1.00108], requires_grad=True), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573], grad_fn=<MvBackward>), loss = 0.00110684\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 445: w = tensor([1.98473, 1.00102], requires_grad=True), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859], grad_fn=<MvBackward>), loss = 0.00099315\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 450: w = tensor([1.98553, 1.00097], requires_grad=True), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131], grad_fn=<MvBackward>), loss = 0.00089105\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 455: w = tensor([1.98630, 1.00092], requires_grad=True), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388], grad_fn=<MvBackward>), loss = 0.00079952\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 460: w = tensor([1.98702, 1.00087], requires_grad=True), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631], grad_fn=<MvBackward>), loss = 0.00071731\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 465: w = tensor([1.98771, 1.00082], requires_grad=True), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862], grad_fn=<MvBackward>), loss = 0.00064363\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 470: w = tensor([1.98835, 1.00078], requires_grad=True), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080], grad_fn=<MvBackward>), loss = 0.00057748\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 475: w = tensor([1.98897, 1.00074], requires_grad=True), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288], grad_fn=<MvBackward>), loss = 0.00051811\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 480: w = tensor([1.98955, 1.00070], requires_grad=True), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483], grad_fn=<MvBackward>), loss = 0.00046487\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 485: w = tensor([1.99010, 1.00066], requires_grad=True), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669], grad_fn=<MvBackward>), loss = 0.00041709\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 490: w = tensor([1.99063, 1.00063], requires_grad=True), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844], grad_fn=<MvBackward>), loss = 0.00037425\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 495: w = tensor([1.99112, 1.00060], requires_grad=True), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011], grad_fn=<MvBackward>), loss = 0.00033581\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 500: w = tensor([1.99159, 1.00056], requires_grad=True), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169], grad_fn=<MvBackward>), loss = 0.00030129\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 505: w = tensor([1.99203, 1.00053], requires_grad=True), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318], grad_fn=<MvBackward>), loss = 0.00027033\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 510: w = tensor([1.99245, 1.00051], requires_grad=True), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460], grad_fn=<MvBackward>), loss = 0.00024254\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 515: w = tensor([1.99285, 1.00048], requires_grad=True), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593], grad_fn=<MvBackward>), loss = 0.00021764\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 520: w = tensor([1.99323, 1.00045], requires_grad=True), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721], grad_fn=<MvBackward>), loss = 0.00019526\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 525: w = tensor([1.99359, 1.00043], requires_grad=True), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841], grad_fn=<MvBackward>), loss = 0.00017518\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 530: w = tensor([1.99392, 1.00041], requires_grad=True), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955], grad_fn=<MvBackward>), loss = 0.00015719\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 535: w = tensor([1.99425, 1.00039], requires_grad=True), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063], grad_fn=<MvBackward>), loss = 0.00014103\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 540: w = tensor([1.99455, 1.00037], requires_grad=True), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165], grad_fn=<MvBackward>), loss = 0.00012652\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 545: w = tensor([1.99484, 1.00035], requires_grad=True), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262], grad_fn=<MvBackward>), loss = 0.00011353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 550: w = tensor([1.99511, 1.00033], requires_grad=True), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354], grad_fn=<MvBackward>), loss = 0.00010187\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 555: w = tensor([1.99537, 1.00031], requires_grad=True), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441], grad_fn=<MvBackward>), loss = 0.00009140\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 560: w = tensor([1.99561, 1.00029], requires_grad=True), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523], grad_fn=<MvBackward>), loss = 0.00008202\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 565: w = tensor([1.99584, 1.00028], requires_grad=True), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600], grad_fn=<MvBackward>), loss = 0.00007359\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 570: w = tensor([1.99606, 1.00026], requires_grad=True), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675], grad_fn=<MvBackward>), loss = 0.00006602\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 575: w = tensor([1.99627, 1.00025], requires_grad=True), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744], grad_fn=<MvBackward>), loss = 0.00005925\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 580: w = tensor([1.99647, 1.00024], requires_grad=True), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811], grad_fn=<MvBackward>), loss = 0.00005316\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 585: w = tensor([1.99665, 1.00022], requires_grad=True), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874], grad_fn=<MvBackward>), loss = 0.00004769\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 590: w = tensor([1.99683, 1.00021], requires_grad=True), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933], grad_fn=<MvBackward>), loss = 0.00004279\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 595: w = tensor([1.99700, 1.00020], requires_grad=True), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989], grad_fn=<MvBackward>), loss = 0.00003839\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 600: w = tensor([1.99716, 1.00019], requires_grad=True), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043], grad_fn=<MvBackward>), loss = 0.00003444\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 605: w = tensor([1.99731, 1.00018], requires_grad=True), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093], grad_fn=<MvBackward>), loss = 0.00003091\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 610: w = tensor([1.99745, 1.00017], requires_grad=True), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141], grad_fn=<MvBackward>), loss = 0.00002773\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 615: w = tensor([1.99758, 1.00016], requires_grad=True), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186], grad_fn=<MvBackward>), loss = 0.00002489\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 620: w = tensor([1.99771, 1.00015], requires_grad=True), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229], grad_fn=<MvBackward>), loss = 0.00002233\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 625: w = tensor([1.99783, 1.00015], requires_grad=True), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270], grad_fn=<MvBackward>), loss = 0.00002004\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 630: w = tensor([1.99795, 1.00014], requires_grad=True), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308], grad_fn=<MvBackward>), loss = 0.00001798\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 635: w = tensor([1.99805, 1.00013], requires_grad=True), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345], grad_fn=<MvBackward>), loss = 0.00001613\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 640: w = tensor([1.99816, 1.00012], requires_grad=True), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379], grad_fn=<MvBackward>), loss = 0.00001448\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 645: w = tensor([1.99825, 1.00012], requires_grad=True), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413], grad_fn=<MvBackward>), loss = 0.00001298\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 650: w = tensor([1.99835, 1.00011], requires_grad=True), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443], grad_fn=<MvBackward>), loss = 0.00001165\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 655: w = tensor([1.99843, 1.00010], requires_grad=True), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473], grad_fn=<MvBackward>), loss = 0.00001045\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 660: w = tensor([1.99852, 1.00010], requires_grad=True), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500], grad_fn=<MvBackward>), loss = 0.00000938\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 665: w = tensor([1.99859, 1.00009], requires_grad=True), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527], grad_fn=<MvBackward>), loss = 0.00000841\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 670: w = tensor([1.99867, 1.00009], requires_grad=True), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552], grad_fn=<MvBackward>), loss = 0.00000755\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 675: w = tensor([1.99874, 1.00008], requires_grad=True), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576], grad_fn=<MvBackward>), loss = 0.00000678\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 680: w = tensor([1.99881, 1.00008], requires_grad=True), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597], grad_fn=<MvBackward>), loss = 0.00000608\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 685: w = tensor([1.99887, 1.00008], requires_grad=True), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619], grad_fn=<MvBackward>), loss = 0.00000546\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 690: w = tensor([1.99893, 1.00007], requires_grad=True), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639], grad_fn=<MvBackward>), loss = 0.00000489\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 695: w = tensor([1.99898, 1.00007], requires_grad=True), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658], grad_fn=<MvBackward>), loss = 0.00000439\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 700: w = tensor([1.99904, 1.00006], requires_grad=True), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677], grad_fn=<MvBackward>), loss = 0.00000394\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 705: w = tensor([1.99909, 1.00006], requires_grad=True), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693], grad_fn=<MvBackward>), loss = 0.00000353\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 710: w = tensor([1.99914, 1.00006], requires_grad=True), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709], grad_fn=<MvBackward>), loss = 0.00000317\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 715: w = tensor([1.99918, 1.00005], requires_grad=True), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725], grad_fn=<MvBackward>), loss = 0.00000284\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 720: w = tensor([1.99923, 1.00005], requires_grad=True), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739], grad_fn=<MvBackward>), loss = 0.00000255\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 725: w = tensor([1.99927, 1.00005], requires_grad=True), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753], grad_fn=<MvBackward>), loss = 0.00000229\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 730: w = tensor([1.99931, 1.00005], requires_grad=True), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766], grad_fn=<MvBackward>), loss = 0.00000206\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 735: w = tensor([1.99934, 1.00004], requires_grad=True), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779], grad_fn=<MvBackward>), loss = 0.00000184\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 740: w = tensor([1.99938, 1.00004], requires_grad=True), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790], grad_fn=<MvBackward>), loss = 0.00000166\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 745: w = tensor([1.99941, 1.00004], requires_grad=True), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801], grad_fn=<MvBackward>), loss = 0.00000149\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 750: w = tensor([1.99944, 1.00004], requires_grad=True), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812], grad_fn=<MvBackward>), loss = 0.00000133\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 755: w = tensor([1.99947, 1.00004], requires_grad=True), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822], grad_fn=<MvBackward>), loss = 0.00000119\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 760: w = tensor([1.99950, 1.00003], requires_grad=True), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831], grad_fn=<MvBackward>), loss = 0.00000107\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 765: w = tensor([1.99952, 1.00003], requires_grad=True), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840], grad_fn=<MvBackward>), loss = 0.00000096\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 770: w = tensor([1.99955, 1.00003], requires_grad=True), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848], grad_fn=<MvBackward>), loss = 0.00000086\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 775: w = tensor([1.99957, 1.00003], requires_grad=True), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857], grad_fn=<MvBackward>), loss = 0.00000077\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 780: w = tensor([1.99960, 1.00003], requires_grad=True), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864], grad_fn=<MvBackward>), loss = 0.00000069\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 785: w = tensor([1.99962, 1.00003], requires_grad=True), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871], grad_fn=<MvBackward>), loss = 0.00000062\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 790: w = tensor([1.99964, 1.00002], requires_grad=True), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878], grad_fn=<MvBackward>), loss = 0.00000056\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 795: w = tensor([1.99966, 1.00002], requires_grad=True), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885], grad_fn=<MvBackward>), loss = 0.00000050\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 800: w = tensor([1.99967, 1.00002], requires_grad=True), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891], grad_fn=<MvBackward>), loss = 0.00000045\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 805: w = tensor([1.99969, 1.00002], requires_grad=True), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896], grad_fn=<MvBackward>), loss = 0.00000040\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 810: w = tensor([1.99971, 1.00002], requires_grad=True), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902], grad_fn=<MvBackward>), loss = 0.00000036\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 815: w = tensor([1.99972, 1.00002], requires_grad=True), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907], grad_fn=<MvBackward>), loss = 0.00000033\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 820: w = tensor([1.99974, 1.00002], requires_grad=True), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912], grad_fn=<MvBackward>), loss = 0.00000029\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 825: w = tensor([1.99975, 1.00002], requires_grad=True), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916], grad_fn=<MvBackward>), loss = 0.00000026\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 830: w = tensor([1.99977, 1.00002], requires_grad=True), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921], grad_fn=<MvBackward>), loss = 0.00000024\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 835: w = tensor([1.99978, 1.00001], requires_grad=True), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925], grad_fn=<MvBackward>), loss = 0.00000021\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 840: w = tensor([1.99979, 1.00001], requires_grad=True), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929], grad_fn=<MvBackward>), loss = 0.00000019\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 845: w = tensor([1.99980, 1.00001], requires_grad=True), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933], grad_fn=<MvBackward>), loss = 0.00000017\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 850: w = tensor([1.99981, 1.00001], requires_grad=True), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936], grad_fn=<MvBackward>), loss = 0.00000015\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 855: w = tensor([1.99982, 1.00001], requires_grad=True), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940], grad_fn=<MvBackward>), loss = 0.00000014\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 860: w = tensor([1.99983, 1.00001], requires_grad=True), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943], grad_fn=<MvBackward>), loss = 0.00000012\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 865: w = tensor([1.99984, 1.00001], requires_grad=True), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946], grad_fn=<MvBackward>), loss = 0.00000011\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 870: w = tensor([1.99985, 1.00001], requires_grad=True), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949], grad_fn=<MvBackward>), loss = 0.00000010\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 875: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951], grad_fn=<MvBackward>), loss = 0.00000009\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 880: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954], grad_fn=<MvBackward>), loss = 0.00000008\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 885: w = tensor([1.99987, 1.00001], requires_grad=True), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956], grad_fn=<MvBackward>), loss = 0.00000007\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 890: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 895: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 900: w = tensor([1.99989, 1.00001], requires_grad=True), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 905: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 910: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 915: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 920: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 925: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 930: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 935: w = tensor([1.99992, 1.00000], requires_grad=True), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 940: w = tensor([1.99993, 1.00001], requires_grad=True), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 945: w = tensor([1.99993, 1.00000], requires_grad=True), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 950: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 955: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 960: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 965: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 970: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 975: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 980: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "#!!! requires_grad=True\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    #!!! backward pass        \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Использование встроенного оптимизатора `optimizer = torch.optim.SGD([w], lr=learning_rate)` и функции потерь `loss = nn.MSELoss()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "epoch 0: w = tensor([0.16900, 2.21000], requires_grad=True), y_pred = tensor([0., 0., 0., 0.], grad_fn=<MvBackward>), loss = 980.00000000\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 5: w = tensor([0.13813, 0.24422], requires_grad=True), y_pred = tensor([81.70274, 61.57523, 41.44772, 21.32021], grad_fn=<MvBackward>), loss = 646.58898926\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 10: w = tensor([0.33966, 1.82453], requires_grad=True), y_pred = tensor([15.22920, 11.70164,  8.17407,  4.64651], grad_fn=<MvBackward>), loss = 427.49307251\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 15: w = tensor([0.34364, 0.53338], requires_grad=True), y_pred = tensor([68.78386, 52.09385, 35.40385, 18.71384], grad_fn=<MvBackward>), loss = 283.42614746\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 20: w = tensor([0.49880, 1.56850], requires_grad=True), y_pred = tensor([25.13912, 19.37721, 13.61531,  7.85340], grad_fn=<MvBackward>), loss = 188.61228943\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 25: w = tensor([0.52316, 0.72007], requires_grad=True), y_pred = tensor([60.23326, 45.87370, 31.51413, 17.15457], grad_fn=<MvBackward>), loss = 126.13926697\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 30: w = tensor([0.64554, 1.39771], requires_grad=True), y_pred = tensor([31.56792, 24.41182, 17.25571, 10.09960], grad_fn=<MvBackward>), loss = 84.91012573\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 35: w = tensor([0.68103, 0.83984], requires_grad=True), y_pred = tensor([54.55604, 41.79293, 29.02982, 16.26671], grad_fn=<MvBackward>), loss = 57.64197159\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 40: w = tensor([0.77979, 1.28313], requires_grad=True), y_pred = tensor([35.72054, 27.71401, 19.70748, 11.70095], grad_fn=<MvBackward>), loss = 39.55477905\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 45: w = tensor([0.82057, 0.91600], requires_grad=True), y_pred = tensor([50.77066, 39.11558, 27.46050, 15.80541], grad_fn=<MvBackward>), loss = 27.51074600\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 50: w = tensor([0.90194, 1.20568], requires_grad=True), y_pred = tensor([38.38666, 29.87982, 21.37299, 12.86615], grad_fn=<MvBackward>), loss = 19.44943237\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 55: w = tensor([0.94440, 0.96380], requires_grad=True), y_pred = tensor([48.23264, 37.35892, 26.48521, 15.61149], grad_fn=<MvBackward>), loss = 14.01713943\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 60: w = tensor([1.01265, 1.15282], requires_grad=True), y_pred = tensor([40.08362, 31.30023, 22.51685, 13.73346], grad_fn=<MvBackward>), loss = 10.32425213\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 65: w = tensor([1.05460, 0.99321], requires_grad=True), y_pred = tensor([46.51847, 36.20624, 25.89402, 15.58179], grad_fn=<MvBackward>), loss = 7.78575325\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 70: w = tensor([1.11272, 1.11631], requires_grad=True), y_pred = tensor([41.15015, 32.23169, 23.31322, 14.39476], grad_fn=<MvBackward>), loss = 6.01642036\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 75: w = tensor([1.15289, 1.01076], requires_grad=True), y_pred = tensor([45.34982, 35.44982, 25.54981, 15.64980], grad_fn=<MvBackward>), loss = 4.76234579\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 80: w = tensor([1.20299, 1.09071], requires_grad=True), y_pred = tensor([41.80791, 32.84242, 23.87693, 14.91144], grad_fn=<MvBackward>), loss = 3.85587597\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 85: w = tensor([1.24068, 1.02071], requires_grad=True), y_pred = tensor([44.54358, 34.95336, 25.36314, 15.77292], grad_fn=<MvBackward>), loss = 3.18603969\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 90: w = tensor([1.28429, 1.07244], requires_grad=True), y_pred = tensor([42.20184, 33.24281, 24.28377, 15.32473], grad_fn=<MvBackward>), loss = 2.67915201\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 95: w = tensor([1.31920, 1.02583], requires_grad=True), y_pred = tensor([43.97913, 34.62745, 25.27579, 15.92412], grad_fn=<MvBackward>), loss = 2.28610182\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 100: w = tensor([1.35745, 1.05912], requires_grad=True), y_pred = tensor([42.42656, 33.50523, 24.58389, 15.66256], grad_fn=<MvBackward>), loss = 1.97393394\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 105: w = tensor([1.38948, 1.02793], requires_grad=True), y_pred = tensor([43.57692, 34.41344, 25.24997, 16.08649], grad_fn=<MvBackward>), loss = 1.72041774\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 110: w = tensor([1.42322, 1.04919], requires_grad=True), y_pred = tensor([42.54377, 33.67719, 24.81061, 15.94403], grad_fn=<MvBackward>), loss = 1.51038170\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 115: w = tensor([1.45243, 1.02818], requires_grad=True), y_pred = tensor([43.28445, 34.27287, 25.26129, 16.24971], grad_fn=<MvBackward>), loss = 1.33336782\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 120: w = tensor([1.48233, 1.04161], requires_grad=True), y_pred = tensor([42.59350, 33.78979, 24.98609, 16.18238], grad_fn=<MvBackward>), loss = 1.18204451\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 125: w = tensor([1.50884, 1.02733], requires_grad=True), y_pred = tensor([43.06691, 34.18049, 25.29407, 16.40766], grad_fn=<MvBackward>), loss = 1.05119181\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 130: w = tensor([1.53541, 1.03567], requires_grad=True), y_pred = tensor([42.60181, 33.86351, 25.12522, 16.38693], grad_fn=<MvBackward>), loss = 0.93700927\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 135: w = tensor([1.55940, 1.02587], requires_grad=True), y_pred = tensor([42.90114, 34.11973, 25.33832, 16.55691], grad_fn=<MvBackward>), loss = 0.83668095\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 140: w = tensor([1.58308, 1.03092], requires_grad=True), y_pred = tensor([42.58541, 33.91172, 25.23803, 16.56434], grad_fn=<MvBackward>), loss = 0.74805164\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 145: w = tensor([1.60474, 1.02409], requires_grad=True), y_pred = tensor([42.77173, 34.07975, 25.38776, 16.69578], grad_fn=<MvBackward>), loss = 0.66944206\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 150: w = tensor([1.62588, 1.02703], requires_grad=True), y_pred = tensor([42.55506, 33.94321, 25.33137, 16.71952], grad_fn=<MvBackward>), loss = 0.59950674\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 155: w = tensor([1.64540, 1.02219], requires_grad=True), y_pred = tensor([42.66828, 34.05339, 25.43850, 16.82362], grad_fn=<MvBackward>), loss = 0.53715217\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 160: w = tensor([1.66429, 1.02378], requires_grad=True), y_pred = tensor([42.51758, 33.96376, 25.40994, 16.85612], grad_fn=<MvBackward>), loss = 0.48146015\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 165: w = tensor([1.68186, 1.02029], requires_grad=True), y_pred = tensor([42.58377, 34.03600, 25.48822, 16.94045], grad_fn=<MvBackward>), loss = 0.43166173\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 170: w = tensor([1.69877, 1.02103], requires_grad=True), y_pred = tensor([42.47721, 33.97713, 25.47704, 16.97695], grad_fn=<MvBackward>), loss = 0.38709140\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 175: w = tensor([1.71457, 1.01845], requires_grad=True), y_pred = tensor([42.51338, 34.02448, 25.53557, 17.04667], grad_fn=<MvBackward>), loss = 0.34717295\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 180: w = tensor([1.72971, 1.01867], requires_grad=True), y_pred = tensor([42.43659, 33.98581, 25.53503, 17.08424], grad_fn=<MvBackward>), loss = 0.31140596\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 185: w = tensor([1.74391, 1.01672], requires_grad=True), y_pred = tensor([42.45380, 34.01683, 25.57986, 17.14289], grad_fn=<MvBackward>), loss = 0.27934441\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 190: w = tensor([1.75748, 1.01662], requires_grad=True), y_pred = tensor([42.39725, 33.99142, 25.58560, 17.17977], grad_fn=<MvBackward>), loss = 0.25059801\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 195: w = tensor([1.77024, 1.01511], requires_grad=True), y_pred = tensor([42.40269, 34.01173, 25.62078, 17.22982], grad_fn=<MvBackward>), loss = 0.22481927\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 200: w = tensor([1.78240, 1.01482], requires_grad=True), y_pred = tensor([42.36006, 33.99504, 25.63001, 17.26499], grad_fn=<MvBackward>), loss = 0.20169993\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 205: w = tensor([1.79385, 1.01363], requires_grad=True), y_pred = tensor([42.35838, 34.00832, 25.65827, 17.30822], grad_fn=<MvBackward>), loss = 0.18096074\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 210: w = tensor([1.80476, 1.01324], requires_grad=True), y_pred = tensor([42.32542, 33.99732, 25.66922, 17.34113], grad_fn=<MvBackward>), loss = 0.16235729\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 215: w = tensor([1.81504, 1.01227], requires_grad=True), y_pred = tensor([42.31963, 34.00603, 25.69242, 17.37882], grad_fn=<MvBackward>), loss = 0.14566770\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 220: w = tensor([1.82482, 1.01185], requires_grad=True), y_pred = tensor([42.29353, 33.99876, 25.70400, 17.40924], grad_fn=<MvBackward>), loss = 0.13069558\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 225: w = tensor([1.83405, 1.01104], requires_grad=True), y_pred = tensor([42.28552, 34.00446, 25.72339, 17.44233], grad_fn=<MvBackward>), loss = 0.11726224\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 230: w = tensor([1.84282, 1.01060], requires_grad=True), y_pred = tensor([42.26440, 33.99967, 25.73494, 17.47021], grad_fn=<MvBackward>), loss = 0.10521053\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 235: w = tensor([1.85111, 1.00993], requires_grad=True), y_pred = tensor([42.25534, 34.00337, 25.75140, 17.49942], grad_fn=<MvBackward>), loss = 0.09439759\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 240: w = tensor([1.85897, 1.00950], requires_grad=True), y_pred = tensor([42.23791, 34.00021, 25.76252, 17.52483], grad_fn=<MvBackward>), loss = 0.08469677\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 245: w = tensor([1.86641, 1.00892], requires_grad=True), y_pred = tensor([42.22856, 34.00261, 25.77667, 17.55073], grad_fn=<MvBackward>), loss = 0.07599217\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 250: w = tensor([1.87346, 1.00851], requires_grad=True), y_pred = tensor([42.21390, 34.00052, 25.78715, 17.57377], grad_fn=<MvBackward>), loss = 0.06818256\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 255: w = tensor([1.88014, 1.00801], requires_grad=True), y_pred = tensor([42.20472, 34.00208, 25.79945, 17.59681], grad_fn=<MvBackward>), loss = 0.06117591\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 260: w = tensor([1.88647, 1.00763], requires_grad=True), y_pred = tensor([42.19220, 34.00068, 25.80916, 17.61764], grad_fn=<MvBackward>), loss = 0.05488885\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 265: w = tensor([1.89246, 1.00719], requires_grad=True), y_pred = tensor([42.18345, 34.00169, 25.81994, 17.63818], grad_fn=<MvBackward>), loss = 0.04924808\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 270: w = tensor([1.89813, 1.00684], requires_grad=True), y_pred = tensor([42.17264, 34.00076, 25.82887, 17.65699], grad_fn=<MvBackward>), loss = 0.04418735\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 275: w = tensor([1.90351, 1.00646], requires_grad=True), y_pred = tensor([42.16444, 34.00140, 25.83836, 17.67532], grad_fn=<MvBackward>), loss = 0.03964623\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 280: w = tensor([1.90860, 1.00614], requires_grad=True), y_pred = tensor([42.15502, 34.00077, 25.84652, 17.69227], grad_fn=<MvBackward>), loss = 0.03557184\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 285: w = tensor([1.91342, 1.00580], requires_grad=True), y_pred = tensor([42.14744, 34.00118, 25.85492, 17.70866], grad_fn=<MvBackward>), loss = 0.03191639\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 290: w = tensor([1.91799, 1.00550], requires_grad=True), y_pred = tensor([42.13918, 34.00076, 25.86233, 17.72391], grad_fn=<MvBackward>), loss = 0.02863660\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 295: w = tensor([1.92232, 1.00520], requires_grad=True), y_pred = tensor([42.13222, 34.00101, 25.86980, 17.73858], grad_fn=<MvBackward>), loss = 0.02569385\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 300: w = tensor([1.92642, 1.00494], requires_grad=True), y_pred = tensor([42.12493, 34.00072, 25.87651, 17.75230], grad_fn=<MvBackward>), loss = 0.02305340\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 305: w = tensor([1.93030, 1.00467], requires_grad=True), y_pred = tensor([42.11858, 34.00087, 25.88315, 17.76544], grad_fn=<MvBackward>), loss = 0.02068412\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 310: w = tensor([1.93398, 1.00443], requires_grad=True), y_pred = tensor([42.11213, 34.00067, 25.88922, 17.77776], grad_fn=<MvBackward>), loss = 0.01855873\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 315: w = tensor([1.93747, 1.00419], requires_grad=True), y_pred = tensor([42.10637, 34.00076, 25.89515, 17.78953], grad_fn=<MvBackward>), loss = 0.01665138\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 320: w = tensor([1.94077, 1.00397], requires_grad=True), y_pred = tensor([42.10064, 34.00063, 25.90062, 17.80061], grad_fn=<MvBackward>), loss = 0.01494033\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 325: w = tensor([1.94389, 1.00376], requires_grad=True), y_pred = tensor([42.09541, 34.00066, 25.90591, 17.81116], grad_fn=<MvBackward>), loss = 0.01340491\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 330: w = tensor([1.94685, 1.00356], requires_grad=True), y_pred = tensor([42.09031, 34.00057, 25.91084, 17.82110], grad_fn=<MvBackward>), loss = 0.01202754\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 335: w = tensor([1.94966, 1.00337], requires_grad=True), y_pred = tensor([42.08558, 34.00058, 25.91557, 17.83055], grad_fn=<MvBackward>), loss = 0.01079139\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 340: w = tensor([1.95231, 1.00320], requires_grad=True), y_pred = tensor([42.08104, 34.00053, 25.92001, 17.83949], grad_fn=<MvBackward>), loss = 0.00968240\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 345: w = tensor([1.95483, 1.00303], requires_grad=True), y_pred = tensor([42.07679, 34.00052, 25.92424, 17.84797], grad_fn=<MvBackward>), loss = 0.00868748\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 350: w = tensor([1.95722, 1.00287], requires_grad=True), y_pred = tensor([42.07271, 34.00047, 25.92823, 17.85599], grad_fn=<MvBackward>), loss = 0.00779471\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 355: w = tensor([1.95947, 1.00272], requires_grad=True), y_pred = tensor([42.06890, 34.00047, 25.93203, 17.86359], grad_fn=<MvBackward>), loss = 0.00699359\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 360: w = tensor([1.96161, 1.00257], requires_grad=True), y_pred = tensor([42.06524, 34.00042, 25.93560, 17.87078], grad_fn=<MvBackward>), loss = 0.00627505\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 365: w = tensor([1.96364, 1.00244], requires_grad=True), y_pred = tensor([42.06182, 34.00042, 25.93901, 17.87761], grad_fn=<MvBackward>), loss = 0.00563018\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 370: w = tensor([1.96556, 1.00231], requires_grad=True), y_pred = tensor([42.05854, 34.00038, 25.94222, 17.88406], grad_fn=<MvBackward>), loss = 0.00505164\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 375: w = tensor([1.96737, 1.00219], requires_grad=True), y_pred = tensor([42.05547, 34.00037, 25.94528, 17.89019], grad_fn=<MvBackward>), loss = 0.00453248\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 380: w = tensor([1.96910, 1.00207], requires_grad=True), y_pred = tensor([42.05252, 34.00034, 25.94816, 17.89598], grad_fn=<MvBackward>), loss = 0.00406663\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 385: w = tensor([1.97073, 1.00196], requires_grad=True), y_pred = tensor([42.04977, 34.00034, 25.95090, 17.90147], grad_fn=<MvBackward>), loss = 0.00364873\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 390: w = tensor([1.97227, 1.00186], requires_grad=True), y_pred = tensor([42.04712, 34.00031, 25.95349, 17.90667], grad_fn=<MvBackward>), loss = 0.00327375\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 395: w = tensor([1.97374, 1.00176], requires_grad=True), y_pred = tensor([42.04465, 34.00030, 25.95595, 17.91160], grad_fn=<MvBackward>), loss = 0.00293732\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 400: w = tensor([1.97512, 1.00167], requires_grad=True), y_pred = tensor([42.04229, 34.00028, 25.95827, 17.91626], grad_fn=<MvBackward>), loss = 0.00263548\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 405: w = tensor([1.97643, 1.00158], requires_grad=True), y_pred = tensor([42.04005, 34.00026, 25.96047, 17.92068], grad_fn=<MvBackward>), loss = 0.00236459\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 410: w = tensor([1.97768, 1.00150], requires_grad=True), y_pred = tensor([42.03794, 34.00025, 25.96256, 17.92487], grad_fn=<MvBackward>), loss = 0.00212172\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 415: w = tensor([1.97886, 1.00142], requires_grad=True), y_pred = tensor([42.03594, 34.00024, 25.96453, 17.92883], grad_fn=<MvBackward>), loss = 0.00190360\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 420: w = tensor([1.97997, 1.00134], requires_grad=True), y_pred = tensor([42.03405, 34.00023, 25.96641, 17.93259], grad_fn=<MvBackward>), loss = 0.00170798\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 425: w = tensor([1.98103, 1.00127], requires_grad=True), y_pred = tensor([42.03225, 34.00022, 25.96818, 17.93615], grad_fn=<MvBackward>), loss = 0.00153252\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 430: w = tensor([1.98203, 1.00120], requires_grad=True), y_pred = tensor([42.03054, 34.00020, 25.96986, 17.93951], grad_fn=<MvBackward>), loss = 0.00137501\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 435: w = tensor([1.98298, 1.00114], requires_grad=True), y_pred = tensor([42.02893, 34.00019, 25.97145, 17.94271], grad_fn=<MvBackward>), loss = 0.00123367\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 440: w = tensor([1.98388, 1.00108], requires_grad=True), y_pred = tensor([42.02741, 34.00019, 25.97296, 17.94573], grad_fn=<MvBackward>), loss = 0.00110684\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 445: w = tensor([1.98473, 1.00102], requires_grad=True), y_pred = tensor([42.02594, 34.00016, 25.97437, 17.94859], grad_fn=<MvBackward>), loss = 0.00099315\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 450: w = tensor([1.98553, 1.00097], requires_grad=True), y_pred = tensor([42.02460, 34.00017, 25.97574, 17.95131], grad_fn=<MvBackward>), loss = 0.00089105\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 455: w = tensor([1.98630, 1.00092], requires_grad=True), y_pred = tensor([42.02329, 34.00015, 25.97701, 17.95388], grad_fn=<MvBackward>), loss = 0.00079952\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 460: w = tensor([1.98702, 1.00087], requires_grad=True), y_pred = tensor([42.02205, 34.00014, 25.97823, 17.95631], grad_fn=<MvBackward>), loss = 0.00071731\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 465: w = tensor([1.98771, 1.00082], requires_grad=True), y_pred = tensor([42.02092, 34.00015, 25.97939, 17.95862], grad_fn=<MvBackward>), loss = 0.00064363\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 470: w = tensor([1.98835, 1.00078], requires_grad=True), y_pred = tensor([42.01978, 34.00011, 25.98046, 17.96080], grad_fn=<MvBackward>), loss = 0.00057748\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 475: w = tensor([1.98897, 1.00074], requires_grad=True), y_pred = tensor([42.01876, 34.00014, 25.98150, 17.96288], grad_fn=<MvBackward>), loss = 0.00051811\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 480: w = tensor([1.98955, 1.00070], requires_grad=True), y_pred = tensor([42.01775, 34.00011, 25.98247, 17.96483], grad_fn=<MvBackward>), loss = 0.00046487\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 485: w = tensor([1.99010, 1.00066], requires_grad=True), y_pred = tensor([42.01683, 34.00012, 25.98340, 17.96669], grad_fn=<MvBackward>), loss = 0.00041709\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 490: w = tensor([1.99063, 1.00063], requires_grad=True), y_pred = tensor([42.01592, 34.00009, 25.98427, 17.96844], grad_fn=<MvBackward>), loss = 0.00037425\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 495: w = tensor([1.99112, 1.00060], requires_grad=True), y_pred = tensor([42.01510, 34.00010, 25.98511, 17.97011], grad_fn=<MvBackward>), loss = 0.00033581\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 500: w = tensor([1.99159, 1.00056], requires_grad=True), y_pred = tensor([42.01431, 34.00010, 25.98589, 17.97169], grad_fn=<MvBackward>), loss = 0.00030129\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 505: w = tensor([1.99203, 1.00053], requires_grad=True), y_pred = tensor([42.01354, 34.00008, 25.98663, 17.97318], grad_fn=<MvBackward>), loss = 0.00027033\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 510: w = tensor([1.99245, 1.00051], requires_grad=True), y_pred = tensor([42.01283, 34.00009, 25.98734, 17.97460], grad_fn=<MvBackward>), loss = 0.00024254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 515: w = tensor([1.99285, 1.00048], requires_grad=True), y_pred = tensor([42.01215, 34.00007, 25.98800, 17.97593], grad_fn=<MvBackward>), loss = 0.00021764\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 520: w = tensor([1.99323, 1.00045], requires_grad=True), y_pred = tensor([42.01151, 34.00008, 25.98864, 17.97721], grad_fn=<MvBackward>), loss = 0.00019526\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 525: w = tensor([1.99359, 1.00043], requires_grad=True), y_pred = tensor([42.01090, 34.00007, 25.98924, 17.97841], grad_fn=<MvBackward>), loss = 0.00017518\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 530: w = tensor([1.99392, 1.00041], requires_grad=True), y_pred = tensor([42.01033, 34.00007, 25.98981, 17.97955], grad_fn=<MvBackward>), loss = 0.00015719\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 535: w = tensor([1.99425, 1.00039], requires_grad=True), y_pred = tensor([42.00978, 34.00006, 25.99035, 17.98063], grad_fn=<MvBackward>), loss = 0.00014103\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 540: w = tensor([1.99455, 1.00037], requires_grad=True), y_pred = tensor([42.00927, 34.00006, 25.99086, 17.98165], grad_fn=<MvBackward>), loss = 0.00012652\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 545: w = tensor([1.99484, 1.00035], requires_grad=True), y_pred = tensor([42.00878, 34.00006, 25.99134, 17.98262], grad_fn=<MvBackward>), loss = 0.00011353\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 550: w = tensor([1.99511, 1.00033], requires_grad=True), y_pred = tensor([42.00831, 34.00005, 25.99179, 17.98354], grad_fn=<MvBackward>), loss = 0.00010187\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 555: w = tensor([1.99537, 1.00031], requires_grad=True), y_pred = tensor([42.00787, 34.00005, 25.99223, 17.98441], grad_fn=<MvBackward>), loss = 0.00009140\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 560: w = tensor([1.99561, 1.00029], requires_grad=True), y_pred = tensor([42.00747, 34.00006, 25.99264, 17.98523], grad_fn=<MvBackward>), loss = 0.00008202\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 565: w = tensor([1.99584, 1.00028], requires_grad=True), y_pred = tensor([42.00705, 34.00004, 25.99302, 17.98600], grad_fn=<MvBackward>), loss = 0.00007359\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 570: w = tensor([1.99606, 1.00026], requires_grad=True), y_pred = tensor([42.00670, 34.00005, 25.99340, 17.98675], grad_fn=<MvBackward>), loss = 0.00006602\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 575: w = tensor([1.99627, 1.00025], requires_grad=True), y_pred = tensor([42.00633, 34.00003, 25.99374, 17.98744], grad_fn=<MvBackward>), loss = 0.00005925\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 580: w = tensor([1.99647, 1.00024], requires_grad=True), y_pred = tensor([42.00601, 34.00005, 25.99408, 17.98811], grad_fn=<MvBackward>), loss = 0.00005316\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 585: w = tensor([1.99665, 1.00022], requires_grad=True), y_pred = tensor([42.00569, 34.00003, 25.99439, 17.98874], grad_fn=<MvBackward>), loss = 0.00004769\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 590: w = tensor([1.99683, 1.00021], requires_grad=True), y_pred = tensor([42.00538, 34.00003, 25.99468, 17.98933], grad_fn=<MvBackward>), loss = 0.00004279\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 595: w = tensor([1.99700, 1.00020], requires_grad=True), y_pred = tensor([42.00511, 34.00004, 25.99497, 17.98989], grad_fn=<MvBackward>), loss = 0.00003839\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 600: w = tensor([1.99716, 1.00019], requires_grad=True), y_pred = tensor([42.00483, 34.00003, 25.99523, 17.99043], grad_fn=<MvBackward>), loss = 0.00003444\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 605: w = tensor([1.99731, 1.00018], requires_grad=True), y_pred = tensor([42.00458, 34.00003, 25.99548, 17.99093], grad_fn=<MvBackward>), loss = 0.00003091\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 610: w = tensor([1.99745, 1.00017], requires_grad=True), y_pred = tensor([42.00435, 34.00003, 25.99572, 17.99141], grad_fn=<MvBackward>), loss = 0.00002773\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 615: w = tensor([1.99758, 1.00016], requires_grad=True), y_pred = tensor([42.00410, 34.00002, 25.99594, 17.99186], grad_fn=<MvBackward>), loss = 0.00002489\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 620: w = tensor([1.99771, 1.00015], requires_grad=True), y_pred = tensor([42.00390, 34.00003, 25.99616, 17.99229], grad_fn=<MvBackward>), loss = 0.00002233\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 625: w = tensor([1.99783, 1.00015], requires_grad=True), y_pred = tensor([42.00368, 34.00002, 25.99636, 17.99270], grad_fn=<MvBackward>), loss = 0.00002004\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 630: w = tensor([1.99795, 1.00014], requires_grad=True), y_pred = tensor([42.00349, 34.00002, 25.99655, 17.99308], grad_fn=<MvBackward>), loss = 0.00001798\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 635: w = tensor([1.99805, 1.00013], requires_grad=True), y_pred = tensor([42.00330, 34.00002, 25.99673, 17.99345], grad_fn=<MvBackward>), loss = 0.00001613\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 640: w = tensor([1.99816, 1.00012], requires_grad=True), y_pred = tensor([42.00313, 34.00002, 25.99691, 17.99379], grad_fn=<MvBackward>), loss = 0.00001448\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 645: w = tensor([1.99825, 1.00012], requires_grad=True), y_pred = tensor([42.00298, 34.00003, 25.99708, 17.99413], grad_fn=<MvBackward>), loss = 0.00001298\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 650: w = tensor([1.99835, 1.00011], requires_grad=True), y_pred = tensor([42.00280, 34.00001, 25.99722, 17.99443], grad_fn=<MvBackward>), loss = 0.00001165\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 655: w = tensor([1.99843, 1.00010], requires_grad=True), y_pred = tensor([42.00267, 34.00002, 25.99738, 17.99473], grad_fn=<MvBackward>), loss = 0.00001045\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 660: w = tensor([1.99852, 1.00010], requires_grad=True), y_pred = tensor([42.00251, 34.00001, 25.99751, 17.99500], grad_fn=<MvBackward>), loss = 0.00000938\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 665: w = tensor([1.99859, 1.00009], requires_grad=True), y_pred = tensor([42.00239, 34.00002, 25.99764, 17.99527], grad_fn=<MvBackward>), loss = 0.00000841\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 670: w = tensor([1.99867, 1.00009], requires_grad=True), y_pred = tensor([42.00225, 34.00001, 25.99776, 17.99552], grad_fn=<MvBackward>), loss = 0.00000755\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 675: w = tensor([1.99874, 1.00008], requires_grad=True), y_pred = tensor([42.00216, 34.00003, 25.99789, 17.99576], grad_fn=<MvBackward>), loss = 0.00000678\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 680: w = tensor([1.99881, 1.00008], requires_grad=True), y_pred = tensor([42.00201, 34.00000, 25.99799, 17.99597], grad_fn=<MvBackward>), loss = 0.00000608\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 685: w = tensor([1.99887, 1.00008], requires_grad=True), y_pred = tensor([42.00194, 34.00002, 25.99811, 17.99619], grad_fn=<MvBackward>), loss = 0.00000546\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 690: w = tensor([1.99893, 1.00007], requires_grad=True), y_pred = tensor([42.00181, 34.00000, 25.99820, 17.99639], grad_fn=<MvBackward>), loss = 0.00000489\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 695: w = tensor([1.99898, 1.00007], requires_grad=True), y_pred = tensor([42.00173, 34.00002, 25.99830, 17.99658], grad_fn=<MvBackward>), loss = 0.00000439\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 700: w = tensor([1.99904, 1.00006], requires_grad=True), y_pred = tensor([42.00164, 34.00002, 25.99839, 17.99677], grad_fn=<MvBackward>), loss = 0.00000394\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 705: w = tensor([1.99909, 1.00006], requires_grad=True), y_pred = tensor([42.00154, 34.00001, 25.99847, 17.99693], grad_fn=<MvBackward>), loss = 0.00000353\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 710: w = tensor([1.99914, 1.00006], requires_grad=True), y_pred = tensor([42.00146, 34.00000, 25.99855, 17.99709], grad_fn=<MvBackward>), loss = 0.00000317\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 715: w = tensor([1.99918, 1.00005], requires_grad=True), y_pred = tensor([42.00139, 34.00002, 25.99863, 17.99725], grad_fn=<MvBackward>), loss = 0.00000284\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 720: w = tensor([1.99923, 1.00005], requires_grad=True), y_pred = tensor([42.00131, 34.00000, 25.99870, 17.99739], grad_fn=<MvBackward>), loss = 0.00000255\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 725: w = tensor([1.99927, 1.00005], requires_grad=True), y_pred = tensor([42.00126, 34.00002, 25.99878, 17.99753], grad_fn=<MvBackward>), loss = 0.00000229\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 730: w = tensor([1.99931, 1.00005], requires_grad=True), y_pred = tensor([42.00117, 34.00000, 25.99883, 17.99766], grad_fn=<MvBackward>), loss = 0.00000206\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 735: w = tensor([1.99934, 1.00004], requires_grad=True), y_pred = tensor([42.00113, 34.00002, 25.99890, 17.99779], grad_fn=<MvBackward>), loss = 0.00000184\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 740: w = tensor([1.99938, 1.00004], requires_grad=True), y_pred = tensor([42.00105, 34.00000, 25.99895, 17.99790], grad_fn=<MvBackward>), loss = 0.00000166\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 745: w = tensor([1.99941, 1.00004], requires_grad=True), y_pred = tensor([42.00101, 34.00001, 25.99901, 17.99801], grad_fn=<MvBackward>), loss = 0.00000149\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 750: w = tensor([1.99944, 1.00004], requires_grad=True), y_pred = tensor([42.00096, 34.00001, 25.99907, 17.99812], grad_fn=<MvBackward>), loss = 0.00000133\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 755: w = tensor([1.99947, 1.00004], requires_grad=True), y_pred = tensor([42.00090, 34.00000, 25.99911, 17.99822], grad_fn=<MvBackward>), loss = 0.00000119\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 760: w = tensor([1.99950, 1.00003], requires_grad=True), y_pred = tensor([42.00086, 34.00001, 25.99916, 17.99831], grad_fn=<MvBackward>), loss = 0.00000107\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 765: w = tensor([1.99952, 1.00003], requires_grad=True), y_pred = tensor([42.00081, 34.00000, 25.99920, 17.99840], grad_fn=<MvBackward>), loss = 0.00000096\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 770: w = tensor([1.99955, 1.00003], requires_grad=True), y_pred = tensor([42.00076, 34.00000, 25.99924, 17.99848], grad_fn=<MvBackward>), loss = 0.00000086\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 775: w = tensor([1.99957, 1.00003], requires_grad=True), y_pred = tensor([42.00073, 34.00001, 25.99929, 17.99857], grad_fn=<MvBackward>), loss = 0.00000077\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 780: w = tensor([1.99960, 1.00003], requires_grad=True), y_pred = tensor([42.00068, 34.00000, 25.99932, 17.99864], grad_fn=<MvBackward>), loss = 0.00000069\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 785: w = tensor([1.99962, 1.00003], requires_grad=True), y_pred = tensor([42.00065, 34.00001, 25.99936, 17.99871], grad_fn=<MvBackward>), loss = 0.00000062\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 790: w = tensor([1.99964, 1.00002], requires_grad=True), y_pred = tensor([42.00062, 34.00001, 25.99940, 17.99878], grad_fn=<MvBackward>), loss = 0.00000056\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 795: w = tensor([1.99966, 1.00002], requires_grad=True), y_pred = tensor([42.00059, 34.00001, 25.99943, 17.99885], grad_fn=<MvBackward>), loss = 0.00000050\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 800: w = tensor([1.99967, 1.00002], requires_grad=True), y_pred = tensor([42.00055, 34.00000, 25.99945, 17.99891], grad_fn=<MvBackward>), loss = 0.00000045\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 805: w = tensor([1.99969, 1.00002], requires_grad=True), y_pred = tensor([42.00053, 34.00001, 25.99949, 17.99896], grad_fn=<MvBackward>), loss = 0.00000040\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 810: w = tensor([1.99971, 1.00002], requires_grad=True), y_pred = tensor([42.00049, 34.00000, 25.99951, 17.99902], grad_fn=<MvBackward>), loss = 0.00000036\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 815: w = tensor([1.99972, 1.00002], requires_grad=True), y_pred = tensor([42.00047, 34.00000, 25.99954, 17.99907], grad_fn=<MvBackward>), loss = 0.00000033\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 820: w = tensor([1.99974, 1.00002], requires_grad=True), y_pred = tensor([42.00045, 34.00000, 25.99956, 17.99912], grad_fn=<MvBackward>), loss = 0.00000029\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 825: w = tensor([1.99975, 1.00002], requires_grad=True), y_pred = tensor([42.00042, 34.00000, 25.99958, 17.99916], grad_fn=<MvBackward>), loss = 0.00000026\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 830: w = tensor([1.99977, 1.00002], requires_grad=True), y_pred = tensor([42.00040, 34.00000, 25.99960, 17.99921], grad_fn=<MvBackward>), loss = 0.00000024\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 835: w = tensor([1.99978, 1.00001], requires_grad=True), y_pred = tensor([42.00038, 34.00000, 25.99963, 17.99925], grad_fn=<MvBackward>), loss = 0.00000021\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 840: w = tensor([1.99979, 1.00001], requires_grad=True), y_pred = tensor([42.00035, 34.00000, 25.99965, 17.99929], grad_fn=<MvBackward>), loss = 0.00000019\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 845: w = tensor([1.99980, 1.00001], requires_grad=True), y_pred = tensor([42.00034, 34.00000, 25.99967, 17.99933], grad_fn=<MvBackward>), loss = 0.00000017\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 850: w = tensor([1.99981, 1.00001], requires_grad=True), y_pred = tensor([42.00032, 34.00000, 25.99968, 17.99936], grad_fn=<MvBackward>), loss = 0.00000015\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 855: w = tensor([1.99982, 1.00001], requires_grad=True), y_pred = tensor([42.00030, 34.00000, 25.99970, 17.99940], grad_fn=<MvBackward>), loss = 0.00000014\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 860: w = tensor([1.99983, 1.00001], requires_grad=True), y_pred = tensor([42.00028, 33.99999, 25.99971, 17.99943], grad_fn=<MvBackward>), loss = 0.00000012\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 865: w = tensor([1.99984, 1.00001], requires_grad=True), y_pred = tensor([42.00027, 34.00000, 25.99973, 17.99946], grad_fn=<MvBackward>), loss = 0.00000011\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 870: w = tensor([1.99985, 1.00001], requires_grad=True), y_pred = tensor([42.00026, 34.00000, 25.99975, 17.99949], grad_fn=<MvBackward>), loss = 0.00000010\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 875: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00023, 33.99999, 25.99975, 17.99951], grad_fn=<MvBackward>), loss = 0.00000009\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 880: w = tensor([1.99986, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00001, 25.99978, 17.99954], grad_fn=<MvBackward>), loss = 0.00000008\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 885: w = tensor([1.99987, 1.00001], requires_grad=True), y_pred = tensor([42.00021, 33.99999, 25.99978, 17.99956], grad_fn=<MvBackward>), loss = 0.00000007\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 890: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00024, 34.00002, 25.99981, 17.99960], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 895: w = tensor([1.99988, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 33.99998, 25.99980, 17.99961], grad_fn=<MvBackward>), loss = 0.00000006\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 900: w = tensor([1.99989, 1.00001], requires_grad=True), y_pred = tensor([42.00020, 34.00001, 25.99982, 17.99963], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 905: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 33.99999, 25.99982, 17.99965], grad_fn=<MvBackward>), loss = 0.00000005\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 910: w = tensor([1.99990, 1.00001], requires_grad=True), y_pred = tensor([42.00018, 34.00001, 25.99984, 17.99967], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 915: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00016, 34.00000, 25.99985, 17.99969], grad_fn=<MvBackward>), loss = 0.00000004\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 920: w = tensor([1.99991, 1.00001], requires_grad=True), y_pred = tensor([42.00015, 34.00000, 25.99985, 17.99970], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 925: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00014, 34.00000, 25.99986, 17.99972], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 930: w = tensor([1.99992, 1.00001], requires_grad=True), y_pred = tensor([42.00013, 33.99999, 25.99986, 17.99973], grad_fn=<MvBackward>), loss = 0.00000003\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 935: w = tensor([1.99992, 1.00000], requires_grad=True), y_pred = tensor([42.00013, 34.00000, 25.99988, 17.99975], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 940: w = tensor([1.99993, 1.00001], requires_grad=True), y_pred = tensor([42.00011, 33.99999, 25.99988, 17.99976], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 945: w = tensor([1.99993, 1.00000], requires_grad=True), y_pred = tensor([42.00012, 34.00000, 25.99989, 17.99977], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 950: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99989, 17.99978], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 955: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00011, 34.00000, 25.99990, 17.99980], grad_fn=<MvBackward>), loss = 0.00000002\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 960: w = tensor([1.99994, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99990, 17.99981], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 965: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00000, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 970: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 33.99999, 25.99991, 17.99982], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 975: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00010, 34.00001, 25.99993, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 980: w = tensor([1.99995, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99992, 17.99984], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00008, 34.00001, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00000, 25.99993, 17.99986], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00006, 34.00000, 25.99993, 17.99987], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([-0.00032, -0.00026])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "# def loss(y, y_pred):\n",
    "#     return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "         \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Использование модели:\n",
    "\n",
    "`torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "\n",
    "Parameters:\n",
    "* `in_features` – size of each input sample\n",
    "* `out_features` – size of each output sample\n",
    "* `bias` – If set to False, the layer will not learn an additive bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Prediction before training: f(5) = tensor([[ 6.43014],\n",
      "        [ 4.13109],\n",
      "        [ 1.83204],\n",
      "        [-0.46701]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# X_test = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(n_samples, n_features)\n",
    "\n",
    "# n_samples, n_features = X_test.shape\n",
    "# input_size = n_features\n",
    "# output_size = n_features\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(n_features, 1)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([4, 2])\n",
      "Y.shape = torch.Size([4])\n",
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "Prediction before training: f(tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])) = tensor([[-0.36370],\n",
      "        [-0.80204],\n",
      "        [-1.24038],\n",
      "        [-1.67872]], grad_fn=<MmBackward>)\n",
      "epoch 0: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 5: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 10: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 15: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 20: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 25: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 30: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 35: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 40: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 45: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 50: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 55: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 60: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 65: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 70: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 75: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 80: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 85: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 90: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 95: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 100: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 105: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 110: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 115: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 120: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 125: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 130: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 135: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 140: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 145: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 150: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 155: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 160: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 165: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 170: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 175: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 180: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 185: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 190: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 195: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 200: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 205: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 210: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 215: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 220: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 225: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 230: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 235: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 240: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 245: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 250: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 255: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 260: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 265: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 270: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 275: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 280: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 285: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 290: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 295: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 300: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 305: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 310: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 315: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 320: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 325: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 330: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 335: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 340: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 345: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 350: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 355: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 360: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 365: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 370: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 375: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 380: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 385: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 390: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 395: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 400: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 405: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 410: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\.conda\\envs\\pyTorch_1_5v2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:432: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 415: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 420: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 425: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 430: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 435: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 440: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 445: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 450: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 455: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 460: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 465: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 470: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 475: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 480: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 485: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 490: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 495: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 500: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 505: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 510: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 515: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 520: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 525: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 530: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 535: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 540: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 545: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 550: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 555: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 560: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 565: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 570: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 575: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 580: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 585: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 590: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 595: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 600: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 605: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 610: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 615: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 620: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 625: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 630: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 635: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 640: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 645: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 650: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 655: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 660: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 665: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 670: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 675: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 680: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 685: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 690: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 695: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 700: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 705: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 710: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 715: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 720: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 725: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 730: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 735: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 740: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 745: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 750: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 755: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 760: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 765: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 770: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 775: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 780: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 785: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 790: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 795: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 800: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 805: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 810: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 815: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 820: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 825: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 830: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 835: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 840: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 845: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 850: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 855: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 860: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 865: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 870: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 875: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 880: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 885: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 890: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 895: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 900: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 905: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 910: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 915: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 920: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 925: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 930: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 935: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 940: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 945: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 950: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 955: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 960: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 965: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 970: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 975: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 980: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 985: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 990: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 995: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n",
      "epoch 1000: w = tensor([1.99996, 1.00000], requires_grad=True), y_pred = tensor([42.00007, 34.00001, 25.99994, 17.99988], grad_fn=<MvBackward>), loss = 0.00000001\n",
      "gradient = tensor([ -130., -1700.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'X.shape = {X.shape}')\n",
    "X_samples, X_features = X.shape\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "print(f'Y.shape = {Y.shape}')\n",
    "Y_features = 1\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(X_features, Y_features)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f({X}) = {model(X)}')\n",
    "\n",
    "\n",
    "# # model (модель, в нашем случае: линейная регрессия)\n",
    "# # прямое распространение:\n",
    "# def forward(X):\n",
    "#     return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "criterion  = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, Y)\n",
    "\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Спасибо за внимание!\n",
    "\n",
    "---\n",
    "### Технический раздел:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * И Введение в искусственные нейронные сети\n",
    "     * Базовые понятия и история\n",
    " * И Машинное обучение и концепция глубокого обучения\n",
    " * И Почему глубокое обучение начало приносить плоды и активно использоваться только после 2010 г?\n",
    "     * Производительность оборудования\n",
    "     * Доступность наборов данных и тестов\n",
    "     * Алгоритмические достижения в области глубокого обучения\n",
    "         * Улчшенные подходы к регуляризации\n",
    "         * Улучшенные схемы инициализации весов\n",
    "         * (повтор) Усовершенствованные методы градиентного супска\n",
    "         \n",
    "\n",
    "* Обратное распространение ошибки\n",
    " * Оптимизация\n",
    "     * Стохастический градиентный спуск\n",
    "     * Усовершенствованные методы градиентного супска\n",
    "* Введение в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> next <em class=\"qs\"></em> qs line \n",
    "<br/> next <em class=\"an\"></em> an line \n",
    "<br/> next <em class=\"nt\"></em> an line \n",
    "<br/> next <em class=\"df\"></em> df line \n",
    "<br/> next <em class=\"ex\"></em> ex line \n",
    "<br/> next <em class=\"pl\"></em> pl line \n",
    "<br/> next <em class=\"mn\"></em> mn line \n",
    "<br/> next <em class=\"plmn\"></em> plmn line \n",
    "<br/> next <em class=\"hn\"></em> hn line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Работа с графом потока вычислений нужна  для того, чтобы решить __задачу обучения многослойной ИНС__. А эта задача требует после получения резуьтатов и оценки ошибки __выполнения обратного прохода__ дающего градиент ошибки для весов (параметров) модели и последующей процедуры оптимизации весов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"./img/ker_7.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "<img src=\"./img/ker_8.png\" alt=\"\" style=\"width: 500px;\"/>    \n",
    "<img src=\"./img/ker_9.png\" alt=\"\" style=\"width: 500px;\"/>        \n",
    "<img src=\"./img/ker_10.png\" alt=\"\" style=\"width: 500px;\"/>        \n",
    "<img src=\"./img/ker_11.png\" alt=\"\" style=\"width: 500px;\"/>            \n",
    "<img src=\"./img/ker_12.png\" alt=\"\" style=\"width: 500px;\"/>                \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
