{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1620559384951,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "0qOQwNlZbFiO",
    "outputId": "17123e05-c337-4d6c-d12f-2b9aa6f5b68c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        verbose: int = 100,\n",
    "        on_epoch_end: t.Callable[[], None] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        if test_dataloader:\n",
    "            test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "\n",
    "        if on_epoch_end:\n",
    "            on_epoch_end()\n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss, num_batches = 0, len(dataloader)\n",
    "    correct, total = 0, 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "\n",
    "        y_test = torch.flatten(y)\n",
    "        y_pred = torch.flatten(pred.argmax(1))\n",
    "        total += y_test.size(0)\n",
    "        correct += (y_pred == y_test).sum()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Error: \\n\"\n",
    "          f\"\\tAccuracy: {accuracy:>4f}, Loss: {avg_loss:>8f}\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.flatten(torch.vstack(y_test).detach().cpu()), torch.flatten(torch.vstack(y_pred).detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmWCBWxrBUB3"
   },
   "source": [
    "## 1. Генерирование русских имен при помощи RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
    "\n",
    "1.1 На основе файла name_rus.txt создайте датасет.\n",
    "  * Учтите, что имена могут иметь различную длину\n",
    "  * Добавьте 4 специальных токена:\n",
    "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
    "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
    "    * `<SOS>` для обозначения начала последовательности;\n",
    "    * `<EOS>` для обозначения конца последовательности.\n",
    "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
    "    * в начало последовательности добавьте токен `<SOS>`;\n",
    "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
    "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ.\n",
    "\n",
    "  Пример:\n",
    "  ```\n",
    "  s = 'The cat sat on the mat'\n",
    "  # преобразуем в индексы\n",
    "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  # получаем x и y (__getitem__)\n",
    "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
    "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем предсказывать каждую следующую букву в имени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesVocab:\n",
    "    PAD = \"<PAD>\"\n",
    "    PAD_IDX = 0\n",
    "    UNK = \"<UNK>\"\n",
    "    UNK_IDX = 1\n",
    "    SOS = \"<SOS>\"\n",
    "    SOS_IDX = 2\n",
    "    EOS = \"<EOS>\"\n",
    "    EOS_IDX = 3\n",
    "\n",
    "    def __init__(self, names: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for name in map(str.lower, names):\n",
    "            uniques.update(name)\n",
    "            max_len = max(len(name), max_len)\n",
    "\n",
    "        self.alphabet = [self.PAD, self.UNK, self.SOS, self.EOS, *uniques]\n",
    "        self.max_len = max_len + 2  # место для <SOS> и <EOS>\n",
    "\n",
    "        ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "        self.ch2i = defaultdict(lambda: self.UNK_IDX, ch2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        # улучшенный метод кодирования\n",
    "        # усложненный сдвиг позволяет сохранить первый и последний символ исходного слова\n",
    "        name = [*name, self.EOS]\n",
    "        if not shift:\n",
    "            name = [self.SOS, *name]\n",
    "        indices = [self.ch2i[ch] for ch in name]\n",
    "        indices += [self.PAD_IDX] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class NamesDataset:\n",
    "    names: t.List[str]\n",
    "    vocab: NamesVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path):\n",
    "        self.names = self.read_names(path)\n",
    "        self.vocab = NamesVocab(self.names)\n",
    "\n",
    "        self.data = torch.vstack([self.encode(name, shift=False) for name in self.names])\n",
    "        self.targets = torch.vstack([self.encode(name, shift=True) for name in self.names])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_names(path: Path) -> t.List[str]:\n",
    "        with open(path, encoding=\"cp1251\") as f:\n",
    "            return list(map(lambda s: s.strip().lower(), f))\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        return self.vocab.encode(name, shift=shift)\n",
    "\n",
    "    def decode(self, vector: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('авдокея',\n",
       " tensor([ 2, 15, 29, 31, 33,  5, 30, 19,  3,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([15, 29, 31, 33,  5, 30, 19,  3,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dataset = NamesDataset(DATA_DIR / \"name_rus.txt\")\n",
    "print(f\"n: {len(names_dataset)}\")\n",
    "(names_dataset.names[0], *names_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой метод кодирования позволяет сохранить на одну букву больше, чем предложенный в задании - теряем `<SOS>`, но сохраняем первый и последний символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590 398\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_names_dataset, test_names_dataset = train_test_split(names_dataset, train_part=0.8)\n",
    "print(len(train_names_dataset), len(test_names_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Создайте и обучите модель для генерации фамилии.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
    "  * Используйте рекуррентные слои;\n",
    "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
    "  * Примерная схема реализации метода `forward`:\n",
    "  ```\n",
    "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
    "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size]\n",
    "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
    "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
    "  ```\n",
    "\n",
    "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
    "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
    "  * Начальное скрытое состояние RNN `h_t = None`;\n",
    "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
    "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
    "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
    "\n",
    "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesRNNGenerator(nn.Module):\n",
    "    _STATE_T = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    rnn_state: _STATE_T\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_embeddings),\n",
    "        )\n",
    "        self.reset_rnn_state()\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.rnn_state = None\n",
    "\n",
    "    def keep_rnn_state(self, state: _STATE_T):\n",
    "        if isinstance(self.rnn, nn.LSTM):  # отдельная обработка скрытого состояния nn.LSTM\n",
    "            self.rnn_state = (state[0].detach(), state[1].detach())\n",
    "        else:\n",
    "            self.rnn_state = state.detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, rnn_state = self.rnn(x, self.rnn_state)\n",
    "        self.keep_rnn_state(rnn_state)  # полный контроль над скрытым состоянием внутри сети\n",
    "\n",
    "        x = self.fc(x)\n",
    "        # размерности отличаются от размерностей в задании:\n",
    "        # [batch_size x кол-во символов x вероятности для каждого символа]\n",
    "        # CrossEntropyLoss умеет так\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        # сбрасываем скрытое состояния при смене состояния сети (эпоха, оценка, использование)\n",
    "        self.reset_rnn_state()\n",
    "        return super().train(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# честная вероятность\n",
    "def true_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    pred -= pred.min()\n",
    "    return pred / pred.sum()\n",
    "\n",
    "\n",
    "# вероятность через softmax - это не вероятность, сильный скос в сторону большего значения\n",
    "def softmax_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.softmax(pred, 0)\n",
    "\n",
    "\n",
    "def generate_name(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    "        prompt: str = None,\n",
    "        prob: t.Callable[[torch.Tensor], torch.Tensor] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> str:\n",
    "    vocab = dataset.vocab\n",
    "    name_vec = [vocab.SOS_IDX]\n",
    "    if prompt:\n",
    "        name_vec += [vocab.ch2i[ch] for ch in prompt]\n",
    "\n",
    "    model.eval()  # сбрасываем скрытое состояние\n",
    "    for i in range(len(name_vec) - 1):  # рассчитываем скрытое состояние для prompt\n",
    "        x = torch.tensor([[name_vec[i]]], device=device)\n",
    "        model(x)  # скрытое состояние неявно изменяется внутри сети\n",
    "\n",
    "    # предсказываем каждый следующий за последним символ имени\n",
    "    for i in range(vocab.max_len - 2 - len(name_vec)):\n",
    "        x = torch.tensor([[name_vec[-1]]], device=device)\n",
    "        pred = model(x).squeeze()\n",
    "        if prob:  # случайный результат на основе вероятностей\n",
    "            next_ch_idx = torch.multinomial(prob(pred), 1)\n",
    "        else:  # лучший результат\n",
    "            next_ch_idx = pred.argmax()\n",
    "\n",
    "        if next_ch_idx == vocab.EOS_IDX:\n",
    "            break\n",
    "        name_vec.append(next_ch_idx.item())\n",
    "\n",
    "    return \"\".join(vocab.alphabet[i] for i in name_vec[1:])\n",
    "\n",
    "\n",
    "def on_epoch_end_generate_names(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    ") -> t.Callable[[], None]:\n",
    "    def _on_epoch_end() -> None:\n",
    "        # честное взятие лучшего варианта\n",
    "        const = generate_name(model, dataset, device=DEVICE)\n",
    "        # случайное взятие на основе вероятности\n",
    "        true_random = generate_name(model, dataset, prob=true_prob, device=DEVICE)\n",
    "        # случайное взятие на softmax преобразования\n",
    "        softmax_random = generate_name(model, dataset, prob=softmax_prob, device=DEVICE)\n",
    "        print(f\"\\tNames: {const} (max), {true_random} (prob), {softmax_random} (softmax)\")\n",
    "\n",
    "    return _on_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "names_gen_net = NamesRNNGenerator(\n",
    "    num_embeddings=len(names_dataset.vocab),\n",
    "    embedding_dim=8,  # для символов большие embedding'и не нужны (наверное)\n",
    "    rnn_hidden_size=64,\n",
    "    rnn_cls=nn.RNN,\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(names_gen_net.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_names_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_names_dataset, batch_size=128, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.518651  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.559201, Loss: 1.808978\n",
      "\tNames: ла (max), улрл (prob), уакоз (softmax)\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.848624  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.642535, Loss: 1.266336\n",
      "\tNames: на (max), эснуииа (prob), вуд (softmax)\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.360972  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.665451, Loss: 1.151004\n",
      "\tNames: ваня (max), язсдзлдд (prob), несня (softmax)\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.225344  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.677257, Loss: 1.101508\n",
      "\tNames: ваня (max), вочьсяиьяю<PAD>м (prob), ладмнн (softmax)\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.169704  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.676215, Loss: 1.064959\n",
      "\tNames: лета (max), втнеифеддр<PAD>н (prob), маня (softmax)\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.136116  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.688194, Loss: 1.035827\n",
      "\tNames: леня (max), хдлйнныиточь (prob), бюрася (softmax)\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.107509  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.691667, Loss: 1.010508\n",
      "\tNames: наня (max), хупрбзюодкыа (prob), расииалка (softmax)\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 1.175915  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.696354, Loss: 0.990140\n",
      "\tNames: леня (max), всвяькипчекц (prob), муся (softmax)\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.092392  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.698958, Loss: 0.969871\n",
      "\tNames: наня (max), хлиемшые<PAD>ц<PAD>е (prob), лисин (softmax)\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 1.001402  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.702778, Loss: 0.951411\n",
      "\tNames: лина (max), п (prob), малорлиа (softmax)\n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.990636  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.707292, Loss: 0.936075\n",
      "\tNames: лена (max), ввф (prob), линахн (softmax)\n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.993766  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.712326, Loss: 0.925111\n",
      "\tNames: ваня (max), киайкчгмддах (prob), селика (softmax)\n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.885734  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.715799, Loss: 0.913175\n",
      "\tNames: лина (max), лнняеьамемшз (prob), юня (softmax)\n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.970214  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.713715, Loss: 0.907105\n",
      "\tNames: лина (max), йельйй<PAD>дь<PAD><PAD>к (prob), тоня (softmax)\n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 1.030657  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.719965, Loss: 0.895959\n",
      "\tNames: лина (max), рлтя (prob), насорынтуф (softmax)\n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.843163  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.722396, Loss: 0.885013\n",
      "\tNames: лена (max), ейчмгаллтог (prob), аинорима (softmax)\n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.976200  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.720139, Loss: 0.877199\n",
      "\tNames: лика (max), еюмлййнкие (prob), тина (softmax)\n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.946599  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.725694, Loss: 0.869862\n",
      "\tNames: лена (max), свмчш<PAD>азхнмц (prob), леношилка (softmax)\n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.929981  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.729167, Loss: 0.863065\n",
      "\tNames: лавина (max), с (prob), станина (softmax)\n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.889581  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.730382, Loss: 0.858643\n",
      "\tNames: лена (max), тыцрыимг<PAD>вшб (prob), нилета (softmax)\n",
      "\n",
      "Epoch 21\n",
      "--------------------------------\n",
      "loss: 0.897315  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.731250, Loss: 0.853733\n",
      "\tNames: лика (max), силгкцнжьеиы (prob), фарюра (softmax)\n",
      "\n",
      "Epoch 22\n",
      "--------------------------------\n",
      "loss: 0.907483  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.732986, Loss: 0.848357\n",
      "\tNames: лика (max), фушуябрикебо (prob), торония (softmax)\n",
      "\n",
      "Epoch 23\n",
      "--------------------------------\n",
      "loss: 0.778462  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.734201, Loss: 0.842091\n",
      "\tNames: лена (max), ыегяйхв<PAD>евук (prob), тонусанья (softmax)\n",
      "\n",
      "Epoch 24\n",
      "--------------------------------\n",
      "loss: 0.814379  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.732292, Loss: 0.836617\n",
      "\tNames: лика (max), итоэшнчуйули (prob), лабюся (softmax)\n",
      "\n",
      "Epoch 25\n",
      "--------------------------------\n",
      "loss: 0.902766  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.738715, Loss: 0.830619\n",
      "\tNames: вастаня (max), июэшодивуейч (prob), дуня (softmax)\n",
      "\n",
      "Epoch 26\n",
      "--------------------------------\n",
      "loss: 0.840468  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.737326, Loss: 0.829916\n",
      "\tNames: лена (max), пуэчмбпечцир (prob), гелира (softmax)\n",
      "\n",
      "Epoch 27\n",
      "--------------------------------\n",
      "loss: 0.766146  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.736285, Loss: 0.825713\n",
      "\tNames: лина (max), азниыьышпиие (prob), вина (softmax)\n",
      "\n",
      "Epoch 28\n",
      "--------------------------------\n",
      "loss: 0.800217  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.740278, Loss: 0.820528\n",
      "\tNames: лина (max), нгц (prob), панюха (softmax)\n",
      "\n",
      "Epoch 29\n",
      "--------------------------------\n",
      "loss: 0.948448  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.739236, Loss: 0.817491\n",
      "\tNames: лина (max), тлоскынтюкят (prob), махеорка (softmax)\n",
      "\n",
      "Epoch 30\n",
      "--------------------------------\n",
      "loss: 0.755283  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.741319, Loss: 0.813649\n",
      "\tNames: лина (max), фггвакв (prob), дрим (softmax)\n",
      "\n",
      "Epoch 31\n",
      "--------------------------------\n",
      "loss: 0.852530  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.744097, Loss: 0.810037\n",
      "\tNames: марина (max), сиккианюичк<PAD> (prob), пекся (softmax)\n",
      "\n",
      "Epoch 32\n",
      "--------------------------------\n",
      "loss: 0.826829  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.741667, Loss: 0.805852\n",
      "\tNames: лена (max), икояже (prob), лена (softmax)\n",
      "\n",
      "Epoch 33\n",
      "--------------------------------\n",
      "loss: 0.889290  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.744097, Loss: 0.807561\n",
      "\tNames: лина (max), зсйбепе (prob), манита (softmax)\n",
      "\n",
      "Epoch 34\n",
      "--------------------------------\n",
      "loss: 0.795370  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.748090, Loss: 0.798169\n",
      "\tNames: мария (max), згюяруо (prob), нидася (softmax)\n",
      "\n",
      "Epoch 35\n",
      "--------------------------------\n",
      "loss: 0.755186  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.749132, Loss: 0.797602\n",
      "\tNames: лина (max), ппвбадюзчсяш (prob), гфрионыч (softmax)\n",
      "\n",
      "Epoch 36\n",
      "--------------------------------\n",
      "loss: 0.768674  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.749653, Loss: 0.793712\n",
      "\tNames: василина (max), длськдчецизь (prob), катонся (softmax)\n",
      "\n",
      "Epoch 37\n",
      "--------------------------------\n",
      "loss: 0.797282  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.747396, Loss: 0.792781\n",
      "\tNames: лена (max), ориекшчьвокс (prob), евмуля (softmax)\n",
      "\n",
      "Epoch 38\n",
      "--------------------------------\n",
      "loss: 0.817721  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.748785, Loss: 0.789018\n",
      "\tNames: лина (max), шсак<SOS>яиж (prob), ладюха (softmax)\n",
      "\n",
      "Epoch 39\n",
      "--------------------------------\n",
      "loss: 0.779589  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.751910, Loss: 0.786757\n",
      "\tNames: вастасия (max), титвьмр<PAD> (prob), тонтора (softmax)\n",
      "\n",
      "Epoch 40\n",
      "--------------------------------\n",
      "loss: 0.787496  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.750694, Loss: 0.789905\n",
      "\tNames: мария (max), ленапсцлцтхх (prob), даим (softmax)\n",
      "\n",
      "Epoch 41\n",
      "--------------------------------\n",
      "loss: 0.736678  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.751042, Loss: 0.783765\n",
      "\tNames: марина (max), жулйх (prob), беменюша (softmax)\n",
      "\n",
      "Epoch 42\n",
      "--------------------------------\n",
      "loss: 0.796588  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.754167, Loss: 0.782820\n",
      "\tNames: марина (max), пмрчнинмняжм (prob), мана (softmax)\n",
      "\n",
      "Epoch 43\n",
      "--------------------------------\n",
      "loss: 0.763418  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.756597, Loss: 0.780892\n",
      "\tNames: марина (max), к (prob), павюша (softmax)\n",
      "\n",
      "Epoch 44\n",
      "--------------------------------\n",
      "loss: 0.698100  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.753819, Loss: 0.786387\n",
      "\tNames: марислав (max), еугкяоьху (prob), алексей (softmax)\n",
      "\n",
      "Epoch 45\n",
      "--------------------------------\n",
      "loss: 0.801948  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.757465, Loss: 0.775530\n",
      "\tNames: мария (max), шяе<PAD>уфый (prob), марья (softmax)\n",
      "\n",
      "Epoch 46\n",
      "--------------------------------\n",
      "loss: 0.690639  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.755729, Loss: 0.776765\n",
      "\tNames: мария (max), онх<UNK>эремьгч (prob), нюся (softmax)\n",
      "\n",
      "Epoch 47\n",
      "--------------------------------\n",
      "loss: 0.814226  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.756771, Loss: 0.774159\n",
      "\tNames: марина (max), <PAD>дийыййан (prob), алюша (softmax)\n",
      "\n",
      "Epoch 48\n",
      "--------------------------------\n",
      "loss: 0.718959  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.756424, Loss: 0.771901\n",
      "\tNames: марина (max), пшсгрмтвурях (prob), малаха (softmax)\n",
      "\n",
      "Epoch 49\n",
      "--------------------------------\n",
      "loss: 0.744768  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.759028, Loss: 0.771324\n",
      "\tNames: марина (max), гпхцтвесцмт<PAD> (prob), ивана (softmax)\n",
      "\n",
      "Epoch 50\n",
      "--------------------------------\n",
      "loss: 0.723722  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.758854, Loss: 0.770327\n",
      "\tNames: мария (max), визжима (prob), генюся (softmax)\n",
      "\n",
      "Epoch 51\n",
      "--------------------------------\n",
      "loss: 0.765618  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.757986, Loss: 0.769107\n",
      "\tNames: мария (max), сжьяйфдумядш (prob), жвиктинка (softmax)\n",
      "\n",
      "Epoch 52\n",
      "--------------------------------\n",
      "loss: 0.736528  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760764, Loss: 0.767179\n",
      "\tNames: марина (max), сюскфмчеухфр (prob), дюка (softmax)\n",
      "\n",
      "Epoch 53\n",
      "--------------------------------\n",
      "loss: 0.724673  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760243, Loss: 0.765650\n",
      "\tNames: мария (max), всогшохйй<UNK><PAD>с (prob), вюняха (softmax)\n",
      "\n",
      "Epoch 54\n",
      "--------------------------------\n",
      "loss: 0.731852  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.757292, Loss: 0.767971\n",
      "\tNames: митя (max), жлемаюэо (prob), тюня (softmax)\n",
      "\n",
      "Epoch 55\n",
      "--------------------------------\n",
      "loss: 0.756582  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761979, Loss: 0.763142\n",
      "\tNames: марина (max), нфем<PAD>ушоймаа (prob), илаха (softmax)\n",
      "\n",
      "Epoch 56\n",
      "--------------------------------\n",
      "loss: 0.713173  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761458, Loss: 0.761634\n",
      "\tNames: марина (max), флкшю<UNK>рцушьп (prob), ларуся (softmax)\n",
      "\n",
      "Epoch 57\n",
      "--------------------------------\n",
      "loss: 0.705638  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.758681, Loss: 0.761400\n",
      "\tNames: марина (max), чгюьмай<UNK> (prob), марьян (softmax)\n",
      "\n",
      "Epoch 58\n",
      "--------------------------------\n",
      "loss: 0.770052  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.758333, Loss: 0.766471\n",
      "\tNames: лина (max), лзюию (prob), ванаша (softmax)\n",
      "\n",
      "Epoch 59\n",
      "--------------------------------\n",
      "loss: 0.678232  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762153, Loss: 0.760805\n",
      "\tNames: вася (max), р<SOS>ддияраяжи (prob), надя (softmax)\n",
      "\n",
      "Epoch 60\n",
      "--------------------------------\n",
      "loss: 0.669151  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761111, Loss: 0.759012\n",
      "\tNames: василина (max), энбябюсжяцдо (prob), ируля (softmax)\n",
      "\n",
      "Epoch 61\n",
      "--------------------------------\n",
      "loss: 0.700242  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760243, Loss: 0.760072\n",
      "\tNames: мариан (max), кеескыб<UNK><PAD>лда (prob), никола (softmax)\n",
      "\n",
      "Epoch 62\n",
      "--------------------------------\n",
      "loss: 0.678062  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.758854, Loss: 0.757201\n",
      "\tNames: марина (max), мрамес<PAD>ара (prob), маситк (softmax)\n",
      "\n",
      "Epoch 63\n",
      "--------------------------------\n",
      "loss: 0.662143  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762847, Loss: 0.756997\n",
      "\tNames: мариан (max), глонсацогонр (prob), наша (softmax)\n",
      "\n",
      "Epoch 64\n",
      "--------------------------------\n",
      "loss: 0.647303  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760417, Loss: 0.757320\n",
      "\tNames: мариан (max), сгдюш (prob), дора (softmax)\n",
      "\n",
      "Epoch 65\n",
      "--------------------------------\n",
      "loss: 0.699971  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760764, Loss: 0.757968\n",
      "\tNames: васильян (max), яирйапмь (prob), адюшака (softmax)\n",
      "\n",
      "Epoch 66\n",
      "--------------------------------\n",
      "loss: 0.704932  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760764, Loss: 0.757005\n",
      "\tNames: марина (max), бтяйобип (prob), михайя (softmax)\n",
      "\n",
      "Epoch 67\n",
      "--------------------------------\n",
      "loss: 0.709893  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761806, Loss: 0.758324\n",
      "\tNames: марина (max), сэблмчкокшла (prob), калида (softmax)\n",
      "\n",
      "Epoch 68\n",
      "--------------------------------\n",
      "loss: 0.646593  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760243, Loss: 0.756826\n",
      "\tNames: мариан (max), ехфбфубрьфшж (prob), нарина (softmax)\n",
      "\n",
      "Epoch 69\n",
      "--------------------------------\n",
      "loss: 0.689085  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761285, Loss: 0.756350\n",
      "\tNames: мариан (max), у<PAD>зурлодыьш<PAD> (prob), вавуня (softmax)\n",
      "\n",
      "Epoch 70\n",
      "--------------------------------\n",
      "loss: 0.637810  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.766667, Loss: 0.759375\n",
      "\tNames: василинка (max), бжкл<PAD> (prob), зоока (softmax)\n",
      "\n",
      "Epoch 71\n",
      "--------------------------------\n",
      "loss: 0.646004  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761458, Loss: 0.759399\n",
      "\tNames: василина (max), нмююхювильдг (prob), лавруся (softmax)\n",
      "\n",
      "Epoch 72\n",
      "--------------------------------\n",
      "loss: 0.660145  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764236, Loss: 0.756737\n",
      "\tNames: мариан (max), бусуяачаоикк (prob), марья (softmax)\n",
      "\n",
      "Epoch 73\n",
      "--------------------------------\n",
      "loss: 0.678318  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764931, Loss: 0.757945\n",
      "\tNames: василинка (max), ел<UNK>ш<UNK>чэтмжку (prob), игуля (softmax)\n",
      "\n",
      "Epoch 74\n",
      "--------------------------------\n",
      "loss: 0.694165  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765451, Loss: 0.758780\n",
      "\tNames: васильян (max), йдигиимьков (prob), филюша (softmax)\n",
      "\n",
      "Epoch 75\n",
      "--------------------------------\n",
      "loss: 0.692288  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761285, Loss: 0.757434\n",
      "\tNames: вася (max), зетячйлыхякр (prob), лико (softmax)\n",
      "\n",
      "Epoch 76\n",
      "--------------------------------\n",
      "loss: 0.674209  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762500, Loss: 0.758374\n",
      "\tNames: вася (max), снюкаэйрьквю (prob), апалилка (softmax)\n",
      "\n",
      "Epoch 77\n",
      "--------------------------------\n",
      "loss: 0.691960  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763194, Loss: 0.756448\n",
      "\tNames: мариан (max), ичгг<PAD>йоця<UNK>зх (prob), малинка (softmax)\n",
      "\n",
      "Epoch 78\n",
      "--------------------------------\n",
      "loss: 0.701943  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761458, Loss: 0.758876\n",
      "\tNames: марина (max), ефл<SOS>фкокюлиы (prob), артамей (softmax)\n",
      "\n",
      "Epoch 79\n",
      "--------------------------------\n",
      "loss: 0.693338  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760590, Loss: 0.755936\n",
      "\tNames: марина (max), юяр (prob), санюша (softmax)\n",
      "\n",
      "Epoch 80\n",
      "--------------------------------\n",
      "loss: 0.678221  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761111, Loss: 0.759473\n",
      "\tNames: мариан (max), зжбапе (prob), степанка (softmax)\n",
      "\n",
      "Epoch 81\n",
      "--------------------------------\n",
      "loss: 0.712676  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761285, Loss: 0.763461\n",
      "\tNames: мариан (max), помнкнтхра (prob), зина (softmax)\n",
      "\n",
      "Epoch 82\n",
      "--------------------------------\n",
      "loss: 0.696111  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763889, Loss: 0.760589\n",
      "\tNames: марина (max), згб (prob), натолиан (softmax)\n",
      "\n",
      "Epoch 83\n",
      "--------------------------------\n",
      "loss: 0.709553  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764410, Loss: 0.761178\n",
      "\tNames: василинка (max), влюлцеорфтйы (prob), вариславка (softmax)\n",
      "\n",
      "Epoch 84\n",
      "--------------------------------\n",
      "loss: 0.629974  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763368, Loss: 0.755702\n",
      "\tNames: василинка (max), ттбхрюычун (prob), васильянка (softmax)\n",
      "\n",
      "Epoch 85\n",
      "--------------------------------\n",
      "loss: 0.668912  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764931, Loss: 0.758585\n",
      "\tNames: василинка (max), ауелиеьинроч (prob), гелюня (softmax)\n",
      "\n",
      "Epoch 86\n",
      "--------------------------------\n",
      "loss: 0.681468  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.766840, Loss: 0.756939\n",
      "\tNames: васильянка (max), трлелюхесйрю (prob), федюша (softmax)\n",
      "\n",
      "Epoch 87\n",
      "--------------------------------\n",
      "loss: 0.630019  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765799, Loss: 0.755231\n",
      "\tNames: васильич (max), бблорогрьаэк (prob), нинаркука (softmax)\n",
      "\n",
      "Epoch 88\n",
      "--------------------------------\n",
      "loss: 0.659246  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765451, Loss: 0.756360\n",
      "\tNames: вася (max), эжчийвпяриых (prob), поликсана (softmax)\n",
      "\n",
      "Epoch 89\n",
      "--------------------------------\n",
      "loss: 0.668015  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763715, Loss: 0.758177\n",
      "\tNames: мариан (max), ейчтбурдубба (prob), нинельич (softmax)\n",
      "\n",
      "Epoch 90\n",
      "--------------------------------\n",
      "loss: 0.676774  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.770139, Loss: 0.760322\n",
      "\tNames: мариан (max), мюльыамжрдыж (prob), венюра (softmax)\n",
      "\n",
      "Epoch 91\n",
      "--------------------------------\n",
      "loss: 0.669955  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764236, Loss: 0.755175\n",
      "\tNames: василиса (max), дыьэпмшнют<UNK>ж (prob), минюша (softmax)\n",
      "\n",
      "Epoch 92\n",
      "--------------------------------\n",
      "loss: 0.632809  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764931, Loss: 0.758322\n",
      "\tNames: мариан (max), пфврю<UNK>иклябр (prob), эмилианка (softmax)\n",
      "\n",
      "Epoch 93\n",
      "--------------------------------\n",
      "loss: 0.607940  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765799, Loss: 0.757863\n",
      "\tNames: мариан (max), яф<PAD> (prob), манга (softmax)\n",
      "\n",
      "Epoch 94\n",
      "--------------------------------\n",
      "loss: 0.608783  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.767535, Loss: 0.757873\n",
      "\tNames: васильян (max), сл<SOS>хь (prob), зашаня (softmax)\n",
      "\n",
      "Epoch 95\n",
      "--------------------------------\n",
      "loss: 0.666912  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.766840, Loss: 0.759226\n",
      "\tNames: мариан (max), бибро<PAD>иеыч (prob), алюся (softmax)\n",
      "\n",
      "Epoch 96\n",
      "--------------------------------\n",
      "loss: 0.626475  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765799, Loss: 0.759124\n",
      "\tNames: мариам (max), рмтарыжьунлс (prob), николай (softmax)\n",
      "\n",
      "Epoch 97\n",
      "--------------------------------\n",
      "loss: 0.664307  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.767361, Loss: 0.761627\n",
      "\tNames: мариан (max), нночаиик (prob), константин (softmax)\n",
      "\n",
      "Epoch 98\n",
      "--------------------------------\n",
      "loss: 0.616383  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.766667, Loss: 0.759431\n",
      "\tNames: мариан (max), чеиюлтухаыи (prob), волюся (softmax)\n",
      "\n",
      "Epoch 99\n",
      "--------------------------------\n",
      "loss: 0.626282  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765278, Loss: 0.761911\n",
      "\tNames: мариан (max), миьмчт (prob), гуня (softmax)\n",
      "\n",
      "Epoch 100\n",
      "--------------------------------\n",
      "loss: 0.654616  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.766667, Loss: 0.766110\n",
      "\tNames: мариан (max), вюэйдасиичшч (prob), гания (softmax)\n",
      "\n",
      "CPU times: user 26.7 s, sys: 3.39 s, total: 30.1 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=100,\n",
    "    model=names_gen_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    on_epoch_end=on_epoch_end_generate_names(names_gen_net, names_dataset),\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <PAD>       1.00      1.00      1.00      3009\n",
      "       <EOS>       0.80      0.93      0.86       384\n",
      "           ы       1.00      0.00      0.00        19\n",
      "           к       0.34      0.20      0.25        92\n",
      "           ж       0.00      0.00      0.00         4\n",
      "           п       0.67      0.24      0.35        34\n",
      "           у       0.44      0.13      0.21        60\n",
      "           н       0.49      0.51      0.50       206\n",
      "           м       0.22      0.49      0.31        81\n",
      "           ш       0.22      0.23      0.22        52\n",
      "           ь       0.62      0.19      0.29        27\n",
      "           х       0.20      0.02      0.04        44\n",
      "           с       0.55      0.49      0.52       103\n",
      "           а       0.64      0.66      0.65       442\n",
      "           р       0.50      0.54      0.52       108\n",
      "           й       0.09      0.05      0.07        19\n",
      "           ц       1.00      0.00      0.00         1\n",
      "           я       0.60      0.49      0.53       134\n",
      "           г       1.00      0.07      0.12        30\n",
      "           и       0.42      0.40      0.41       176\n",
      "           ф       0.00      0.00      0.00        11\n",
      "           б       0.50      0.29      0.36        14\n",
      "           э       1.00      0.00      0.00         6\n",
      "           ч       0.83      0.86      0.84        28\n",
      "           л       0.56      0.35      0.43       156\n",
      "           ю       0.15      0.11      0.12        66\n",
      "           т       0.69      0.56      0.62       113\n",
      "           в       0.14      0.50      0.21        76\n",
      "           е       0.50      0.44      0.47       136\n",
      "           д       0.53      0.46      0.49        52\n",
      "           з       0.50      0.33      0.40         6\n",
      "           о       0.51      0.55      0.53        71\n",
      "\n",
      "    accuracy                           0.77      5760\n",
      "   macro avg       0.52      0.35      0.35      5760\n",
      "weighted avg       0.78      0.77      0.76      5760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(names_gen_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[names_dataset.vocab.alphabet[i] for i in y_test.unique().sort()[0]],\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно порадоваться за модель - она научилась успешно предсказывать `<PAD>` и `<EOS>`. Это была несложная задача"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мариан\n",
      "тася\n",
      "юна\n",
      "юния\n",
      "ангелинка\n",
      "ангелина\n"
     ]
    }
   ],
   "source": [
    "print(generate_name(names_gen_net, names_dataset, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prob=softmax_prob, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"юн\", device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"юн\", prob=softmax_prob, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"анг\", device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"анг\", prob=softmax_prob, device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что это если не убийца ChatGPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJf5iaA2fOTM"
   },
   "source": [
    "## 2. Генерирование текста при помощи RNN\n",
    "\n",
    "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
    "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
    "\n",
    "2.2 На основе выбранного произведения создайте датасет. \n",
    "\n",
    "Отличия от задачи 1:\n",
    "  * Токены `<SOS>`, `<EOS>` и `<UNK>` можно не добавлять;\n",
    "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета используется текст \"Анна Каренина\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVocab:\n",
    "    PAD = \"<PAD>\"\n",
    "    PAD_IDX = 0\n",
    "    UNK = \"<UNK>\"\n",
    "    UNK_IDX = 1\n",
    "\n",
    "    def __init__(self, seqs: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for seq in map(str.lower, seqs):\n",
    "            uniques.update(seq)\n",
    "            max_len = max(len(seq), max_len)\n",
    "\n",
    "        self.alphabet = [self.PAD, self.UNK, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "        self.ch2i = defaultdict(lambda: self.UNK_IDX, ch2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, seq: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in seq]\n",
    "        indices += [self.PAD_IDX] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class TextDataset:\n",
    "    seqs: t.List[str]\n",
    "    vocab: TextVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, *paths: Path, window: int, overlap: int = 0):\n",
    "        self.seqs = self.read_seqs(*paths, window=window, overlap=overlap)\n",
    "        self.vocab = TextVocab(self.seqs)\n",
    "        self.vocab.max_len -= 1\n",
    "\n",
    "        self.data = torch.vstack([self.encode(seq[:-1]) for seq in self.seqs])\n",
    "        self.targets = torch.vstack([self.encode(seq[1:]) for seq in self.seqs])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_seqs(*paths: Path, window: int, overlap: int = 0) -> t.List[str]:\n",
    "        text = \"\"\n",
    "        for path in paths:\n",
    "            with open(path, encoding=\"cp1251\") as f:\n",
    "                text += \" \" + \" \".join(map(lambda s: s.strip().lower(), f))\n",
    "\n",
    "        text = re.sub(r\"[^а-яё]\", repl=\" \", string=text)\n",
    "        text = text.replace(\"ё\", \"е\")\n",
    "        text = \" \".join(text.split())  # избавляемся от длинных пробельных последовательностей\n",
    "\n",
    "        seqs = []\n",
    "        for i in range(0, len(text), window):\n",
    "            # последовательности длины window с перекрытием overlap с обоих сторон\n",
    "            seqs.append(text[i:i + window + overlap])\n",
    "\n",
    "        # выбросим неполную последовательность - можем себе позволить\n",
    "        return seqs[:-1]\n",
    "\n",
    "    def encode(self, seq: str) -> torch.Tensor:\n",
    "        return self.vocab.encode(seq)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 96414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('лев николаевич толстой анна каренина мне отмщение и аз воздам часть ',\n",
       " tensor([26, 30, 29,  2,  9, 21,  4, 34, 26, 16, 30, 29, 21, 25,  2, 28, 34, 26,\n",
       "         10, 28, 34, 17,  2, 16,  9,  9, 16,  2,  4, 16, 15, 30,  9, 21,  9, 16,\n",
       "          2, 11,  9, 30,  2, 34, 28, 11, 32, 30,  9, 21, 30,  2, 21,  2, 16, 33,\n",
       "          2, 29, 34, 33, 31, 16, 11,  2, 25, 16, 10, 28, 12]),\n",
       " tensor([30, 29,  2,  9, 21,  4, 34, 26, 16, 30, 29, 21, 25,  2, 28, 34, 26, 10,\n",
       "         28, 34, 17,  2, 16,  9,  9, 16,  2,  4, 16, 15, 30,  9, 21,  9, 16,  2,\n",
       "         11,  9, 30,  2, 34, 28, 11, 32, 30,  9, 21, 30,  2, 21,  2, 16, 33,  2,\n",
       "         29, 34, 33, 31, 16, 11,  2, 25, 16, 10, 28, 12,  2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = filter(lambda x: x.suffix == \".txt\", (DATA_DIR / \"texts\").iterdir())\n",
    "text_dataset = TextDataset(*paths, window=64, overlap=4)\n",
    "print(f\"n: {len(text_dataset)}\")\n",
    "(text_dataset.seqs[0], *text_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лев николаевич толстой анна каренина мне отмщение и аз воздам часть ',\n",
       " 'сть первая все счастливые семьи похожи друг на друга каждая несчастл',\n",
       " 'астливая семья несчастлива по своему все смешалось в доме облонских ',\n",
       " 'ких жена узнала что муж был в связи с бывшею в их доме француженкою ',\n",
       " 'кою гувернанткой и объявила мужу что не может жить с ним в одном дом',\n",
       " ' доме положение это продолжалось уже третий день и мучительно чувств',\n",
       " 'вствовалось и самими супругами и всеми членами семьи и домочадцами в',\n",
       " 'ми все члены семьи и домочадцы чувствовали что нет смысла в их сожит',\n",
       " 'ожительстве и что на каждом постоялом дворе случайно сошедшиеся люди',\n",
       " 'люди более связаны между собой чем они члены семьи и домочадцы облон']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset.seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86773 9641\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_text_dataset, test_text_dataset = train_test_split(text_dataset, train_part=0.9)\n",
    "print(len(train_text_dataset), len(test_text_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Создайте и обучите модель для генерации текста\n",
    "  * Задача ставится точно так же как в 1.2;\n",
    "  * При необходимости можете применить:\n",
    "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
    "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNNGenerator(nn.Module):\n",
    "    _STATE_T = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    rnn_state: _STATE_T\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        # применить двухуровневые рекуррентные слои можно,\n",
    "        # было бы хорошо, если это как-то влияло на точность ну хоть чуть-чуть\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=0.25,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_embeddings),\n",
    "        )\n",
    "        self.reset_rnn_state()\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.rnn_state = None\n",
    "\n",
    "    def keep_rnn_state(self, state: _STATE_T):\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            self.rnn_state = (state[0].detach(), state[1].detach())\n",
    "        else:\n",
    "            self.rnn_state = state.detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, rnn_state = self.rnn(x, self.rnn_state)\n",
    "        self.keep_rnn_state(rnn_state)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        self.reset_rnn_state()\n",
    "        return super().train(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "text_gen_net = TextRNNGenerator(\n",
    "    num_embeddings=len(text_dataset.vocab),\n",
    "    embedding_dim=12,\n",
    "    rnn_hidden_size=64,\n",
    "    rnn_cls=nn.LSTM,\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(text_gen_net.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_text_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_text_dataset, batch_size=1024, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.557759  [    0/86773]\n",
      "loss: 2.324675  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.358620, Loss: 2.118479\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 2.203799  [    0/86773]\n",
      "loss: 2.057365  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.407857, Loss: 1.919583\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 2.040117  [    0/86773]\n",
      "loss: 1.986713  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.429971, Loss: 1.830987\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.930743  [    0/86773]\n",
      "loss: 1.917571  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.442321, Loss: 1.782366\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.861859  [    0/86773]\n",
      "loss: 1.843128  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.451525, Loss: 1.749601\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.855849  [    0/86773]\n",
      "loss: 1.856893  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.457213, Loss: 1.727185\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.839658  [    0/86773]\n",
      "loss: 1.831106  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.461237, Loss: 1.711253\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 1.789014  [    0/86773]\n",
      "loss: 1.790777  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.464351, Loss: 1.698475\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.828283  [    0/86773]\n",
      "loss: 1.812547  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.465919, Loss: 1.688803\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 1.769230  [    0/86773]\n",
      "loss: 1.736868  [64000/86773]\n",
      "Test Error: \n",
      "\tAccuracy: 0.469668, Loss: 1.679503\n",
      "\n",
      "CPU times: user 2min 14s, sys: 24.4 s, total: 2min 38s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=10,\n",
    "    model=text_gen_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вполне себе динамика обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.61      0.87      0.72    101889\n",
      "           ы       0.49      0.28      0.36      9488\n",
      "           к       0.62      0.35      0.45     18678\n",
      "           ж       0.51      0.22      0.31      5438\n",
      "           ъ       1.00      0.00      0.00       163\n",
      "           п       0.16      0.16      0.16     13529\n",
      "           у       0.79      0.12      0.21     14507\n",
      "           н       0.28      0.50      0.36     33215\n",
      "           с       0.19      0.32      0.23     27137\n",
      "           м       0.54      0.19      0.29     15232\n",
      "           ь       0.63      0.75      0.69     10157\n",
      "           х       0.44      0.03      0.05      4732\n",
      "           ш       0.64      0.35      0.46      4883\n",
      "           р       0.57      0.31      0.40     21647\n",
      "           а       0.52      0.51      0.52     41996\n",
      "           й       0.36      0.42      0.39      5524\n",
      "           ц       0.66      0.33      0.44      1797\n",
      "           я       0.68      0.44      0.54     10746\n",
      "           г       0.47      0.28      0.35      9673\n",
      "           и       0.45      0.31      0.37     34829\n",
      "           ф       0.78      0.20      0.32       731\n",
      "           б       0.51      0.13      0.21      9079\n",
      "           э       1.00      0.00      0.00      1403\n",
      "           ч       0.60      0.12      0.20      8460\n",
      "           л       0.50      0.47      0.48     25167\n",
      "           ю       0.75      0.16      0.26      3363\n",
      "           т       0.50      0.44      0.47     31054\n",
      "           в       0.30      0.36      0.33     24430\n",
      "           е       0.52      0.41      0.46     43848\n",
      "           д       0.44      0.36      0.40     15977\n",
      "           щ       0.44      0.11      0.18      1448\n",
      "           з       0.60      0.33      0.42      8653\n",
      "           о       0.47      0.61      0.53     58599\n",
      "\n",
      "    accuracy                           0.47    617472\n",
      "   macro avg       0.55      0.32      0.35    617472\n",
      "weighted avg       0.50      0.47      0.45    617472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(text_gen_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[text_dataset.vocab.alphabet[i] for i in y_test.unique().sort()[0]],\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
    "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова)\n",
    "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
    "  * После этого вы генерируете строку нужной длины аналогично 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "        model: TextRNNGenerator,\n",
    "        dataset: TextDataset,\n",
    "        prompt: str,  # стартовая строка\n",
    "        size: int,  # любая длина генерируемого текста\n",
    "        prob: t.Callable[[torch.Tensor], torch.Tensor] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> str:\n",
    "    vocab = dataset.vocab\n",
    "    text_vec = [vocab.ch2i[ch] for ch in prompt]\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(len(text_vec) - 1):\n",
    "        x = torch.tensor([[text_vec[i]]], device=device)\n",
    "        model(x)\n",
    "\n",
    "    for i in range(size - len(text_vec)):\n",
    "        x = torch.tensor([[text_vec[-1]]], device=device)\n",
    "        pred = model(x).squeeze()\n",
    "        if prob:\n",
    "            next_ch_idx = torch.multinomial(prob(pred), 1)\n",
    "        else:\n",
    "            next_ch_idx = pred.argmax()\n",
    "        text_vec.append(next_ch_idx.item())\n",
    "\n",
    "    return \"\".join(vocab.alphabet[i] for i in text_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "доброе утро...\n",
      "доброе утро выскупленных желает с стал сказала присад сказал вопроска внумному и воравшись совесанную голос его бога ус что два схирался голочому одной сигая петровников вы овосить и была и там особенно молод кись обстыхительным рочь был стало асгароску еду на него окак ни забвать у нему не спокрум  \n",
      "\n",
      "добрый вечер...\n",
      "добрый вечер правля расселось кободник и ничего обналась глазай да вместе волопли все как меге парту сим которых неостойную это не здоровных лицо помою говорилось гресной мне могла планком морожных на сказала чрезледияла что что с чертный ечиного мы как все верое вдруг которате ему везела князь и бы \n",
      "\n",
      "на каждом постоялом дворе...\n",
      "на каждом постоялом дворе позволить решилось его в смыхать приятели бойцы и попол с себе не сбезду врегя губвал меже сказал за таких новой тамов чисторно при он глаза заенского так дело в особенное что что она грючироди димилась только ведет потому родко его видите черточневался левин щелых лежал ли \n",
      "\n",
      "дело шло о том...\n",
      "дело шло о том не с ней эту как предстрякнуя с сполнести долго никогда зачем гусатся но в так все павали дворься неястой в говорил с приносила этих по лапался во в своих нужно водсском волос фам полновначеских кол говорил чувствовала на взголубушк допас если я с одну мерете бвой олда у ним шади то о \n",
      "\n",
      "как только...\n",
      "как только не до чувствие наполодяли паричан минутаща имеет на бесещирах деревна навые размал тем да нехлюдов а женщездранность в чину видел превраяется сказалось и родную марья в унучнею и только их она утишь не угусительным ныбука вы того что вы в старать мяс и пожинул и еще сказал приследон верть \n",
      "\n",
      "белая береза...\n",
      "белая береза чудара дутали гребе сь тет тут в которому помнил дринцы и личитое госодными вызявшись перед раз в коминенго что он дагакова крысым что это все налебныю ропского б он мысли отвечала так принеслова нехлюдов помню молодые что прелаженной просил мою помогала вабима меня раз нее осдемиться б \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\n",
    "    \"доброе утро\",\n",
    "    \"добрый вечер\",\n",
    "    \"на каждом постоялом дворе\",\n",
    "    \"дело шло о том\",\n",
    "    \"как только\",\n",
    "    \"белая береза\",\n",
    "]:\n",
    "    print(prompt + \"...\")\n",
    "    print(generate_text(text_gen_net, text_dataset, prompt + \" \", 300, prob=softmax_prob, device=DEVICE), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А это \"Убийца ChatGPT 2.0\"\n",
    "\n",
    "> As we can see, the generated text may not make any sense, however there are some words and phrases that seem to form an idea, for example...\n",
    "\n",
    "[Источник](https://towardsdatascience.com/text-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c#:~:text=As%20we%20can%20see%2C%20the%20generated%20text%20may%20not%20make%20any%20sense%2C%20however%20there%20are%20some%20words%20and%20phrases%20that%20seem%20to%20form%20an%20idea%2C%20for%20example%3A)\n",
    "\n",
    "Из хорошего, модель научилась вовремя расставлять пробелы - сочетания букв по длинам похожи на слова (самостоятельные и предлоги - если сильно фантазировать 🥳)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOt/b54+xoKtnmvuSlliKDY",
   "collapsed_sections": [],
   "name": "blank__08_rnn_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
