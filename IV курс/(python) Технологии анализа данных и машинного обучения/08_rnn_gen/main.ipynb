{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 849,
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1620559384951,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "0qOQwNlZbFiO",
    "outputId": "17123e05-c337-4d6c-d12f-2b9aa6f5b68c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        verbose: int = 100,\n",
    "        on_epoch_end: t.Callable[[], None] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        if test_dataloader:\n",
    "            test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "\n",
    "        if on_epoch_end:\n",
    "            on_epoch_end()\n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss, num_batches = 0, len(dataloader)\n",
    "    correct, total = 0, 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "\n",
    "        y_test = torch.flatten(y)\n",
    "        y_pred = torch.flatten(pred.argmax(1))\n",
    "        total += y_test.size(0)\n",
    "        correct += (y_pred == y_test).sum()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Error: \\n\"\n",
    "          f\"\\tAccuracy: {accuracy:>4f}, Loss: {avg_loss:>8f}\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.flatten(torch.vstack(y_test).detach().cpu()), torch.flatten(torch.vstack(y_pred).detach().cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmWCBWxrBUB3"
   },
   "source": [
    "## 1. Генерирование русских имен при помощи RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
    "\n",
    "1.1 На основе файла name_rus.txt создайте датасет.\n",
    "  * Учтите, что имена могут иметь различную длину\n",
    "  * Добавьте 4 специальных токена:\n",
    "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
    "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
    "    * `<SOS>` для обозначения начала последовательности;\n",
    "    * `<EOS>` для обозначения конца последовательности.\n",
    "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
    "    * в начало последовательности добавьте токен `<SOS>`;\n",
    "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
    "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ.\n",
    "\n",
    "  Пример:\n",
    "  ```\n",
    "  s = 'The cat sat on the mat'\n",
    "  # преобразуем в индексы\n",
    "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  # получаем x и y (__getitem__)\n",
    "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
    "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  ```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "outputs": [],
   "source": [
    "class NamesVocab:\n",
    "    PAD = \"<PAD>\"\n",
    "    PAD_IDX = 0\n",
    "    UNK = \"<UNK>\"\n",
    "    UNK_IDX = 1\n",
    "    SOS = \"<SOS>\"\n",
    "    SOS_IDX = 2\n",
    "    EOS = \"<EOS>\"\n",
    "    EOS_IDX = 3\n",
    "\n",
    "    def __init__(self, names: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for name in map(str.lower, names):\n",
    "            uniques.update(name)\n",
    "            max_len = max(len(name), max_len)\n",
    "\n",
    "        self.alphabet = [self.PAD, self.UNK, self.SOS, self.EOS, *uniques]\n",
    "        self.max_len = max_len + 2  # место для <SOS> и <EOS>\n",
    "\n",
    "        ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "        self.ch2i = defaultdict(lambda: self.UNK_IDX, ch2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        name = [*name, self.EOS]\n",
    "        if not shift:\n",
    "            name = [self.SOS, *name]\n",
    "        indices = [self.ch2i[ch] for ch in name]\n",
    "        indices += [self.PAD_IDX] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class NamesDataset:\n",
    "    names: t.List[str]\n",
    "    vocab: NamesVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path):\n",
    "        self.names = self.read_names(path)\n",
    "        self.vocab = NamesVocab(self.names)\n",
    "\n",
    "        self.data = torch.vstack([self.encode(name, shift=False) for name in self.names])\n",
    "        self.targets = torch.vstack([self.encode(name, shift=True) for name in self.names])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_names(path: Path) -> t.List[str]:\n",
    "        with open(path) as f:\n",
    "            return list(map(lambda s: s.strip().lower(), f))\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        return self.vocab.encode(name, shift=shift)\n",
    "\n",
    "    def decode(self, vector: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(vector)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1988\n"
     ]
    },
    {
     "data": {
      "text/plain": "('авдокея',\n tensor([ 2, 31, 21, 26, 30, 23,  8, 20,  3,  0,  0,  0,  0,  0,  0]),\n tensor([31, 21, 26, 30, 23,  8, 20,  3,  0,  0,  0,  0,  0,  0,  0]))"
     },
     "execution_count": 1181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dataset = NamesDataset(DATA_DIR / \"name_rus.txt\")\n",
    "print(f\"n: {len(names_dataset)}\")\n",
    "(names_dataset.names[0], *names_dataset[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590 398\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_names_dataset, test_names_dataset = train_test_split(names_dataset, train_part=0.8)\n",
    "print(len(train_names_dataset), len(test_names_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.2 Создайте и обучите модель для генерации фамилии.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
    "  * Используйте рекуррентные слои;\n",
    "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
    "  * Примерная схема реализации метода `forward`:\n",
    "  ```\n",
    "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
    "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size]\n",
    "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
    "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
    "  ```\n",
    "\n",
    "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
    "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
    "  * Начальное скрытое состояние RNN `h_t = None`;\n",
    "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
    "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
    "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
    "\n",
    "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "outputs": [],
   "source": [
    "class NamesRNNGenerator(nn.Module):\n",
    "    _STATE_T = t.Union[t.Optional[torch.Tensor], t.Tuple[t.Optional[torch.Tensor], t.Optional[torch.Tensor]]]\n",
    "    rnn_state: _STATE_T\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, num_embeddings),\n",
    "        )\n",
    "        self.reset_rnn_state()\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.rnn_state = None\n",
    "\n",
    "    def keep_rnn_state(self, state: _STATE_T):\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            self.rnn_state = state[0].detach(), state[1].detach()\n",
    "        else:\n",
    "            self.rnn_state = state.detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, rnn_state = self.rnn(x, self.rnn_state)\n",
    "        self.keep_rnn_state(rnn_state)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x.permute(0, 2, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "outputs": [],
   "source": [
    "def true_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    pred -= pred.min()\n",
    "    return pred / pred.sum()\n",
    "\n",
    "\n",
    "def softmax_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.softmax(pred, 0)\n",
    "\n",
    "\n",
    "def generate_name(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    "        start=\"\",\n",
    "        prob: t.Callable[[torch.Tensor], torch.Tensor] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> str:\n",
    "    len_start = len(start)\n",
    "    name = dataset.encode(start).to(device)\n",
    "    name[len_start + 1] = 0  # заменяем <EOS> на <PAD>\n",
    "\n",
    "    model.eval()\n",
    "    model.reset_rnn_state()\n",
    "    for i in range(name.size(0) - len_start - 2):\n",
    "        pred = model(name.unsqueeze(0)).squeeze()[:, len_start + i]\n",
    "        if prob:\n",
    "            next_ch_idx = torch.multinomial(prob(pred), 1)\n",
    "        else:\n",
    "            next_ch_idx = pred.argmax()\n",
    "\n",
    "        if next_ch_idx == NamesVocab.EOS_IDX:\n",
    "            break\n",
    "        name[len_start + i + 1] = next_ch_idx\n",
    "\n",
    "    return dataset.decode(name).replace(NamesVocab.SOS, \"\")\n",
    "\n",
    "\n",
    "def on_epoch_end_generate_names(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    ") -> t.Callable[[], None]:\n",
    "    def _on_epoch_end() -> None:\n",
    "        const = generate_name(model, dataset, device=DEVICE)\n",
    "        true_random = generate_name(model, dataset, prob=true_prob, device=DEVICE)\n",
    "        softmax_random = generate_name(model, dataset, prob=softmax_prob, device=DEVICE)\n",
    "        print(f\"\\tNames: {const} (max), {true_random} (prob), {softmax_random} (softmax)\")\n",
    "\n",
    "    return _on_epoch_end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "names_gen_net = NamesRNNGenerator(\n",
    "    num_embeddings=len(names_dataset.vocab),\n",
    "    embedding_dim=8,\n",
    "    rnn_hidden_size=64,\n",
    "    rnn_cls=nn.RNN,\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(names_gen_net.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_names_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_names_dataset, batch_size=128)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.594028  [    0/ 1590]\n",
      "loss: 2.128230  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.588945, Loss: 1.591906\n",
      "\tNames: а (max), итуисннарь (prob), лев (softmax)\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.557775  [    0/ 1590]\n",
      "loss: 1.351737  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.649246, Loss: 1.252368\n",
      "\tNames: а (max), ергфчшышшю (prob), па (softmax)\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.308698  [    0/ 1590]\n",
      "loss: 1.181783  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.669179, Loss: 1.151966\n",
      "\tNames: линя (max), шы (prob), ллира (softmax)\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.068193  [    0/ 1590]\n",
      "loss: 1.040174  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.676382, Loss: 1.101062\n",
      "\tNames: а (max), ук (prob), дня (softmax)\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.066873  [    0/ 1590]\n",
      "loss: 1.104704  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.678392, Loss: 1.075477\n",
      "\tNames: а (max), ебпукиаырабти (prob), брашвома (softmax)\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.162529  [    0/ 1590]\n",
      "loss: 1.067770  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.686600, Loss: 1.055635\n",
      "\tNames: линя (max), ггжпктыхмьктч (prob), са (softmax)\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.189549  [    0/ 1590]\n",
      "loss: 1.036665  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.689950, Loss: 1.061177\n",
      "\tNames: линя (max), осруовв (prob), а (softmax)\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 1.226713  [    0/ 1590]\n",
      "loss: 1.037322  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.694472, Loss: 1.035335\n",
      "\tNames: линя (max), зу (prob), ньмилюшиарила (softmax)\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.088484  [    0/ 1590]\n",
      "loss: 1.001848  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.691792, Loss: 1.030185\n",
      "\tNames: лина (max), юяхндаюя (prob), хвима (softmax)\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 1.093011  [    0/ 1590]\n",
      "loss: 1.011654  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.693132, Loss: 1.019863\n",
      "\tNames: линя (max), снмлнфьо (prob), фьдуляня (softmax)\n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.966830  [    0/ 1590]\n",
      "loss: 1.098072  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.690620, Loss: 1.024265\n",
      "\tNames: лина (max), ммюаиюоуашхйх (prob), рюша (softmax)\n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 1.038577  [    0/ 1590]\n",
      "loss: 0.929713  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.693635, Loss: 1.009721\n",
      "\tNames: ся (max), цфуксвхякньгй (prob), лаша (softmax)\n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 1.187051  [    0/ 1590]\n",
      "loss: 0.959235  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.693635, Loss: 1.009987\n",
      "\tNames: линя (max), фтшэс (prob), а (softmax)\n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 1.061944  [    0/ 1590]\n",
      "loss: 1.049330  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.691960, Loss: 1.007972\n",
      "\tNames: линя (max), мягдргвв (prob), илия (softmax)\n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.978979  [    0/ 1590]\n",
      "loss: 1.035316  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.693467, Loss: 1.005576\n",
      "\tNames: линя (max), эюнерфьбеч (prob), та (softmax)\n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 1.075458  [    0/ 1590]\n",
      "loss: 1.176772  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.695645, Loss: 0.995010\n",
      "\tNames: ля (max), эдбедлжюшб (prob), ка (softmax)\n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.950644  [    0/ 1590]\n",
      "loss: 1.143229  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.694640, Loss: 1.007377\n",
      "\tNames: ся (max), шташжкясхнс (prob), а (softmax)\n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.984233  [    0/ 1590]\n",
      "loss: 1.208533  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.699497, Loss: 0.985389\n",
      "\tNames: ся (max), ждьбаижмлюпчя (prob), дока (softmax)\n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.974279  [    0/ 1590]\n",
      "loss: 1.000622  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.693467, Loss: 0.995337\n",
      "\tNames: линя (max), куа (prob), кса (softmax)\n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.999725  [    0/ 1590]\n",
      "loss: 1.028397  [  800/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.696650, Loss: 0.996370\n",
      "\tNames: ся (max), бб (prob), ксла (softmax)\n",
      "\n",
      "CPU times: total: 9.88 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=names_gen_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    on_epoch_end=on_epoch_end_generate_names(names_gen_net, names_dataset),\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <PAD>       1.00      1.00      1.00      3110\n",
      "       <EOS>       0.62      0.97      0.76       398\n",
      "           ф       1.00      0.00      0.00        12\n",
      "           н       0.25      0.52      0.34       210\n",
      "           п       1.00      0.00      0.00        36\n",
      "           ц       1.00      0.00      0.00         1\n",
      "           е       0.30      0.26      0.28       142\n",
      "           и       0.18      0.27      0.22       183\n",
      "           ю       1.00      0.00      0.00        71\n",
      "           г       1.00      0.00      0.00        33\n",
      "           й       1.00      0.00      0.00        20\n",
      "           э       1.00      0.00      0.00         6\n",
      "           ы       1.00      0.00      0.00        20\n",
      "           х       1.00      0.00      0.00        46\n",
      "           б       1.00      0.00      0.00        15\n",
      "           ш       0.29      0.41      0.34        56\n",
      "           л       0.44      0.11      0.17       160\n",
      "           т       0.00      0.00      0.00       118\n",
      "           я       0.27      0.62      0.38       136\n",
      "           в       0.12      0.62      0.20        77\n",
      "           ж       1.00      0.00      0.00         4\n",
      "           к       0.17      0.01      0.02        98\n",
      "           ь       1.00      0.00      0.00        29\n",
      "           ч       0.73      0.63      0.68        30\n",
      "           д       1.00      0.00      0.00        54\n",
      "           р       0.00      0.00      0.00       115\n",
      "           з       1.00      0.00      0.00         8\n",
      "           с       0.26      0.12      0.16       104\n",
      "           о       1.00      0.00      0.00        77\n",
      "           а       0.51      0.55      0.53       457\n",
      "           м       1.00      0.00      0.00        82\n",
      "           у       1.00      0.00      0.00        62\n",
      "\n",
      "    accuracy                           0.69      5970\n",
      "   macro avg       0.69      0.19      0.16      5970\n",
      "weighted avg       0.75      0.69      0.66      5970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(names_gen_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[names_dataset.vocab.alphabet[i] for i in y_test.unique().sort()[0]],\n",
    "    zero_division=True,\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ангелиня\n",
      "ангелдатешца\n",
      "ангелин\n"
     ]
    }
   ],
   "source": [
    "print(generate_name(names_gen_net, names_dataset, start=\"ангел\", device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, start=\"ангел\", prob=true_prob, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, start=\"ангел\", prob=softmax_prob, device=DEVICE))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJf5iaA2fOTM"
   },
   "source": [
    "## 2. Генерирование текста при помощи RNN\n",
    "\n",
    "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
    "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
    "\n",
    "2.2 На основе выбранного произведения создайте датасет. \n",
    "\n",
    "Отличия от задачи 1:\n",
    "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
    "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием).\n",
    "\n",
    "2.3 Создайте и обучите модель для генерации текста\n",
    "  * Задача ставится точно так же как в 1.2;\n",
    "  * При необходимости можете применить:\n",
    "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
    "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "\n",
    "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
    "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова) \n",
    "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
    "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOt/b54+xoKtnmvuSlliKDY",
   "collapsed_sections": [],
   "name": "blank__08_rnn_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
