{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1620559384951,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "0qOQwNlZbFiO",
    "outputId": "17123e05-c337-4d6c-d12f-2b9aa6f5b68c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        verbose: int = 100,\n",
    "        on_epoch_end: t.Callable[[], None] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        if test_dataloader:\n",
    "            test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "\n",
    "        if on_epoch_end:\n",
    "            on_epoch_end()\n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss, num_batches = 0, len(dataloader)\n",
    "    correct, total = 0, 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "\n",
    "        y_test = torch.flatten(y)\n",
    "        y_pred = torch.flatten(pred.argmax(1))\n",
    "        total += y_test.size(0)\n",
    "        correct += (y_pred == y_test).sum()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Error: \\n\"\n",
    "          f\"\\tAccuracy: {accuracy:>4f}, Loss: {avg_loss:>8f}\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.flatten(torch.vstack(y_test).detach().cpu()), torch.flatten(torch.vstack(y_pred).detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmWCBWxrBUB3"
   },
   "source": [
    "## 1. Генерирование русских имен при помощи RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
    "\n",
    "1.1 На основе файла name_rus.txt создайте датасет.\n",
    "  * Учтите, что имена могут иметь различную длину\n",
    "  * Добавьте 4 специальных токена:\n",
    "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
    "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
    "    * `<SOS>` для обозначения начала последовательности;\n",
    "    * `<EOS>` для обозначения конца последовательности.\n",
    "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
    "    * в начало последовательности добавьте токен `<SOS>`;\n",
    "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
    "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ.\n",
    "\n",
    "  Пример:\n",
    "  ```\n",
    "  s = 'The cat sat on the mat'\n",
    "  # преобразуем в индексы\n",
    "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  # получаем x и y (__getitem__)\n",
    "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
    "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Будем предсказывать каждую следующую букву в имени:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesVocab:\n",
    "    PAD = \"<PAD>\"\n",
    "    PAD_IDX = 0\n",
    "    UNK = \"<UNK>\"\n",
    "    UNK_IDX = 1\n",
    "    SOS = \"<SOS>\"\n",
    "    SOS_IDX = 2\n",
    "    EOS = \"<EOS>\"\n",
    "    EOS_IDX = 3\n",
    "\n",
    "    def __init__(self, names: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for name in map(str.lower, names):\n",
    "            uniques.update(name)\n",
    "            max_len = max(len(name), max_len)\n",
    "\n",
    "        self.alphabet = [self.PAD, self.UNK, self.SOS, self.EOS, *uniques]\n",
    "        self.max_len = max_len + 2  # место для <SOS> и <EOS>\n",
    "\n",
    "        ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "        self.ch2i = defaultdict(lambda: self.UNK_IDX, ch2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        # улучшенный метод кодирования\n",
    "        # усложненный сдвиг позволяет сохранить первый и последний символ исходного слова\n",
    "        name = [*name, self.EOS]\n",
    "        if not shift:\n",
    "            name = [self.SOS, *name]\n",
    "        indices = [self.ch2i[ch] for ch in name]\n",
    "        indices += [self.PAD_IDX] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class NamesDataset:\n",
    "    names: t.List[str]\n",
    "    vocab: NamesVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path):\n",
    "        self.names = self.read_names(path)\n",
    "        self.vocab = NamesVocab(self.names)\n",
    "\n",
    "        self.data = torch.vstack([self.encode(name, shift=False) for name in self.names])\n",
    "        self.targets = torch.vstack([self.encode(name, shift=True) for name in self.names])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_names(path: Path) -> t.List[str]:\n",
    "        with open(path, encoding=\"cp1251\") as f:\n",
    "            return list(map(lambda s: s.strip().lower(), f))\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        return self.vocab.encode(name, shift=shift)\n",
    "\n",
    "    def decode(self, vector: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('авдокея',\n",
       " tensor([ 2, 26, 13, 20, 16,  7, 29, 22,  3,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([26, 13, 20, 16,  7, 29, 22,  3,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dataset = NamesDataset(DATA_DIR / \"name_rus.txt\")\n",
    "print(f\"n: {len(names_dataset)}\")\n",
    "(names_dataset.names[0], *names_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Такой метод кодирования позволяет сохранить на одну букву больше, чем предложенный в задании - теряем `<SOS>`, но сохраняем первый и последний символ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590 398\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_names_dataset, test_names_dataset = train_test_split(names_dataset, train_part=0.8)\n",
    "print(len(train_names_dataset), len(test_names_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Создайте и обучите модель для генерации фамилии.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
    "  * Используйте рекуррентные слои;\n",
    "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
    "  * Примерная схема реализации метода `forward`:\n",
    "  ```\n",
    "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
    "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size]\n",
    "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
    "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
    "  ```\n",
    "\n",
    "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
    "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
    "  * Начальное скрытое состояние RNN `h_t = None`;\n",
    "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
    "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
    "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
    "\n",
    "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesRNNGenerator(nn.Module):\n",
    "    _STATE_T = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    rnn_state: _STATE_T\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_embeddings),\n",
    "        )\n",
    "        self.reset_rnn_state()\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.rnn_state = None\n",
    "\n",
    "    def keep_rnn_state(self, state: _STATE_T):\n",
    "        if isinstance(self.rnn, nn.LSTM):  # отдельная обработка скрытого состояния nn.LSTM\n",
    "            self.rnn_state = state[0].detach(), state[1].detach()\n",
    "        else:\n",
    "            self.rnn_state = state.detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, rnn_state = self.rnn(x, self.rnn_state)\n",
    "        self.keep_rnn_state(rnn_state)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        # размерности отличаются от размерностей в задании:\n",
    "        # [batch_size x кол-во символов x вероятности для каждого символа]\n",
    "        # CrossEntropyLoss умеет так\n",
    "        return x.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# честная вероятность\n",
    "def true_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    pred -= pred.min()\n",
    "    return pred / pred.sum()\n",
    "\n",
    "\n",
    "# вероятность через softmax - это не вероятность, сильный скос в сторону большего значения\n",
    "def softmax_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.softmax(pred, 0)\n",
    "\n",
    "\n",
    "def generate_name(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    "        prompt=\"\",\n",
    "        prob: t.Callable[[torch.Tensor], torch.Tensor] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> str:\n",
    "    len_start = len(prompt)\n",
    "    name = dataset.encode(prompt).to(device)\n",
    "    name[len_start + 1] = 0  # заменяем <EOS> на <PAD>\n",
    "\n",
    "    model.eval()\n",
    "    model.reset_rnn_state()\n",
    "    for i in range(name.size(0) - len_start - 2):\n",
    "        # здесь передаем все слово, а не по 1-ой букве\n",
    "        # не зря же модель учили словами, а не буквами...\n",
    "        pred = model(name.unsqueeze(0)).squeeze()[:, len_start + i]\n",
    "        if prob:  # случайность\n",
    "            next_ch_idx = torch.multinomial(prob(pred), 1)\n",
    "        else:  # честное взятие лучшего варианта\n",
    "            next_ch_idx = pred.argmax()\n",
    "\n",
    "        if next_ch_idx == NamesVocab.EOS_IDX:\n",
    "            break\n",
    "        name[len_start + i + 1] = next_ch_idx\n",
    "\n",
    "    return dataset.decode(name).replace(NamesVocab.SOS, \"\")\n",
    "\n",
    "\n",
    "def on_epoch_end_generate_names(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    ") -> t.Callable[[], None]:\n",
    "    def _on_epoch_end() -> None:\n",
    "        # честное взятие лучшего варианта\n",
    "        const = generate_name(model, dataset, device=DEVICE)\n",
    "        # случайное взятие на основе вероятности\n",
    "        true_random = generate_name(model, dataset, prob=true_prob, device=DEVICE)\n",
    "        # случайное взятие на softmax преобразования\n",
    "        softmax_random = generate_name(model, dataset, prob=softmax_prob, device=DEVICE)\n",
    "        print(f\"\\tNames: {const} (max), {true_random} (prob), {softmax_random} (softmax)\")\n",
    "\n",
    "    return _on_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "names_gen_net = NamesRNNGenerator(\n",
    "    num_embeddings=len(names_dataset.vocab),\n",
    "    embedding_dim=8,  # для символов большие embedding'и не нужны (наверное)\n",
    "    rnn_hidden_size=64,\n",
    "    rnn_cls=nn.LSTM,\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(names_gen_net.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_names_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_names_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.494831  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.529816, Loss: 1.912179\n",
      "\tNames:  (max), аоа (prob), лпэьг (softmax)\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.987921  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.578559, Loss: 1.577202\n",
      "\tNames: а (max), тале (prob), ы (softmax)\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.672587  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.608710, Loss: 1.442052\n",
      "\tNames: а (max), ухйч (prob), с (softmax)\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.637822  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.639531, Loss: 1.313473\n",
      "\tNames: а (max), руеьхилчзичй (prob), дабюшяй (softmax)\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.303850  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.664824, Loss: 1.211651\n",
      "\tNames: а (max), нююо (prob), гтсыя (softmax)\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.203625  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.677052, Loss: 1.144651\n",
      "\tNames: а (max),  (prob), нжаня (softmax)\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.085940  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.676884, Loss: 1.113376\n",
      "\tNames: аня (max), кмжгцьюфумокь (prob), ткма (softmax)\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 1.131636  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.681742, Loss: 1.083748\n",
      "\tNames: аня (max), чштечичзкхннм (prob), ття (softmax)\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.058267  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.687772, Loss: 1.067291\n",
      "\tNames: аня (max), уазнгпунам (prob), я (softmax)\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 1.090989  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.685595, Loss: 1.057040\n",
      "\tNames: аня (max), гцк (prob), молька (softmax)\n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.964861  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.692462, Loss: 1.048042\n",
      "\tNames: аня (max), ееея (prob), ашикга (softmax)\n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.986785  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.692630, Loss: 1.039757\n",
      "\tNames: аня (max), дадрхдшююоввш (prob), аделя (softmax)\n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 1.083475  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.692127, Loss: 1.030732\n",
      "\tNames: аня (max), сннитыншпгцдй (prob), а (softmax)\n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 1.007874  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.694640, Loss: 1.030147\n",
      "\tNames: аня (max), аиэреюи (prob), антрунуйкия (softmax)\n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 1.053198  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.691122, Loss: 1.024069\n",
      "\tNames: аня (max), срляеьоляшйсй (prob), счшероха (softmax)\n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.981023  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.694807, Loss: 1.015623\n",
      "\tNames: аня (max), шыбмнюуккмяхл (prob), амлюнянин (softmax)\n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 1.080860  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.695812, Loss: 1.014742\n",
      "\tNames: аня (max), юшоднцк (prob), лисялен (softmax)\n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 1.108083  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.695812, Loss: 1.006074\n",
      "\tNames: аня (max), юйыцьрсмпишгх (prob), кстедабач (softmax)\n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 1.004632  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.696650, Loss: 0.999227\n",
      "\tNames: аня (max), тьх (prob), а (softmax)\n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 1.184873  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.694640, Loss: 0.993254\n",
      "\tNames: аня (max), ярбнобкхллг (prob), етилтмия (softmax)\n",
      "\n",
      "Epoch 21\n",
      "--------------------------------\n",
      "loss: 1.024972  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.697152, Loss: 0.988814\n",
      "\tNames: аня (max), везыджуйпикюш (prob), ядоднка (softmax)\n",
      "\n",
      "Epoch 22\n",
      "--------------------------------\n",
      "loss: 0.978891  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.697152, Loss: 0.991699\n",
      "\tNames: аня (max), риктффп (prob), аткофа (softmax)\n",
      "\n",
      "Epoch 23\n",
      "--------------------------------\n",
      "loss: 1.053505  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.699665, Loss: 0.988576\n",
      "\tNames: ана (max), итбятркгф (prob), тхдусиша (softmax)\n",
      "\n",
      "Epoch 24\n",
      "--------------------------------\n",
      "loss: 0.985349  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.699665, Loss: 0.980080\n",
      "\tNames: аня (max), тютоггссй (prob), жаня (softmax)\n",
      "\n",
      "Epoch 25\n",
      "--------------------------------\n",
      "loss: 1.058748  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.700000, Loss: 0.981137\n",
      "\tNames: ана (max), кшфда (prob), ирьюка (softmax)\n",
      "\n",
      "Epoch 26\n",
      "--------------------------------\n",
      "loss: 0.915813  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.700503, Loss: 0.973821\n",
      "\tNames: аня (max), ехвшхлцуйол (prob), ехоня (softmax)\n",
      "\n",
      "Epoch 27\n",
      "--------------------------------\n",
      "loss: 0.986339  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.698492, Loss: 0.973878\n",
      "\tNames: ана (max), геьшодмоювейх (prob), аднтус (softmax)\n",
      "\n",
      "Epoch 28\n",
      "--------------------------------\n",
      "loss: 1.043992  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.703015, Loss: 0.979476\n",
      "\tNames: анана (max), е (prob), ироза (softmax)\n",
      "\n",
      "Epoch 29\n",
      "--------------------------------\n",
      "loss: 0.964306  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.702680, Loss: 0.968898\n",
      "\tNames: ана (max), мбыгитымрадуп (prob), льусивасендич (softmax)\n",
      "\n",
      "Epoch 30\n",
      "--------------------------------\n",
      "loss: 0.981211  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.704523, Loss: 0.963933\n",
      "\tNames: ана (max), рюяуяюгячрньх (prob), иня (softmax)\n",
      "\n",
      "CPU times: user 9.62 s, sys: 907 ms, total: 10.5 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=30,\n",
    "    model=names_gen_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    on_epoch_end=on_epoch_end_generate_names(names_gen_net, names_dataset),\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <PAD>       1.00      1.00      1.00      3110\n",
      "       <EOS>       0.77      0.94      0.84       398\n",
      "           у       1.00      0.02      0.03        62\n",
      "           ф       1.00      0.00      0.00        12\n",
      "           б       1.00      0.00      0.00        15\n",
      "           к       0.21      0.11      0.15        98\n",
      "           л       0.50      0.11      0.18       160\n",
      "           с       0.28      0.07      0.11       104\n",
      "           ш       0.27      0.34      0.30        56\n",
      "           п       1.00      0.00      0.00        36\n",
      "           и       0.21      0.23      0.22       183\n",
      "           в       0.14      0.03      0.04        77\n",
      "           т       0.36      0.07      0.11       118\n",
      "           ы       1.00      0.00      0.00        20\n",
      "           о       0.35      0.09      0.14        77\n",
      "           э       1.00      0.00      0.00         6\n",
      "           м       1.00      0.00      0.00        82\n",
      "           ч       0.90      0.60      0.72        30\n",
      "           д       1.00      0.00      0.00        54\n",
      "           з       1.00      0.00      0.00         8\n",
      "           я       0.30      0.56      0.39       136\n",
      "           ж       1.00      0.00      0.00         4\n",
      "           й       1.00      0.00      0.00        20\n",
      "           х       1.00      0.00      0.00        46\n",
      "           а       0.33      0.71      0.45       457\n",
      "           ц       1.00      0.00      0.00         1\n",
      "           р       0.16      0.13      0.14       115\n",
      "           е       0.35      0.30      0.32       142\n",
      "           н       0.25      0.55      0.34       210\n",
      "           ю       1.00      0.00      0.00        71\n",
      "           г       1.00      0.00      0.00        33\n",
      "           ь       1.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.70      5970\n",
      "   macro avg       0.70      0.18      0.17      5970\n",
      "weighted avg       0.76      0.70      0.67      5970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(names_gen_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[names_dataset.vocab.alphabet[i] for i in y_test.unique().sort()[0]],\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можно порадоваться за модель - она научилась успешно предсказывать `<PAD>` и `<EOS>`. Это была несложная задача"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "юната\n",
      "юнаша\n",
      "юна\n"
     ]
    }
   ],
   "source": [
    "print(generate_name(names_gen_net, names_dataset, prompt=\"юн\", device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"юн\", prob=true_prob, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"юн\", prob=softmax_prob, device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что это если не убийца ChatGPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJf5iaA2fOTM"
   },
   "source": [
    "## 2. Генерирование текста при помощи RNN\n",
    "\n",
    "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
    "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
    "\n",
    "2.2 На основе выбранного произведения создайте датасет. \n",
    "\n",
    "Отличия от задачи 1:\n",
    "  * Токены `<SOS>`, `<EOS>` и `<UNK>` можно не добавлять;\n",
    "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве датасета используется текст \"Анна Каренина\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVocab:\n",
    "    PAD = \"<PAD>\"\n",
    "    PAD_IDX = 0\n",
    "    UNK = \"<UNK>\"\n",
    "    UNK_IDX = 1\n",
    "\n",
    "    def __init__(self, seqs: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for seq in map(str.lower, seqs):\n",
    "            uniques.update(seq)\n",
    "            max_len = max(len(seq), max_len)\n",
    "\n",
    "        self.alphabet = [self.PAD, self.UNK, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "        self.ch2i = defaultdict(lambda: self.UNK_IDX, ch2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, seq: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in seq]\n",
    "        indices += [self.PAD_IDX] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class TextDataset:\n",
    "    seqs: t.List[str]\n",
    "    vocab: TextVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path, window: int, overlap: int = 0):\n",
    "        self.seqs = self.read_seqs(path, window=window, overlap=overlap)\n",
    "        self.vocab = TextVocab(self.seqs)\n",
    "\n",
    "        self.data = torch.vstack([self.encode(seq[:-1]) for seq in self.seqs])\n",
    "        self.targets = torch.vstack([self.encode(seq[1:]) for seq in self.seqs])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_seqs(path: Path, window: int, overlap: int = 0) -> t.List[str]:\n",
    "        with open(path, encoding=\"cp1251\") as f:\n",
    "            text = list(map(lambda s: s.strip().lower(), f))\n",
    "\n",
    "        text = \" \".join(text)\n",
    "        text = re.sub(r\"[^а-яё]\", repl=\" \", string=text)\n",
    "        text = \" \".join(text.split())  # избавляемся от длинных пробельных последовательностей\n",
    "\n",
    "        seqs = []\n",
    "        for i in range(0, len(text), window):\n",
    "            # последовательности длины window с перекрытием overlap с обоих сторон\n",
    "            seqs.append(text[i:i + window + overlap])\n",
    "\n",
    "        return seqs\n",
    "\n",
    "    def encode(self, seq: str) -> torch.Tensor:\n",
    "        return self.vocab.encode(seq)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 25099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('лев николаевич толстой анна каренина мне отмщение и аз воздам часть ',\n",
       " tensor([ 7, 30, 13,  6, 31, 12,  8, 16,  7, 26, 30, 13, 12, 19,  6, 14, 16,  7,\n",
       "          9, 14, 16, 24,  6, 26, 31, 31, 26,  6,  8, 26, 29, 30, 31, 12, 31, 26,\n",
       "          6, 18, 31, 30,  6, 16, 14, 18,  2, 30, 31, 12, 30,  6, 12,  6, 26, 20,\n",
       "          6, 13, 16, 20, 21, 26, 18,  6, 19, 26,  9, 14, 34,  0]),\n",
       " tensor([30, 13,  6, 31, 12,  8, 16,  7, 26, 30, 13, 12, 19,  6, 14, 16,  7,  9,\n",
       "         14, 16, 24,  6, 26, 31, 31, 26,  6,  8, 26, 29, 30, 31, 12, 31, 26,  6,\n",
       "         18, 31, 30,  6, 16, 14, 18,  2, 30, 31, 12, 30,  6, 12,  6, 26, 20,  6,\n",
       "         13, 16, 20, 21, 26, 18,  6, 19, 26,  9, 14, 34,  6,  0]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset = TextDataset(DATA_DIR / \"anna_karenina.txt\", window=64, overlap=4)\n",
    "print(f\"n: {len(text_dataset)}\")\n",
    "(text_dataset.seqs[0], *text_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лев николаевич толстой анна каренина мне отмщение и аз воздам часть ',\n",
       " 'сть первая все счастливые семьи похожи друг на друга каждая несчастл',\n",
       " 'астливая семья несчастлива по своему все смешалось в доме облонских ',\n",
       " 'ких жена узнала что муж был в связи с бывшею в их доме француженкою ',\n",
       " 'кою гувернанткой и объявила мужу что не может жить с ним в одном дом',\n",
       " ' доме положение это продолжалось уже третий день и мучительно чувств',\n",
       " 'вствовалось и самими супругами и всеми членами семьи и домочадцами в',\n",
       " 'ми все члены семьи и домочадцы чувствовали что нет смысла в их сожит',\n",
       " 'ожительстве и что на каждом постоялом дворе случайно сошедшиеся люди',\n",
       " 'люди более связаны между собой чем они члены семьи и домочадцы облон']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset.seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20079 5020\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_text_dataset, test_text_dataset = train_test_split(text_dataset, train_part=0.8)\n",
    "print(len(train_text_dataset), len(test_text_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Создайте и обучите модель для генерации текста\n",
    "  * Задача ставится точно так же как в 1.2;\n",
    "  * При необходимости можете применить:\n",
    "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
    "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNNGenerator(nn.Module):\n",
    "    _STATE_T = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    rnn_state: _STATE_T\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        # применить двухуровневые рекуррентные слои можно,\n",
    "        # было бы хорошо, если это как-то влияло на точность ну хоть чуть-чуть\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size, num_layers=2, dropout=0.25)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_embeddings),\n",
    "        )\n",
    "        self.reset_rnn_state()\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.rnn_state = None\n",
    "\n",
    "    def keep_rnn_state(self, state: _STATE_T):\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            self.rnn_state = state[0].detach(), state[1].detach()\n",
    "        else:\n",
    "            self.rnn_state = state.detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, rnn_state = self.rnn(x, self.rnn_state)\n",
    "        self.keep_rnn_state(rnn_state)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "text_gen_net = TextRNNGenerator(\n",
    "    num_embeddings=len(text_dataset.vocab),\n",
    "    embedding_dim=16,\n",
    "    rnn_hidden_size=64,\n",
    "    rnn_cls=nn.LSTM,\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(text_gen_net.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_text_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_text_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.563401  [    0/20079]\n",
      "loss: 2.622643  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.256237, Loss: 2.550842\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 2.585614  [    0/20079]\n",
      "loss: 2.513025  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.267832, Loss: 2.458726\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 2.482423  [    0/20079]\n",
      "loss: 2.491698  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.269589, Loss: 2.429040\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 2.458759  [    0/20079]\n",
      "loss: 2.475520  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.270081, Loss: 2.422082\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 2.470808  [    0/20079]\n",
      "loss: 2.416837  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.270081, Loss: 2.419302\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 2.436810  [    0/20079]\n",
      "loss: 2.431002  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.269844, Loss: 2.417486\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 2.424822  [    0/20079]\n",
      "loss: 2.452827  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.270076, Loss: 2.416049\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 2.459981  [    0/20079]\n",
      "loss: 2.407877  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.270187, Loss: 2.415489\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 2.456297  [    0/20079]\n",
      "loss: 2.442845  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.270055, Loss: 2.414720\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 2.421776  [    0/20079]\n",
      "loss: 2.420926  [12800/20079]\n",
      "Test Error: \n",
      "\tAccuracy: 0.270049, Loss: 2.414003\n",
      "\n",
      "CPU times: user 46.8 s, sys: 11 s, total: 57.7 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=10,\n",
    "    model=text_gen_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=200,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ну тут либо постановка задачи невыполнимая, либо модель не обучается"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <PAD>       1.00      1.00      1.00      5020\n",
      "           щ       1.00      0.00      0.00       807\n",
      "           у       1.00      0.00      0.00      7684\n",
      "           ф       1.00      0.00      0.00       289\n",
      "           б       1.00      0.00      0.00      4962\n",
      "                   0.32      0.79      0.46     56390\n",
      "           л       1.00      0.00      0.00     14217\n",
      "           к       1.00      0.00      0.00      9621\n",
      "           с       0.10      0.37      0.15     14917\n",
      "           п       1.00      0.00      0.00      6787\n",
      "           ш       1.00      0.00      0.00      2446\n",
      "           и       1.00      0.00      0.00     18485\n",
      "           в       1.00      0.00      0.00     13055\n",
      "           т       0.29      0.35      0.32     16653\n",
      "           ы       0.29      0.28      0.28      5149\n",
      "           о       0.30      0.47      0.37     32323\n",
      "           э       1.00      0.00      0.00      1008\n",
      "           м       1.00      0.00      0.00      7965\n",
      "           ч       1.00      0.00      0.00      4692\n",
      "           з       1.00      0.00      0.00      4582\n",
      "           д       1.00      0.00      0.00      8369\n",
      "           я       1.00      0.00      0.00      5934\n",
      "           ж       1.00      0.00      0.00      3256\n",
      "           й       1.00      0.00      0.00      2898\n",
      "           х       1.00      0.00      0.00      2160\n",
      "           а       0.21      0.44      0.28     23265\n",
      "           ц       1.00      0.00      0.00       714\n",
      "           ъ       1.00      0.00      0.00        82\n",
      "           р       1.00      0.00      0.00     11100\n",
      "           е       0.27      0.17      0.21     24567\n",
      "           н       1.00      0.00      0.00     19345\n",
      "           г       1.00      0.00      0.00      5126\n",
      "           ю       1.00      0.00      0.00      1761\n",
      "           ь       1.00      0.00      0.00      5731\n",
      "\n",
      "    accuracy                           0.27    341360\n",
      "   macro avg       0.85      0.11      0.09    341360\n",
      "weighted avg       0.63      0.27      0.19    341360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(text_gen_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[text_dataset.vocab.alphabet[i] for i in y_test.unique().sort()[0]],\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
    "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова)\n",
    "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
    "  * После этого вы генерируете строку нужной длины аналогично 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "        model: TextRNNGenerator,\n",
    "        dataset: TextDataset,\n",
    "        prompt: str,  # стартовая строка\n",
    "        size: int,  # любая длина генерируемого текста\n",
    "        prob: t.Callable[[torch.Tensor], torch.Tensor] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> str:\n",
    "    text = [dataset.vocab.ch2i[ch] for ch in prompt]\n",
    "\n",
    "    model.eval()\n",
    "    model.reset_rnn_state()\n",
    "    for i in range(size - len(text)):\n",
    "        x = torch.tensor([text[i:]], device=device)\n",
    "        pred = model(x).squeeze()[:, -1]\n",
    "        if prob:\n",
    "            next_ch_idx = torch.multinomial(prob(pred), 1)\n",
    "        else:\n",
    "            next_ch_idx = pred.argmax()\n",
    "        text.append(next_ch_idx.item())\n",
    "\n",
    "    return dataset.decode(torch.tensor(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "доброе утро жда ем еналаи кицу блсетаю после стьскромол ела одвснееенидевыесевоже од икс т\n",
      "добрый вечер гомаковшиск ка з предего инубествери мыт тупепретовос те см нытлоскокнай азас\n",
      "на каждом постоялом дворе спровиполю вонобло подрину фре деля в атото еме ка дь стви ду вы\n",
      "дело шло о том б нс канн по ей е тару чкрия пув монтри чевровой не ито пренака эт вожата м\n",
      "как только ро ола заза выкста поерыепри бкодру сти ий нытодошьзн пре о вытулю лен усделялю\n",
      "белая береза гол жче яшь онелодрго стамо мо пу иниль ко столю чирыло овылиедл к ситотенолу\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\n",
    "    \"доброе утро \",\n",
    "    \"добрый вечер \",\n",
    "    \"на каждом постоялом дворе \",\n",
    "    \"дело шло о том \",\n",
    "    \"как только \",\n",
    "    \"белая береза \",\n",
    "]:\n",
    "    print(generate_text(text_gen_net, text_dataset, prompt, 90, prob=softmax_prob, device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А это \"Убийца ChatGPT 2.0\"\n",
    "\n",
    "> As we can see, the generated text may not make any sense, however there are some words and phrases that seem to form an idea, for example...\n",
    "\n",
    "[Источник](https://towardsdatascience.com/text-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c#:~:text=As%20we%20can%20see%2C%20the%20generated%20text%20may%20not%20make%20any%20sense%2C%20however%20there%20are%20some%20words%20and%20phrases%20that%20seem%20to%20form%20an%20idea%2C%20for%20example%3A)\n",
    "\n",
    "Из хорошего, модель научилась вовремя расставлять пробелы - сочетания букв по длинам похожи на слова (самостоятельные и предлоги - если сильно фантазировать 🥳)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOt/b54+xoKtnmvuSlliKDY",
   "collapsed_sections": [],
   "name": "blank__08_rnn_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
