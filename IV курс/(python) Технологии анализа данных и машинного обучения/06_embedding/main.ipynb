{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Классификация текстов при помощи сверточных сетей\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        lr_scheduler=None,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "        if test_dataloader:\n",
    "            loss, acc = test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx75RigN8xIJ"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных в виде последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScKIAey9dAM"
   },
   "source": [
    "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "phEw721T9SYW"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[14, 2, 12, 8, 21, 4, 5, 23]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "\n",
    "alphabet = list(set(nltk.word_tokenize(text.replace(\".\", \"\"))))\n",
    "word2index = {w: i for i, w in enumerate(alphabet)}\n",
    "\n",
    "first_sentence = nltk.sent_tokenize(text)[0].replace(\".\", \"\")\n",
    "[word2index[w] for w in nltk.word_tokenize(first_sentence)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFQCPtD9x5J"
   },
   "source": [
    "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RZS4XLV0-buf"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 1., 0.]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "\n",
    "alphabet = list(set(nltk.word_tokenize(text.replace(\".\", \"\"))))\n",
    "word2index = {w: i for i, w in enumerate(alphabet)}\n",
    "\n",
    "first_sentence = nltk.sent_tokenize(text)[0].replace(\".\", \"\")\n",
    "words = nltk.word_tokenize(first_sentence)\n",
    "\n",
    "vectors = torch.zeros(len(words), len(alphabet))\n",
    "indices = [(i, word2index[w]) for i, w in enumerate(words)]\n",
    "vectors[list(zip(*indices))] = 1\n",
    "vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQKHYA-mJN"
   },
   "source": [
    "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.9595, -1.1038,  0.5411,  1.5390,  1.0860,  1.2464,  0.1151,  1.6193,\n          0.4637,  1.3007,  0.8732,  0.0651,  0.7732, -0.9701, -0.8877, -0.3183,\n         -0.3344,  0.4543,  0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334,\n         -0.0347],\n        [ 1.0554,  0.1778, -0.2303, -0.3918,  0.5433, -0.3952, -0.4462,  0.7440,\n          1.5210,  3.4105, -1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200,\n          1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356, -1.6293, -0.5497,\n         -0.4798],\n        [ 1.4628, -0.6204,  0.9884, -0.4322, -0.6232, -0.2162, -0.4887,  0.7870,\n          0.1076, -1.0715, -0.1166, -1.0170, -1.1980,  0.4784, -1.2295, -1.3700,\n          1.5435, -0.0332, -0.4186, -0.2556, -0.1292, -0.0546,  0.4083,  1.1264,\n          1.9351],\n        [-0.0744, -1.0922,  0.3920,  0.5945,  0.6623, -1.2063,  0.6074, -0.5472,\n          1.1711,  0.0975,  0.9634,  0.8403, -1.2537,  0.9868, -0.4947, -1.2830,\n          0.9552,  1.2836, -0.6659,  0.5651,  0.2877, -0.0334, -1.0619, -0.1144,\n         -0.3433],\n        [ 1.2532, -0.4445,  0.8185, -0.8180,  0.3603, -1.6146, -2.4734,  0.0362,\n         -0.3422, -0.3817, -0.0569,  0.8436,  0.6829,  3.3944, -1.6688,  0.5109,\n         -0.2860,  0.3351,  1.1719,  1.2955,  0.8909, -0.4898, -1.1727, -0.6870,\n         -2.3349],\n        [-2.6133, -1.6965, -0.2282,  0.2800,  0.2469,  0.0769,  0.3380,  0.4544,\n          0.4569, -0.8654,  0.7813, -0.9268, -0.2188, -2.4351, -0.0729, -0.0340,\n          0.9625,  0.3492, -0.9215, -0.0562, -0.6227, -0.4637,  1.9218, -0.4025,\n          0.1239],\n        [ 1.1648,  0.9234,  1.3873, -0.8834, -0.4189, -0.8048,  0.5656,  0.6104,\n          0.4669,  1.9507, -1.0631, -0.0773,  0.1164, -0.5940, -1.2439, -0.1021,\n         -1.0335, -0.3126,  0.2458, -0.2596,  0.1183,  0.2440,  1.1646,  0.2886,\n          0.3866],\n        [-0.5297, -0.8733,  0.0043, -1.2579, -1.0845,  0.7530,  0.3236, -0.2750,\n          1.3056,  0.2118,  0.2720, -0.9268, -2.7330, -0.5642, -0.2740,  0.1398,\n          0.5086,  0.2771, -0.9812,  0.8888,  1.5690, -0.0819, -0.3494,  0.2024,\n         -0.2884]], grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "# кажется, что просят невозможного. Попадание в размерность - уже успех\n",
    "embeds = nn.Embedding(num_embeddings=len(alphabet), embedding_dim=len(alphabet))\n",
    "indices = torch.tensor([word2index[w] for w in nltk.word_tokenize(first_sentence)])\n",
    "embeds(indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjM7qEUNFY_"
   },
   "source": [
    "## 2. Классификация фамилий по национальности (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1 Считать файл `surnames/surnames.csv`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French\n2      Kore     English\n3     Koury      Arabic\n4    Lebzak     Russian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_df = pd.read_csv(DATA_DIR / \"surnames.csv\")\n",
    "surnames_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2 Закодировать национальности числами, начиная с 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "    surname nationality  target\n0  Woodford     English       4\n1      Coté      French       5\n2      Kore     English       4\n3     Koury      Arabic       0\n4    Lebzak     Russian      14",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_labeler = LabelEncoder()\n",
    "surnames_df[\"target\"] = surnames_labeler.fit_transform(surnames_df[\"nationality\"])\n",
    "print(f\"classes: {len(surnames_labeler.classes_)}\")\n",
    "surnames_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
    "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    pad = \"<PAD>\"\n",
    "\n",
    "    def __init__(self, series: pd.Series):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for w in map(str.lower, series):\n",
    "            uniques.update(w)\n",
    "            max_len = max(len(w), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, *uniques]\n",
    "        self.max_len = max_len\n",
    "        self.ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in word]\n",
    "        # дополняем до одинакового размера индексом служебного символа\n",
    "        indices += [self.ch2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.pad], as_tuple=True)[0]  # noqa\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]  # отрезаем служебные символы\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2, 50, 12,  8, 41,  7, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]) kovalev\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(surnames_df[\"surname\"])\n",
    "indices = vocab.encode(\"kovalev\")\n",
    "print(indices, vocab.decode(indices))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса>\n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, vocab: Vocab, transform: t.Callable = None):\n",
    "        self.surnames = df[\"surname\"].tolist()\n",
    "\n",
    "        if transform:\n",
    "            # 1 раз transform - прохождение эпох быстрее\n",
    "            size = transform(self.surnames[0]).size()\n",
    "            self.data = torch.vstack([transform(w) for w in self.surnames]).view(len(self.surnames), *size)\n",
    "        else:\n",
    "            self.data = self.surnames\n",
    "        self.targets = torch.tensor(df[\"target\"], dtype=torch.long)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "((tensor([11, 50, 50, 36, 51, 50, 34, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n  tensor(4)),\n (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.]]),\n  tensor(4)))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_indices(word: str) -> torch.Tensor:\n",
    "    return vocab.encode(word.lower())\n",
    "\n",
    "\n",
    "def one_hot(word: str) -> torch.Tensor:\n",
    "    vectors = torch.zeros(vocab.max_len, len(vocab.alphabet))\n",
    "    indices = [(i, vocab.ch2i[ch]) for i, ch in enumerate(word.lower())]\n",
    "    vectors[list(zip(*indices))] = 1\n",
    "    return vectors\n",
    "\n",
    "\n",
    "# Зачем 2 датасета?\n",
    "# - нужно для задания 2.6\n",
    "# - удобнее работать с интерфейсом Dataset'а (разбиение на выборки, DataLoader)\n",
    "# - эффективнее - одно преобразование вместо преобразование/эпоха\n",
    "\n",
    "# датасет для обучения через Embedding\n",
    "surnames_indices_dataset = SurnamesDataset(surnames_df, vocab, transform=to_indices)\n",
    "# датасет для обучения без Embedding (слова закодированы one-hot методом)\n",
    "surnames_one_hot_dataset = SurnamesDataset(surnames_df, vocab, transform=one_hot)\n",
    "surnames_indices_dataset[0], surnames_one_hot_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3 Разбить датасет на обучающую и тестовую выборку"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8784 2196\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_indices_dataset, test_indices_dataset = train_test_split(surnames_indices_dataset, train_part=0.8)\n",
    "train_one_hot_dataset, test_one_hot_dataset = train_test_split(surnames_one_hot_dataset, train_part=0.8)\n",
    "print(len(train_indices_dataset), len(test_indices_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.6. Обучить классификатор.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта:\n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab: Vocab,\n",
    "            out_features: int,\n",
    "            embedding_dim: int = 128,\n",
    "            use_embedding: bool = True,\n",
    "            debug: bool = False,\n",
    "    ):\n",
    "        super(SurnamesClassifier, self).__init__()\n",
    "        self.use_embedding = use_embedding\n",
    "        self.debug = debug\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        last_conv_out_channels = 64\n",
    "        adaptive_avg_pool = 8\n",
    "\n",
    "        # Как же этой модели все это... безразлично\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(vocab.alphabet), embedding_dim=embedding_dim)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=last_conv_out_channels, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=last_conv_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        # Единственный полезный (и понятный зачем) слой. Зачем? - Позволяет не думать о размерностях\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(adaptive_avg_pool)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(last_conv_out_channels * adaptive_avg_pool, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, out_features),\n",
    "        )\n",
    "\n",
    "        if self.debug:\n",
    "            # то же самое, но умеет в вывод размерностей\n",
    "            self.forward = self._debug_forward\n",
    "        else:\n",
    "            self.forward = self._forward\n",
    "\n",
    "    def _forward(self, x: torch.Tensor):\n",
    "        if self.use_embedding:\n",
    "            x = self.embedding(x)\n",
    "        else:\n",
    "            # Для эксперимента с one-hot - что будет, если растянуть вектора до размера embedding_dim?\n",
    "            # Ответ: ничего\n",
    "            x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "\n",
    "        # (batch_size, num_features [embedding_dim], n_tokens)\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        # Интересно, почему в лекциях иногда используют CrossEntropyLoss, а иногда SoftMax + NLLLoss?\n",
    "        # Они же эквивалентны: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "    def _debug_forward(self, x: torch.Tensor):\n",
    "        print(\"x: \", x.size())\n",
    "        if self.use_embedding:\n",
    "            x = self.embedding(x)\n",
    "            print(\"embedding: \", x.size())\n",
    "        else:\n",
    "            x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "            print(\"pad: \", x.size())\n",
    "\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        print(\"reshape: \", x.size())\n",
    "        x = self.features(x)\n",
    "        print(\"features: \", x.size())\n",
    "        x = self.avgpool(x)\n",
    "        print(\"avgpool: \", x.size())\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(\"flatten: \", x.size())\n",
    "        x = self.classifier(x)\n",
    "        print(\"classifier: \", x.size())\n",
    "        return torch.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "common_net = SurnamesClassifier(vocab, len(surnames_labeler.classes_)).to(DEVICE)\n",
    "# Интересно, почему в лекциях иногда используют CrossEntropyLoss, а иногда SoftMax + NLLLoss?\n",
    "# Они же эквивалентны: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(common_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.891928  [    0/ 8784]\n",
      "loss: 1.551697  [ 4000/ 8784]\n",
      "loss: 1.452432  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.584699, Avg loss: 1.495229 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.791197  [    0/ 8784]\n",
      "loss: 1.371429  [ 4000/ 8784]\n",
      "loss: 0.867927  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.588342, Avg loss: 1.448119 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.714791  [    0/ 8784]\n",
      "loss: 1.631755  [ 4000/ 8784]\n",
      "loss: 1.456163  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.636157, Avg loss: 1.296021 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.026548  [    0/ 8784]\n",
      "loss: 0.521745  [ 4000/ 8784]\n",
      "loss: 0.734489  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.637067, Avg loss: 1.276319 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.066671  [    0/ 8784]\n",
      "loss: 1.343641  [ 4000/ 8784]\n",
      "loss: 1.031964  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.632969, Avg loss: 1.280678 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.982903  [    0/ 8784]\n",
      "loss: 1.387582  [ 4000/ 8784]\n",
      "loss: 1.429383  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.649818, Avg loss: 1.254469 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.573719  [    0/ 8784]\n",
      "loss: 0.934324  [ 4000/ 8784]\n",
      "loss: 2.050699  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.642532, Avg loss: 1.271365 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.773151  [    0/ 8784]\n",
      "loss: 1.462401  [ 4000/ 8784]\n",
      "loss: 0.469032  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.651639, Avg loss: 1.244371 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.523181  [    0/ 8784]\n",
      "loss: 0.541402  [ 4000/ 8784]\n",
      "loss: 0.519697  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.646630, Avg loss: 1.308373 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 1.289540  [    0/ 8784]\n",
      "loss: 0.556838  [ 4000/ 8784]\n",
      "loss: 1.088891  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.660747, Avg loss: 1.239585 \n",
      "\n",
      "CPU times: total: 1min 1s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# One-Hot представление\n",
    "common_net.use_embedding = False\n",
    "_ = common_train(\n",
    "    epochs=10,\n",
    "    model=common_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=DataLoader(train_one_hot_dataset, batch_size=8, shuffle=True),\n",
    "    test_dataloader=DataLoader(test_one_hot_dataset, batch_size=512),\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "embeddings_net = SurnamesClassifier(vocab, len(surnames_labeler.classes_)).to(DEVICE)\n",
    "# Интересно, почему в лекциях иногда используют CrossEntropyLoss, а иногда SoftMax + NLLLoss?\n",
    "# Они же эквивалентны: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(embeddings_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.781295  [    0/ 8784]\n",
      "loss: 1.637953  [ 4000/ 8784]\n",
      "loss: 1.239725  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.573315, Avg loss: 1.554180 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.596291  [    0/ 8784]\n",
      "loss: 0.245179  [ 4000/ 8784]\n",
      "loss: 0.928826  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.621585, Avg loss: 1.326015 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.813803  [    0/ 8784]\n",
      "loss: 0.742921  [ 4000/ 8784]\n",
      "loss: 0.756268  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.665756, Avg loss: 1.183925 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.923795  [    0/ 8784]\n",
      "loss: 0.833571  [ 4000/ 8784]\n",
      "loss: 1.849127  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.664390, Avg loss: 1.165710 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.151880  [    0/ 8784]\n",
      "loss: 1.739708  [ 4000/ 8784]\n",
      "loss: 0.405361  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.693534, Avg loss: 1.075316 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.577488  [    0/ 8784]\n",
      "loss: 1.423235  [ 4000/ 8784]\n",
      "loss: 0.378620  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.687158, Avg loss: 1.112288 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.452275  [    0/ 8784]\n",
      "loss: 0.902769  [ 4000/ 8784]\n",
      "loss: 0.384811  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.699454, Avg loss: 1.088695 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 1.350967  [    0/ 8784]\n",
      "loss: 1.210560  [ 4000/ 8784]\n",
      "loss: 0.590641  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.696266, Avg loss: 1.161110 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.370603  [    0/ 8784]\n",
      "loss: 0.568192  [ 4000/ 8784]\n",
      "loss: 0.385137  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.695355, Avg loss: 1.061058 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.511005  [    0/ 8784]\n",
      "loss: 0.958134  [ 4000/ 8784]\n",
      "loss: 0.548951  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.704463, Avg loss: 1.075817 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.737833  [    0/ 8784]\n",
      "loss: 1.828275  [ 4000/ 8784]\n",
      "loss: 0.824349  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.723133, Avg loss: 1.154756 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.824099  [    0/ 8784]\n",
      "loss: 0.598246  [ 4000/ 8784]\n",
      "loss: 0.751876  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.697177, Avg loss: 1.146314 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.266274  [    0/ 8784]\n",
      "loss: 0.570966  [ 4000/ 8784]\n",
      "loss: 0.340732  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.709016, Avg loss: 1.299051 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.019477  [    0/ 8784]\n",
      "loss: 0.356437  [ 4000/ 8784]\n",
      "loss: 0.250511  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.704007, Avg loss: 1.465986 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 1.391345  [    0/ 8784]\n",
      "loss: 0.248716  [ 4000/ 8784]\n",
      "loss: 0.082539  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.708106, Avg loss: 1.282580 \n",
      "\n",
      "CPU times: total: 1min 13s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Embedding представление\n",
    "embeddings_net.use_embedding = True\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=embeddings_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=DataLoader(train_indices_dataset, batch_size=8, shuffle=True),\n",
    "    test_dataloader=DataLoader(test_indices_dataset, batch_size=512),\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.708106, Avg loss: 1.282580 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(\n",
    "    dataloader=DataLoader(test_indices_dataset, batch_size=512),\n",
    "    model=embeddings_net,\n",
    "    loss_fn=loss_fn,\n",
    "    device=DEVICE,\n",
    ");"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def inference(\n",
    "        surname: str,\n",
    "        target: str,\n",
    "        model: nn.Module,\n",
    "        vocab: Vocab,\n",
    "        labeler: LabelEncoder,\n",
    "        k: int = 3,\n",
    "        device: str = \"cpu\",\n",
    "):\n",
    "    x = vocab.encode(surname.lower())\n",
    "    x = x.to(device)\n",
    "\n",
    "    pred = model(x.unsqueeze(0))\n",
    "    pred_proba, pred_label_indices = F.softmax(pred, 1).topk(k, dim=1)\n",
    "    pred_labels = labeler.inverse_transform(pred_label_indices.squeeze().cpu())\n",
    "\n",
    "    predicts = \", \".join(\n",
    "        [f\"{label} ({prob:.2f})\" for (label, prob) in zip(pred_labels, pred_proba.squeeze())]\n",
    "    )\n",
    "    print(f\"Surname : {surname}\")\n",
    "    print(f\"True    : {target}\")\n",
    "    print(f\"Predicts: {predicts}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GHjCRqQg1sw5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname : Alexandrova\n",
      "True    : Russian\n",
      "Predicts: Italian (0.88), Russian (0.12), Spanish (0.00)\n",
      "\n",
      "Surname : Baranov\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), English (0.00), Czech (0.00)\n",
      "\n",
      "Surname : Brusova\n",
      "True    : Russian\n",
      "Predicts: Italian (0.71), Czech (0.17), Japanese (0.04)\n",
      "\n",
      "Surname : Volkova\n",
      "True    : Russian\n",
      "Predicts: Russian (0.99), Czech (0.01), Polish (0.00)\n",
      "\n",
      "Surname : Kovalev\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Czech (0.00), Polish (0.00)\n",
      "\n",
      "Surname : Kostyuchenko\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), English (0.00), Czech (0.00)\n",
      "\n",
      "Surname : Kuzin\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Japanese (0.00), Czech (0.00)\n",
      "\n",
      "Surname : Likhachev\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Czech (0.00), Polish (0.00)\n",
      "\n",
      "Surname : Telitsyn\n",
      "True    : Russian\n",
      "Predicts: English (0.98), Russian (0.01), Czech (0.01)\n",
      "\n",
      "Surname : Ustimova\n",
      "True    : Russian\n",
      "Predicts: Czech (0.47), Russian (0.22), Spanish (0.11)\n",
      "\n",
      "Surname : Khamikoeva\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Czech (0.00), English (0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students = [\n",
    "    \"Alexandrova\",\n",
    "    \"Baranov\",\n",
    "    \"Brusova\",\n",
    "    \"Volkova\",\n",
    "    \"Kovalev\",\n",
    "    \"Kostyuchenko\",\n",
    "    \"Kuzin\",\n",
    "    \"Likhachev\",\n",
    "    \"Telitsyn\",\n",
    "    \"Ustimova\",\n",
    "    \"Khamikoeva\",\n",
    "]\n",
    "for surname in students:\n",
    "    inference(\n",
    "        surname=surname,\n",
    "        target=\"Russian\",\n",
    "        model=embeddings_net,\n",
    "        vocab=vocab,\n",
    "        labeler=surnames_labeler,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Вывод:** использование Embedding позволило увеличить точность модели."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo-hf5CQ0iWv"
   },
   "source": [
    "## 3. Классификация обзоров на фильмы (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# NLTK теги частей речи отличаются от WORDNET тегов\n",
    "def get_pos(word: str) -> str:\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_review(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    # удаляем все символы кроме букв латинского алфавита\n",
    "    text = re.sub(r\"[^a-z]\", repl=\" \", string=text, flags=re.MULTILINE)\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    words = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word not in STOPWORDS:  # удаляем стоп-слова до лемматизации - так можно чуть-чуть сэкономить\n",
    "            lemma = lemmatizer.lemmatize(word, pos=get_pos(word))\n",
    "            # удаляем стоп-слова, наивное предположение - не брать леммы короче 3-х символов дало значительный прирост точности\n",
    "            if lemma not in STOPWORDS and len(lemma) > 2:\n",
    "                words.append(lemma)\n",
    "\n",
    "    return \" \".join(words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, positive_path: Path, negative_path: Path, seed: int = None):\n",
    "        self.positive_path = positive_path\n",
    "        self.negative_path = negative_path\n",
    "        self.positive_reviews = self.read_reviews(positive_path, preprocess_review)\n",
    "        self.negative_reviews = self.read_reviews(negative_path, preprocess_review)\n",
    "\n",
    "        data = self.positive_reviews + self.negative_reviews\n",
    "        targets = torch.cat([torch.ones(len(self.positive_reviews)), torch.zeros(len(self.negative_reviews))])\n",
    "\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        indices = torch.randperm(len(data))\n",
    "\n",
    "        self.data = [data[i] for i in indices]\n",
    "        self.targets = targets[indices].to(torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_reviews(path: Path, process: t.Callable[[str], str]) -> list[str]:\n",
    "        reviews = []\n",
    "        with open(path) as f:\n",
    "            for review in f.readlines():\n",
    "                review = process(review)\n",
    "                if review:\n",
    "                    reviews.append(review)\n",
    "        return reviews\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(10660, ('play less like come age romance infomercial', tensor(0)))"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# кажется, файлы с отзывами были перепутаны:\n",
    "# хорошие отзывы находились в файле negative_reviews.txt - переименовал.\n",
    "reviews_path = DATA_DIR / \"polarity\"\n",
    "reviews_dataset = ReviewsDataset(\n",
    "    reviews_path / \"positive_reviews.txt\",\n",
    "    reviews_path / \"negative_reviews.txt\",\n",
    "    seed=0,\n",
    ")\n",
    "len(reviews_dataset), reviews_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(8528, 2132)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_reviews, test_reviews = train_test_split(reviews_dataset, train_part=0.8)\n",
    "len(train_reviews), len(test_reviews)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class ReviewsVocab:\n",
    "    pad = \"<PAD>\"\n",
    "    unknown = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, reviews: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for review in reviews:\n",
    "            words = nltk.word_tokenize(review)\n",
    "            uniques.update(words)\n",
    "            max_len = max(len(words), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, self.unknown, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        w2i = {w: i for i, w in enumerate(self.alphabet)}\n",
    "        # если ключ отсутствует, будет возвращена 1 - индекс служебного символа\n",
    "        self.w2i = defaultdict(lambda: 1, w2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    @lru_cache(maxsize=8192)  # сомнительная эффективность? Ну да\n",
    "    def encode(self, review: str) -> torch.Tensor:\n",
    "        indices = [self.w2i[w] for w in nltk.word_tokenize(review)]\n",
    "        indices += [self.w2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.w2i[self.pad], as_tuple=True)[0]  # noqa\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \" \".join(self.alphabet[i] for i in indices)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet: 13216 longest: 29\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([   1,    1,    1, 3619, 8572,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0]),\n '<UNK> <UNK> <UNK> neutral review')"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ReviewsVocab([review for review, _ in train_reviews])\n",
    "print(f\"alphabet: {len(vocab)}\", f\"longest: {vocab.max_len}\")\n",
    "encoded = vocab.encode(\"this is a neutral review\")\n",
    "encoded, vocab.decode(encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2. Обучите классификатор.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`\n",
    "    - подберите адекватную размерность вектора эмбеддинга:\n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class ReviewsClassifier(nn.Module):\n",
    "    LAST_CONV_OUT_CHANNELS = 64\n",
    "    ADAPTIVE_AVG_POOL = 8\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super(ReviewsClassifier, self).__init__()\n",
    "\n",
    "        # Как же этой модели все это... безразлично\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=self.LAST_CONV_OUT_CHANNELS, kernel_size=2),\n",
    "            nn.BatchNorm1d(num_features=self.LAST_CONV_OUT_CHANNELS),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        # Единственный полезный (и понятный зачем) слой. Зачем? - Позволяет не думать о размерностях\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(self.ADAPTIVE_AVG_POOL)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.LAST_CONV_OUT_CHANNELS * self.ADAPTIVE_AVG_POOL, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def collate(batch: t.List[t.Tuple[str, torch.Tensor]]) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for x, y in batch:\n",
    "        xs.append(vocab.encode(x))\n",
    "        ys.append(y)\n",
    "    return torch.vstack(xs), torch.hstack(ys)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "net = ReviewsClassifier(num_embeddings=len(vocab), embedding_dim=128).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.000914092001)  # а почему нет?\n",
    "# будет изменять lr = lr * factor, если на протяжении patience эпох ошибка не менялась более чем на threshold\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode=\"min\",\n",
    "    patience=5,\n",
    "    factor=0.333333,\n",
    "    min_lr=0.000001,\n",
    "    threshold=0.001,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Однако batch_size - очень важно. При 8 и 64 застреваем в плохом оптимуме, 32 тоже не очень...\n",
    "# 22 > Embedding, Conv1d, BatchNorm1d, ReLU, MaxPool1d, AdaptiveAvgPool1d, Dropout, lr_scheduler, optimizer\n",
    "train_dataloader = DataLoader(train_reviews, batch_size=22, collate_fn=collate, shuffle=True)\n",
    "test_dataloader = DataLoader(test_reviews, batch_size=512, collate_fn=collate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 0.774067  [    0/ 8528]\n",
      "loss: 0.710574  [ 3300/ 8528]\n",
      "loss: 0.683714  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.507974, Avg loss: 0.693086 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 0.693190  [    0/ 8528]\n",
      "loss: 0.690218  [ 3300/ 8528]\n",
      "loss: 0.691362  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.497655, Avg loss: 0.693248 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.697149  [    0/ 8528]\n",
      "loss: 0.695406  [ 3300/ 8528]\n",
      "loss: 0.695937  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.525797, Avg loss: 0.692885 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.693817  [    0/ 8528]\n",
      "loss: 0.693174  [ 3300/ 8528]\n",
      "loss: 0.691953  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.526735, Avg loss: 0.693175 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.690541  [    0/ 8528]\n",
      "loss: 0.693057  [ 3300/ 8528]\n",
      "loss: 0.692580  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.502814, Avg loss: 0.693590 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.681000  [    0/ 8528]\n",
      "loss: 0.693076  [ 3300/ 8528]\n",
      "loss: 0.724950  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.500938, Avg loss: 0.693095 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.693771  [    0/ 8528]\n",
      "loss: 0.688238  [ 3300/ 8528]\n",
      "loss: 0.674451  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.570826, Avg loss: 0.681065 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.616464  [    0/ 8528]\n",
      "loss: 0.780712  [ 3300/ 8528]\n",
      "loss: 0.656322  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.669794, Avg loss: 0.619530 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.566931  [    0/ 8528]\n",
      "loss: 0.662415  [ 3300/ 8528]\n",
      "loss: 0.781444  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.691370, Avg loss: 0.627701 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.423342  [    0/ 8528]\n",
      "loss: 0.292211  [ 3300/ 8528]\n",
      "loss: 0.211681  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.713884, Avg loss: 0.645488 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.765670  [    0/ 8528]\n",
      "loss: 0.092790  [ 3300/ 8528]\n",
      "loss: 0.374990  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.719981, Avg loss: 0.755992 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.177059  [    0/ 8528]\n",
      "loss: 0.218504  [ 3300/ 8528]\n",
      "loss: 0.108423  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.737336, Avg loss: 0.911041 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.088062  [    0/ 8528]\n",
      "loss: 0.071147  [ 3300/ 8528]\n",
      "loss: 0.318784  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.722795, Avg loss: 1.058568 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.222083  [    0/ 8528]\n",
      "loss: 0.076467  [ 3300/ 8528]\n",
      "loss: 0.035098  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.730769, Avg loss: 1.269336 \n",
      "\n",
      "Epoch 00014: reducing learning rate of group 0 to 3.0470e-04.\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.077892  [    0/ 8528]\n",
      "loss: 0.047767  [ 3300/ 8528]\n",
      "loss: 0.087756  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.733583, Avg loss: 1.390100 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.066956  [    0/ 8528]\n",
      "loss: 0.029947  [ 3300/ 8528]\n",
      "loss: 0.006088  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.735460, Avg loss: 1.524220 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.141970  [    0/ 8528]\n",
      "loss: 0.089074  [ 3300/ 8528]\n",
      "loss: 0.026015  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.733583, Avg loss: 1.612054 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.010059  [    0/ 8528]\n",
      "loss: 0.043093  [ 3300/ 8528]\n",
      "loss: 0.016076  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.739212, Avg loss: 1.769913 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.073957  [    0/ 8528]\n",
      "loss: 0.010112  [ 3300/ 8528]\n",
      "loss: 0.083702  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.731707, Avg loss: 1.734917 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.030364  [    0/ 8528]\n",
      "loss: 0.001393  [ 3300/ 8528]\n",
      "loss: 0.056716  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.736867, Avg loss: 1.893856 \n",
      "\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0157e-04.\n",
      "CPU times: total: 1min 50s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    verbose=150,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "У модели явная проблема с переобучением. После 3-ей эпохи тестовая ошибка начала увеличиваться,\n",
    "в то же время ошибка на обучающей выборки быстро приблизилась к 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.75      0.74      1061\n",
      "    positive       0.74      0.73      0.73      1071\n",
      "\n",
      "    accuracy                           0.74      2132\n",
      "   macro avg       0.74      0.74      0.74      2132\n",
      "weighted avg       0.74      0.74      0.74      2132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[\"negative\", \"positive\"],\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def inference(\n",
    "        review: str,\n",
    "        target: str,\n",
    "        model: nn.Module,\n",
    "        vocab: ReviewsVocab,\n",
    "        target_names: list[str],\n",
    "        device: str = \"cpu\",\n",
    "):\n",
    "    x = vocab.encode(preprocess_review(review))\n",
    "    x = x.to(device)\n",
    "\n",
    "    pred = model(x.unsqueeze(0))\n",
    "    pred_proba, pred_label_idx = F.softmax(pred, 1).max(dim=1)\n",
    "    pred_label = target_names[pred_label_idx.cpu()]\n",
    "\n",
    "    print(f\"Review : {review}\")\n",
    "    print(f\"True   : {target}\")\n",
    "    print(f\"Predict: {pred_label} ({pred_proba.item():.2f})\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review : No intrigue, poor character disclosure.\n",
      "True   : negative\n",
      "Predict: negative (0.97)\n",
      "\n",
      "Review : A fascinating story. The actors played their characters perfectly.\n",
      "True   : positive\n",
      "Predict: positive (1.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    (\"No intrigue, poor character disclosure.\", \"negative\"),\n",
    "    (\"A fascinating story. The actors played their characters perfectly.\", \"positive\"),\n",
    "]\n",
    "for review, target in reviews:\n",
    "    inference(\n",
    "        review=review,\n",
    "        target=target,\n",
    "        model=net,\n",
    "        vocab=vocab,\n",
    "        target_names=[\"negative\", \"positive\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "# вот это уверенность"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Вывод:** единственная обоснованная, логичная и очень простая идея не включать в датасет слова (после обработки) короче 3-х символов дала ощутимый прирост точности модели.\n",
    "\n",
    "_Размер батча - самый важный гипер и негипер параметр_\n",
    "\n",
    "_Выкинул из модели слой свертки из 4-х блоков - она этого не заметила_"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPUQGO96Qffh8FYHODzJzxf",
   "collapsed_sections": [],
   "name": "blank__06_CNN_embeddings_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
