{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Классификация текстов при помощи сверточных сетей\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        verbose: int = 100,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "        if test_dataloader:\n",
    "            test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        transform: t.Callable = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx75RigN8xIJ"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных в виде последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScKIAey9dAM"
   },
   "source": [
    "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "id": "phEw721T9SYW"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 7, 14, 19, 17, 22, 20, 24]"
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "\n",
    "alphabet = list(set(nltk.word_tokenize(text.replace(\".\", \"\"))))\n",
    "word2index = {w: i for i, w in enumerate(alphabet)}\n",
    "\n",
    "first_sentence = nltk.sent_tokenize(text)[0].replace(\".\", \"\")\n",
    "[word2index[w] for w in nltk.word_tokenize(first_sentence)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFQCPtD9x5J"
   },
   "source": [
    "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "id": "RZS4XLV0-buf"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 1.]])"
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "\n",
    "alphabet = list(set(nltk.word_tokenize(text.replace(\".\", \"\"))))\n",
    "word2index = {w: i for i, w in enumerate(alphabet)}\n",
    "\n",
    "first_sentence = nltk.sent_tokenize(text)[0].replace(\".\", \"\")\n",
    "words = nltk.word_tokenize(first_sentence)\n",
    "\n",
    "vectors = torch.zeros(len(words), len(alphabet))\n",
    "indices = [(i, word2index[w]) for i, w in enumerate(words)]\n",
    "vectors[list(zip(*indices))] = 1\n",
    "vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQKHYA-mJN"
   },
   "source": [
    "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0554,  0.1778, -0.2303, -0.3918,  0.5433, -0.3952, -0.4462,  0.7440,\n          1.5210,  3.4105, -1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200,\n          1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356, -1.6293, -0.5497,\n         -0.4798],\n        [-1.3962, -0.0661, -0.3584, -1.5616, -0.3546,  1.0811,  0.1315,  1.5735,\n          0.7814, -1.0787, -0.7209,  1.4708,  0.2756,  0.6668, -0.9944, -1.1894,\n         -1.1959, -0.5596,  0.5335,  0.4069,  0.3946,  0.1715,  0.8760, -0.2871,\n          1.0216],\n        [ 1.9595, -1.1038,  0.5411,  1.5390,  1.0860,  1.2464,  0.1151,  1.6193,\n          0.4637,  1.3007,  0.8732,  0.0651,  0.7732, -0.9701, -0.8877, -0.3183,\n         -0.3344,  0.4543,  0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334,\n         -0.0347],\n        [ 0.0358,  0.2160, -0.9161,  1.5599, -3.1537, -0.5611, -0.4303, -0.3332,\n         -1.5464, -0.0147,  1.2251,  1.5936, -1.6315, -0.0569,  0.6297,  0.2712,\n         -0.6860, -1.0918,  1.6797, -0.8808,  0.5800,  0.3642,  0.0881, -1.3069,\n         -0.7064],\n        [ 0.2715,  0.6716,  1.8500,  1.1910, -0.5899,  0.9647, -1.5094,  2.2557,\n          1.2288, -0.4855,  0.4536,  1.3514,  0.4339, -0.5133, -0.1860,  0.2757,\n          0.1097,  0.3594, -0.7537,  0.2294, -0.2544,  1.5800, -0.2444, -1.1991,\n         -0.0257],\n        [ 0.0940, -0.2021, -0.0595,  2.0118, -0.3368,  0.3260,  0.5352,  1.9733,\n         -0.2075, -0.0306,  0.1267,  0.0055,  0.7943,  0.4072, -0.3609,  1.3103,\n         -0.9651,  0.8806, -0.1025, -0.6770, -0.4107, -1.6186,  0.5079,  2.3230,\n          0.2298],\n        [-0.1642, -0.9715, -1.0308,  0.6473, -0.1906,  0.7167, -2.0002, -2.4097,\n          0.2194, -1.6989,  1.3094, -1.6613, -0.5461, -0.6302, -0.6347,  0.9747,\n          0.2098,  0.0299,  1.7092, -0.7258, -0.7735,  0.5962, -1.2504,  1.1456,\n          0.7393],\n        [ 0.1483,  2.4187,  1.3279, -0.2639,  0.3645,  2.5440, -2.6895,  2.4426,\n          0.0104, -0.5644, -0.9184, -0.7496, -0.0949,  1.1009,  1.3105, -0.2928,\n          0.4181, -0.1695, -2.1749,  0.7202,  0.2854,  0.2290,  1.2833, -1.3792,\n          1.4042]], grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# попадание в размерность - уже успех\n",
    "embeds = nn.Embedding(num_embeddings=len(alphabet), embedding_dim=len(alphabet))\n",
    "indices = torch.tensor([word2index[w] for w in nltk.word_tokenize(first_sentence)])\n",
    "embeds(indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjM7qEUNFY_"
   },
   "source": [
    "## 2. Классификация фамилий по национальности (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1 Считать файл `surnames/surnames.csv`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French\n2      Kore     English\n3     Koury      Arabic\n4    Lebzak     Russian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_df = pd.read_csv(DATA_DIR / \"surnames.csv\")\n",
    "surnames_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2 Закодировать национальности числами, начиная с 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "    surname nationality  target\n0  Woodford     English       4\n1      Coté      French       5\n2      Kore     English       4\n3     Koury      Arabic       0\n4    Lebzak     Russian      14",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_labeler = LabelEncoder()\n",
    "surnames_df[\"target\"] = surnames_labeler.fit_transform(surnames_df[\"nationality\"])\n",
    "print(f\"classes: {len(surnames_labeler.classes_)}\")\n",
    "surnames_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
    "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    pad = \"<PAD>\"\n",
    "\n",
    "    def __init__(self, series: pd.Series):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for w in map(str.lower, series):\n",
    "            uniques.update(w)\n",
    "            max_len = max(len(w), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, *uniques]\n",
    "        self.max_len = max_len\n",
    "        self.ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in word]\n",
    "        indices += [self.ch2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.pad], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17, 46, 54,  7, 50, 19, 54,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]) kovalev\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(surnames_df[\"surname\"])\n",
    "indices = vocab.encode(\"kovalev\")\n",
    "print(indices, vocab.decode(indices))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса>\n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, vocab: Vocab, transform: t.Callable = None):\n",
    "        self.surnames = df[\"surname\"].tolist()\n",
    "\n",
    "        if transform:\n",
    "            size = transform(self.surnames[0]).size()\n",
    "            self.data = torch.vstack([transform(w) for w in self.surnames]).view(len(self.surnames), *size)\n",
    "        else:\n",
    "            self.data = self.surnames\n",
    "        self.targets = torch.tensor(df[\"target\"], dtype=torch.long)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "outputs": [
    {
     "data": {
      "text/plain": "((tensor([ 4, 46, 46, 43, 48, 46, 51, 43,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n  tensor(4)),\n (tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.]]),\n  tensor(4)))"
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_indices(word: str) -> torch.Tensor:\n",
    "    return vocab.encode(word.lower())\n",
    "\n",
    "\n",
    "def one_hot(word: str) -> torch.Tensor:\n",
    "    vectors = torch.zeros(vocab.max_len, len(vocab.alphabet))\n",
    "    indices = [(i, vocab.ch2i[ch]) for i, ch in enumerate(word.lower())]\n",
    "    vectors[list(zip(*indices))] = 1\n",
    "    return vectors\n",
    "\n",
    "\n",
    "surnames_indices_dataset = SurnamesDataset(surnames_df, vocab, transform=to_indices)\n",
    "surnames_one_hot_dataset = SurnamesDataset(surnames_df, vocab, transform=one_hot)\n",
    "surnames_indices_dataset[0], surnames_one_hot_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3 Разбить датасет на обучающую и тестовую выборку"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8784 2196\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_indices_dataset, test_indices_dataset = train_test_split(surnames_indices_dataset, train_part=0.8)\n",
    "train_one_hot_dataset, test_one_hot_dataset = train_test_split(surnames_one_hot_dataset, train_part=0.8)\n",
    "print(len(train_indices_dataset), len(test_indices_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.6. Обучить классификатор.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта:\n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab: Vocab, out_features: int,\n",
    "            embedding_dim: int = 128,\n",
    "            use_embedding: bool = True,\n",
    "            debug: bool = False,\n",
    "    ):\n",
    "        super(SurnamesClassifier, self).__init__()\n",
    "        self.use_embedding = use_embedding\n",
    "        self.debug = debug\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        last_conv_out_channels = 64\n",
    "        adaptive_avg_pool = 8\n",
    "\n",
    "        # как же этой модели все это... безразлично\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(vocab.alphabet), embedding_dim=embedding_dim)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=last_conv_out_channels, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=last_conv_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(adaptive_avg_pool)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(last_conv_out_channels * adaptive_avg_pool, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, out_features),\n",
    "        )\n",
    "\n",
    "        if self.debug:\n",
    "            self.forward = self._debug_forward\n",
    "        else:\n",
    "            self.forward = self._forward\n",
    "\n",
    "    def _forward(self, x: torch.Tensor):\n",
    "        if self.use_embedding:\n",
    "            x = self.embedding(x)\n",
    "        else:\n",
    "            x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "    def _debug_forward(self, x: torch.Tensor):\n",
    "        print(\"x: \", x.size())\n",
    "        if self.use_embedding:\n",
    "            x = self.embedding(x)\n",
    "            print(\"embedding: \", x.size())\n",
    "        else:\n",
    "            x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "            print(\"pad: \", x.size())\n",
    "\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        print(\"reshape: \", x.size())\n",
    "        x = self.features(x)\n",
    "        print(\"features: \", x.size())\n",
    "        x = self.avgpool(x)\n",
    "        print(\"avgpool: \", x.size())\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(\"flatten: \", x.size())\n",
    "        x = self.classifier(x)\n",
    "        print(\"classifier: \", x.size())\n",
    "        return torch.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "common_net = SurnamesClassifier(vocab, len(surnames_labeler.classes_)).to(DEVICE)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(common_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.059291  [    0/ 8784]\n",
      "loss: 2.116892  [ 4000/ 8784]\n",
      "loss: 1.438677  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.556011, Avg loss: 1.581033 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.255365  [    0/ 8784]\n",
      "loss: 1.509195  [ 4000/ 8784]\n",
      "loss: 1.438071  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.588798, Avg loss: 1.398945 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.540711  [    0/ 8784]\n",
      "loss: 1.001740  [ 4000/ 8784]\n",
      "loss: 2.063588  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.602004, Avg loss: 1.368793 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.989875  [    0/ 8784]\n",
      "loss: 1.222487  [ 4000/ 8784]\n",
      "loss: 0.826472  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.615209, Avg loss: 1.331192 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.626660  [    0/ 8784]\n",
      "loss: 0.596323  [ 4000/ 8784]\n",
      "loss: 1.152216  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.628415, Avg loss: 1.319504 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.927089  [    0/ 8784]\n",
      "loss: 1.089187  [ 4000/ 8784]\n",
      "loss: 0.538036  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.632058, Avg loss: 1.312236 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.104079  [    0/ 8784]\n",
      "loss: 1.214090  [ 4000/ 8784]\n",
      "loss: 0.339135  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.633424, Avg loss: 1.277779 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.349008  [    0/ 8784]\n",
      "loss: 1.519058  [ 4000/ 8784]\n",
      "loss: 2.072258  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.639344, Avg loss: 1.279298 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.355651  [    0/ 8784]\n",
      "loss: 1.232887  [ 4000/ 8784]\n",
      "loss: 0.991665  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.632514, Avg loss: 1.311041 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.394635  [    0/ 8784]\n",
      "loss: 1.491568  [ 4000/ 8784]\n",
      "loss: 1.313603  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.635246, Avg loss: 1.320240 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_net.use_embedding = False\n",
    "common_train(\n",
    "    epochs=10,\n",
    "    model=common_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=DataLoader(train_one_hot_dataset, batch_size=8, shuffle=True),\n",
    "    test_dataloader=DataLoader(test_one_hot_dataset, batch_size=512),\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ");"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "embeddings_net = SurnamesClassifier(vocab, len(surnames_labeler.classes_)).to(DEVICE)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(embeddings_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.848648  [    0/ 8784]\n",
      "loss: 1.548867  [ 4000/ 8784]\n",
      "loss: 1.213963  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.563297, Avg loss: 1.592500 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.657608  [    0/ 8784]\n",
      "loss: 1.978383  [ 4000/ 8784]\n",
      "loss: 1.008881  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.610200, Avg loss: 1.386550 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 2.417265  [    0/ 8784]\n",
      "loss: 1.959956  [ 4000/ 8784]\n",
      "loss: 0.906961  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.643898, Avg loss: 1.269374 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.755406  [    0/ 8784]\n",
      "loss: 0.984007  [ 4000/ 8784]\n",
      "loss: 1.120149  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.666667, Avg loss: 1.172726 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.749230  [    0/ 8784]\n",
      "loss: 1.461187  [ 4000/ 8784]\n",
      "loss: 0.636841  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.695355, Avg loss: 1.096197 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.772892  [    0/ 8784]\n",
      "loss: 1.274755  [ 4000/ 8784]\n",
      "loss: 0.619464  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.695355, Avg loss: 1.115461 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.682013  [    0/ 8784]\n",
      "loss: 0.787376  [ 4000/ 8784]\n",
      "loss: 0.223473  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.699909, Avg loss: 1.076720 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.355089  [    0/ 8784]\n",
      "loss: 1.900984  [ 4000/ 8784]\n",
      "loss: 0.306672  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.698087, Avg loss: 1.120857 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.750164  [    0/ 8784]\n",
      "loss: 2.182133  [ 4000/ 8784]\n",
      "loss: 1.194031  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.712659, Avg loss: 1.091669 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.194823  [    0/ 8784]\n",
      "loss: 0.253921  [ 4000/ 8784]\n",
      "loss: 1.079365  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.709472, Avg loss: 1.117095 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.415541  [    0/ 8784]\n",
      "loss: 0.751677  [ 4000/ 8784]\n",
      "loss: 0.555756  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.700364, Avg loss: 1.142851 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.235044  [    0/ 8784]\n",
      "loss: 0.275435  [ 4000/ 8784]\n",
      "loss: 1.920849  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.701730, Avg loss: 1.252693 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.156945  [    0/ 8784]\n",
      "loss: 0.846938  [ 4000/ 8784]\n",
      "loss: 2.080090  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.704007, Avg loss: 1.174366 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.346022  [    0/ 8784]\n",
      "loss: 0.686985  [ 4000/ 8784]\n",
      "loss: 0.193229  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.711293, Avg loss: 1.192468 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.083269  [    0/ 8784]\n",
      "loss: 0.802766  [ 4000/ 8784]\n",
      "loss: 0.862043  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.701730, Avg loss: 1.326849 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_net.use_embedding = True\n",
    "common_train(\n",
    "    epochs=15,\n",
    "    model=embeddings_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=DataLoader(train_indices_dataset, batch_size=8, shuffle=True),\n",
    "    test_dataloader=DataLoader(test_indices_dataset, batch_size=512),\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ");"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "outputs": [],
   "source": [
    "def inference(\n",
    "        surname: str,\n",
    "        target: str,\n",
    "        model: nn.Module,\n",
    "        vocab: Vocab,\n",
    "        labeler: LabelEncoder,\n",
    "        k: int = 3,\n",
    "        device: str = \"cpu\",\n",
    "):\n",
    "    x = vocab.encode(surname.lower())\n",
    "    x = x.to(device)\n",
    "\n",
    "    pred = model(x.unsqueeze(0))\n",
    "    pred_proba, pred_label_indices = F.softmax(pred, 1).topk(k, dim=1)\n",
    "    pred_labels = labeler.inverse_transform(pred_label_indices.squeeze().cpu())\n",
    "\n",
    "    predicts = \", \".join(\n",
    "        [f\"{label} ({prob:.2f})\" for (label, prob) in zip(pred_labels, pred_proba.squeeze())]\n",
    "    )\n",
    "    print(f\"Surname : {surname}\")\n",
    "    print(f\"True    : {target}\")\n",
    "    print(f\"Predicts: {predicts}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "id": "GHjCRqQg1sw5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname : Alexandrova\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Greek (0.00), Czech (0.00)\n",
      "\n",
      "Surname : Baranov\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Czech (0.00), English (0.00)\n",
      "\n",
      "Surname : Brusova\n",
      "True    : Russian\n",
      "Predicts: Czech (0.56), Spanish (0.17), Italian (0.08)\n",
      "\n",
      "Surname : Volkova\n",
      "True    : Russian\n",
      "Predicts: Russian (0.98), Czech (0.02), Polish (0.00)\n",
      "\n",
      "Surname : Kovalev\n",
      "True    : Russian\n",
      "Predicts: Russian (0.99), Czech (0.01), Polish (0.00)\n",
      "\n",
      "Surname : Kostyuchenko\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), English (0.00), German (0.00)\n",
      "\n",
      "Surname : Kuzin\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Czech (0.00), Irish (0.00)\n",
      "\n",
      "Surname : Likhachev\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Czech (0.00), English (0.00)\n",
      "\n",
      "Surname : Telitsyn\n",
      "True    : Russian\n",
      "Predicts: English (0.96), German (0.01), Irish (0.01)\n",
      "\n",
      "Surname : Ustimova\n",
      "True    : Russian\n",
      "Predicts: Japanese (0.61), Russian (0.39), Czech (0.00)\n",
      "\n",
      "Surname : Khamikoeva\n",
      "True    : Russian\n",
      "Predicts: Russian (1.00), Czech (0.00), Polish (0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students = [\n",
    "    \"Alexandrova\",\n",
    "    \"Baranov\",\n",
    "    \"Brusova\",\n",
    "    \"Volkova\",\n",
    "    \"Kovalev\",\n",
    "    \"Kostyuchenko\",\n",
    "    \"Kuzin\",\n",
    "    \"Likhachev\",\n",
    "    \"Telitsyn\",\n",
    "    \"Ustimova\",\n",
    "    \"Khamikoeva\",\n",
    "]\n",
    "for surname in students:\n",
    "    inference(\n",
    "        surname=surname,\n",
    "        target=\"Russian\",\n",
    "        model=embeddings_net,\n",
    "        vocab=vocab,\n",
    "        labeler=surnames_labeler,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo-hf5CQ0iWv"
   },
   "source": [
    "## 3. Классификация обзоров на фильмы (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "outputs": [],
   "source": [
    "def get_pos(word: str) -> str:\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_review(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z]\", repl=\" \", string=text, flags=re.MULTILINE)\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    words = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word not in STOPWORDS:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=get_pos(word))\n",
    "            if lemma not in STOPWORDS and len(lemma) > 1:\n",
    "                words.append(lemma)\n",
    "\n",
    "    return \" \".join(words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, positive_path: Path, negative_path: Path, seed: int = None):\n",
    "        self.positive_path = positive_path\n",
    "        self.negative_path = negative_path\n",
    "        self.positive_reviews = self.read_reviews(positive_path, preprocess_review)\n",
    "        self.negative_reviews = self.read_reviews(negative_path, preprocess_review)\n",
    "\n",
    "        data = self.positive_reviews + self.negative_reviews\n",
    "        targets = torch.cat([torch.ones(len(self.positive_reviews)), torch.zeros(len(self.negative_reviews))])\n",
    "\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        indices = torch.randperm(len(data))\n",
    "\n",
    "        self.data = [data[i] for i in indices]\n",
    "        self.targets = targets[indices].to(torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_reviews(path: Path, process: t.Callable[[str], str]) -> list[str]:\n",
    "        reviews = []\n",
    "        with open(path) as f:\n",
    "            for review in f.readlines():\n",
    "                review = process(review)\n",
    "                if review:\n",
    "                    reviews.append(review)\n",
    "        return reviews\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "outputs": [
    {
     "data": {
      "text/plain": "(10661,\n ('ludicrous director carl franklin add enough flourish freak make entertain',\n  tensor(0)))"
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_path = DATA_DIR / \"polarity\"\n",
    "reviews_dataset = ReviewsDataset(\n",
    "    reviews_path / \"positive_reviews.txt\",\n",
    "    reviews_path / \"negative_reviews.txt\",\n",
    "    seed=0,\n",
    ")\n",
    "len(reviews_dataset), reviews_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "outputs": [
    {
     "data": {
      "text/plain": "(9595, 1066)"
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_reviews, test_reviews = train_test_split(reviews_dataset, train_part=0.9)\n",
    "len(train_reviews), len(test_reviews)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "outputs": [],
   "source": [
    "class ReviewsVocab:\n",
    "    pad = \"<PAD>\"\n",
    "    unknown = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, reviews: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for review in reviews:\n",
    "            words = nltk.word_tokenize(review)\n",
    "            uniques.update(words)\n",
    "            max_len = max(len(words), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, self.unknown, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        w2i = {w: i for i, w in enumerate(self.alphabet)}\n",
    "        self.w2i = defaultdict(lambda: 1, w2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def encode(self, review: str) -> torch.Tensor:\n",
    "        indices = [self.w2i[w] for w in nltk.word_tokenize(review)]\n",
    "        indices += [self.w2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.w2i[self.pad], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \" \".join(self.alphabet[i] for i in indices)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet: 14097 longest: 38\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([    1,     1,     1, 13041,  4541,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]),\n '<UNK> <UNK> <UNK> neutral review')"
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ReviewsVocab([review for review, _ in train_reviews])\n",
    "print(f\"alphabet: {len(vocab)}\", f\"longest: {vocab.max_len}\")\n",
    "encoded = vocab.encode(\"this is a neutral review\")\n",
    "encoded, vocab.decode(encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2. Обучите классификатор.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`\n",
    "    - подберите адекватную размерность вектора эмбеддинга:\n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "outputs": [],
   "source": [
    "class ReviewsClassifier(nn.Module):\n",
    "    LAST_CONV_OUT_CHANNELS = 64\n",
    "    ADAPTIVE_AVG_POOL = 8\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super(ReviewsClassifier, self).__init__()\n",
    "\n",
    "        # как же этой модели все это... безразлично\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=self.LAST_CONV_OUT_CHANNELS, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=self.LAST_CONV_OUT_CHANNELS),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(self.ADAPTIVE_AVG_POOL)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.LAST_CONV_OUT_CHANNELS * self.ADAPTIVE_AVG_POOL, 256),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def collate_fn(batch: t.List[t.Tuple[str, torch.Tensor]]) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for x, y in batch:\n",
    "        xs.append(vocab.encode(x))\n",
    "        ys.append(y)\n",
    "    return torch.vstack(xs), torch.hstack(ys)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "net = ReviewsClassifier(num_embeddings=len(vocab), embedding_dim=128).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.000914092001)  # а почему бы и нет?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 0.697946  [    0/ 9595]\n",
      "loss: 0.690526  [ 3150/ 9595]\n",
      "loss: 0.689911  [ 6300/ 9595]\n",
      "loss: 0.692454  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.497186, Avg loss: 0.694098 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 0.687758  [    0/ 9595]\n",
      "loss: 0.704012  [ 3150/ 9595]\n",
      "loss: 0.657614  [ 6300/ 9595]\n",
      "loss: 0.665169  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.518762, Avg loss: 0.693731 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.644710  [    0/ 9595]\n",
      "loss: 0.599036  [ 3150/ 9595]\n",
      "loss: 0.512018  [ 6300/ 9595]\n",
      "loss: 0.718273  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.640713, Avg loss: 0.646798 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.485913  [    0/ 9595]\n",
      "loss: 0.683398  [ 3150/ 9595]\n",
      "loss: 0.597125  [ 6300/ 9595]\n",
      "loss: 0.521362  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.671670, Avg loss: 0.686190 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.300442  [    0/ 9595]\n",
      "loss: 0.563343  [ 3150/ 9595]\n",
      "loss: 0.306593  [ 6300/ 9595]\n",
      "loss: 0.406883  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.651032, Avg loss: 0.894114 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.130583  [    0/ 9595]\n",
      "loss: 0.172008  [ 3150/ 9595]\n",
      "loss: 0.084764  [ 6300/ 9595]\n",
      "loss: 0.172841  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.684803, Avg loss: 0.909196 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.089976  [    0/ 9595]\n",
      "loss: 0.007095  [ 3150/ 9595]\n",
      "loss: 0.199363  [ 6300/ 9595]\n",
      "loss: 0.023641  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.665103, Avg loss: 1.156370 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.020891  [    0/ 9595]\n",
      "loss: 0.010573  [ 3150/ 9595]\n",
      "loss: 0.014575  [ 6300/ 9595]\n",
      "loss: 0.057046  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.680113, Avg loss: 1.363881 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.025374  [    0/ 9595]\n",
      "loss: 0.003933  [ 3150/ 9595]\n",
      "loss: 0.003779  [ 6300/ 9595]\n",
      "loss: 0.027197  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.672608, Avg loss: 1.480069 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.001474  [    0/ 9595]\n",
      "loss: 0.030632  [ 3150/ 9595]\n",
      "loss: 0.168651  [ 6300/ 9595]\n",
      "loss: 0.026562  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.673546, Avg loss: 1.608278 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.004445  [    0/ 9595]\n",
      "loss: 0.008325  [ 3150/ 9595]\n",
      "loss: 0.000322  [ 6300/ 9595]\n",
      "loss: 0.003836  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.668856, Avg loss: 1.819266 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.004535  [    0/ 9595]\n",
      "loss: 0.006742  [ 3150/ 9595]\n",
      "loss: 0.012293  [ 6300/ 9595]\n",
      "loss: 0.019292  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.667917, Avg loss: 1.895648 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.003056  [    0/ 9595]\n",
      "loss: 0.022578  [ 3150/ 9595]\n",
      "loss: 0.013233  [ 6300/ 9595]\n",
      "loss: 0.000374  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.681989, Avg loss: 2.103672 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.004053  [    0/ 9595]\n",
      "loss: 0.002934  [ 3150/ 9595]\n",
      "loss: 0.000283  [ 6300/ 9595]\n",
      "loss: 0.029810  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.673546, Avg loss: 2.197270 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.067452  [    0/ 9595]\n",
      "loss: 0.000067  [ 3150/ 9595]\n",
      "loss: 0.000620  [ 6300/ 9595]\n",
      "loss: 0.010316  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.681989, Avg loss: 2.116356 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.027057  [    0/ 9595]\n",
      "loss: 0.000073  [ 3150/ 9595]\n",
      "loss: 0.000635  [ 6300/ 9595]\n",
      "loss: 0.000231  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.684803, Avg loss: 2.273048 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.003608  [    0/ 9595]\n",
      "loss: 0.000390  [ 3150/ 9595]\n",
      "loss: 0.000061  [ 6300/ 9595]\n",
      "loss: 0.140889  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.678236, Avg loss: 2.418122 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.006295  [    0/ 9595]\n",
      "loss: 0.000089  [ 3150/ 9595]\n",
      "loss: 0.000166  [ 6300/ 9595]\n",
      "loss: 0.001069  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.678236, Avg loss: 2.462211 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.000245  [    0/ 9595]\n",
      "loss: 0.001363  [ 3150/ 9595]\n",
      "loss: 0.000077  [ 6300/ 9595]\n",
      "loss: 0.000039  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.671670, Avg loss: 2.582768 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.000009  [    0/ 9595]\n",
      "loss: 0.000080  [ 3150/ 9595]\n",
      "loss: 0.000782  [ 6300/ 9595]\n",
      "loss: 0.000088  [ 9450/ 9595]\n",
      "Test Error: \n",
      " Accuracy: 0.670732, Avg loss: 2.224228 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_train(\n",
    "    epochs=20,\n",
    "    model=net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    # 21 > Embedding, Conv1d, BatchNorm1d, ReLU, MaxPool1d, AdaptiveAvgPool1d, Dropout\n",
    "    train_dataloader=DataLoader(train_reviews, batch_size=21, collate_fn=collate_fn, shuffle=True),\n",
    "    test_dataloader=DataLoader(test_reviews, batch_size=512, collate_fn=collate_fn),\n",
    "    verbose=150,\n",
    "    device=DEVICE,\n",
    ");"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPUQGO96Qffh8FYHODzJzxf",
   "collapsed_sections": [],
   "name": "blank__06_CNN_embeddings_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
