{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5669,
     "status": "ok",
     "timestamp": 1619632510103,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "zKMq7dp2W15Y",
    "outputId": "ce2273c5-6a96-4216-9d88-fbee51bf5ff0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        lr_scheduler=None,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "        if test_dataloader:\n",
    "            loss, acc = test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm-QilGISxkt"
   },
   "source": [
    "## 1. Классификация фамилий (RNN)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/frNchuaBQVLxyA?w=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdPr92i6k-If"
   },
   "source": [
    "1.1 Используя класс `nn.RNNCell` (абстракцию для отдельного временного шага RNN), реализуйте простейшую рекуррентную сеть Элмана в виде класса `RNN`. Используя созданный класс `RNN`, решите задачу классификации фамилий. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ir6UUkl6l4tp"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, hx: torch.Tensor = None):\n",
    "        batch_size, sequence_size, _ = inputs.size()\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(batch_size, self.hidden_size, dtype=inputs.dtype, device=inputs.device)\n",
    "        else:\n",
    "            hx = hx.squeeze(0)\n",
    "\n",
    "        hidden = []\n",
    "        for i in range(sequence_size):\n",
    "            hx = self.rnn_cell(inputs[i], hx)\n",
    "            hidden.append(hx)\n",
    "\n",
    "        hidden = torch.stack(hidden)\n",
    "        hx = hidden[-1].unsqueeze(0)\n",
    "        return hidden.permute(1, 0, 2), hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка реализации RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "input_size, hidden_size = 4, 5\n",
    "inputs = torch.randn(2, 3, input_size)\n",
    "hx = torch.randn(1, 2, hidden_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "my_rnn = RNN(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "true_rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6515,  0.5430,  0.4023,  0.6325, -0.6068],\n",
       "          [ 0.9149, -0.1088,  0.6385, -0.7387,  0.7532],\n",
       "          [-0.6936,  0.5123, -0.2784, -0.5693, -0.0055]],\n",
       " \n",
       "         [[ 0.1954,  0.6152,  0.2958, -0.8005,  0.8074],\n",
       "          [-0.4577,  0.7566,  0.2972, -0.8834,  0.1265],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<PermuteBackward>),\n",
       " tensor([[[-0.6936,  0.5123, -0.2784, -0.5693, -0.0055],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rnn(inputs, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6515,  0.5430,  0.4023,  0.6325, -0.6068],\n",
       "          [ 0.9149, -0.1088,  0.6385, -0.7387,  0.7532],\n",
       "          [-0.6936,  0.5123, -0.2784, -0.5693, -0.0055]],\n",
       " \n",
       "         [[ 0.1954,  0.6152,  0.2958, -0.8005,  0.8074],\n",
       "          [-0.4577,  0.7566,  0.2972, -0.8834,  0.1265],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<TransposeBackward1>),\n",
       " tensor([[[-0.6936,  0.5123, -0.2784, -0.5693, -0.0055],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_rnn(inputs, hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesRNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.hx = None\n",
    "        self.rnn = RNN(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x, hx = self.rnn(x, self.hx)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesVocab:\n",
    "    pad = \"<PAD>\"\n",
    "\n",
    "    def __init__(self, surnames: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for w in map(str.lower, surnames):\n",
    "            uniques.update(w)\n",
    "            max_len = max(len(w), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, *uniques]\n",
    "        self.max_len = max_len\n",
    "        self.ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in word]\n",
    "        indices += [self.ch2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.pad], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class SurnamesDataset(Dataset):\n",
    "    df: pd.DataFrame\n",
    "    surnames: t.List[str]\n",
    "    vocab: SurnamesVocab\n",
    "    labeler: LabelEncoder\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path):\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "        self.surnames = self.df[\"surname\"].tolist()\n",
    "        self.vocab = SurnamesVocab(self.surnames)\n",
    "        size = self.vocab.encode(self.surnames[0].lower()).size()\n",
    "        data = torch.vstack([self.vocab.encode(w.lower()) for w in self.surnames])\n",
    "        self.data = data.view(len(self.surnames), *size)\n",
    "\n",
    "        self.labeler = LabelEncoder()\n",
    "        targets = self.labeler.fit_transform(self.df[\"nationality\"])\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        return self.vocab.encode(word)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10980"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_dataset = SurnamesDataset(DATA_DIR / \"surnames.csv\")\n",
    "len(surnames_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8784 2196\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_surnames_dataset, test_surnames_dataset = train_test_split(surnames_dataset, train_part=0.8)\n",
    "print(len(train_surnames_dataset), len(test_surnames_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handmade RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "handmade_rnn_net = SurnamesRNNClassifier(\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(handmade_rnn_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.894605  [    0/ 8784]\n",
      "loss: 1.477110  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.625683, Avg loss: 1.310740 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.462449  [    0/ 8784]\n",
      "loss: 1.213015  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.698087, Avg loss: 1.074561 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.054994  [    0/ 8784]\n",
      "loss: 0.907282  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.723588, Avg loss: 0.966488 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.925219  [    0/ 8784]\n",
      "loss: 0.604936  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.739071, Avg loss: 0.911016 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.821252  [    0/ 8784]\n",
      "loss: 0.572490  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.744080, Avg loss: 0.870962 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.643748  [    0/ 8784]\n",
      "loss: 0.752343  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.751821, Avg loss: 0.865607 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.456838  [    0/ 8784]\n",
      "loss: 0.786814  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.749545, Avg loss: 0.847784 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.575263  [    0/ 8784]\n",
      "loss: 0.517968  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.758652, Avg loss: 0.846680 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.434301  [    0/ 8784]\n",
      "loss: 0.714787  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.758197, Avg loss: 0.848796 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.537116  [    0/ 8784]\n",
      "loss: 0.560013  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.767304, Avg loss: 0.848031 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.489112  [    0/ 8784]\n",
      "loss: 0.553501  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.759107, Avg loss: 0.831740 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.431962  [    0/ 8784]\n",
      "loss: 0.560924  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.758652, Avg loss: 0.843060 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.408930  [    0/ 8784]\n",
      "loss: 0.545018  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.765027, Avg loss: 0.848230 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.453086  [    0/ 8784]\n",
      "loss: 0.556772  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.760018, Avg loss: 0.884489 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.386779  [    0/ 8784]\n",
      "loss: 0.233403  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.770492, Avg loss: 0.904058 \n",
      "\n",
      "CPU times: user 13.6 s, sys: 1.6 s, total: 15.2 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=handmade_rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.97      1.00      0.99       340\n",
      "     Chinese       0.75      0.71      0.73        38\n",
      "       Czech       0.64      0.28      0.39        96\n",
      "       Dutch       0.79      0.43      0.56        51\n",
      "     English       0.68      0.90      0.78       573\n",
      "      French       0.27      0.10      0.15        39\n",
      "      German       0.59      0.42      0.49       121\n",
      "       Greek       0.86      0.56      0.68        34\n",
      "       Irish       0.75      0.32      0.45        37\n",
      "     Italian       0.72      0.77      0.74       128\n",
      "    Japanese       0.83      0.85      0.84       156\n",
      "      Korean       0.44      0.40      0.42        10\n",
      "      Polish       0.61      0.54      0.57        26\n",
      "  Portuguese       0.00      0.00      0.00         9\n",
      "     Russian       0.86      0.87      0.86       458\n",
      "    Scottish       1.00      0.00      0.00        17\n",
      "     Spanish       0.57      0.54      0.56        50\n",
      "  Vietnamese       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.77      2196\n",
      "   macro avg       0.63      0.48      0.51      2196\n",
      "weighted avg       0.76      0.77      0.75      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(handmade_rnn_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2MIErKTo9aO"
   },
   "source": [
    "1.2 Замените модуль `RNN` из 1.1 на модули `nn.RNN`, `nn.LSTM` и `nn.GRU` (не забудьте указать аргумент `batch_first=True`). Сравните результаты работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesAutobotRNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.hx = None\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x, hx = self.rnn(x, self.hx)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "rnn_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.RNN,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.901579  [    0/ 8784]\n",
      "loss: 1.583601  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.577413, Avg loss: 1.460604 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.594686  [    0/ 8784]\n",
      "loss: 1.441510  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.637978, Avg loss: 1.242304 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.201505  [    0/ 8784]\n",
      "loss: 1.026210  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.678506, Avg loss: 1.120839 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.138262  [    0/ 8784]\n",
      "loss: 0.840799  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.704007, Avg loss: 1.015837 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.024570  [    0/ 8784]\n",
      "loss: 0.755569  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.710838, Avg loss: 1.000529 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.771986  [    0/ 8784]\n",
      "loss: 0.869066  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.722678, Avg loss: 0.957680 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.713295  [    0/ 8784]\n",
      "loss: 0.903014  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.722222, Avg loss: 0.949138 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.652614  [    0/ 8784]\n",
      "loss: 0.720292  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.734517, Avg loss: 0.929289 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.560474  [    0/ 8784]\n",
      "loss: 0.794221  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.734973, Avg loss: 0.941923 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.711342  [    0/ 8784]\n",
      "loss: 0.644326  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.739071, Avg loss: 0.931708 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.596264  [    0/ 8784]\n",
      "loss: 0.690651  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.738160, Avg loss: 0.920841 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.599928  [    0/ 8784]\n",
      "loss: 0.731072  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.742714, Avg loss: 0.922767 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.539134  [    0/ 8784]\n",
      "loss: 0.736039  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.746812, Avg loss: 0.933416 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.521956  [    0/ 8784]\n",
      "loss: 0.559023  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.747723, Avg loss: 0.946976 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.441063  [    0/ 8784]\n",
      "loss: 0.426684  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.750000, Avg loss: 0.969711 \n",
      "\n",
      "CPU times: user 11.7 s, sys: 1.35 s, total: 13 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.97      1.00      0.99       340\n",
      "     Chinese       0.68      0.71      0.69        38\n",
      "       Czech       0.59      0.25      0.35        96\n",
      "       Dutch       0.88      0.41      0.56        51\n",
      "     English       0.65      0.90      0.76       573\n",
      "      French       0.23      0.08      0.12        39\n",
      "      German       0.61      0.41      0.49       121\n",
      "       Greek       0.69      0.59      0.63        34\n",
      "       Irish       0.70      0.19      0.30        37\n",
      "     Italian       0.62      0.68      0.65       128\n",
      "    Japanese       0.86      0.85      0.86       156\n",
      "      Korean       0.10      0.10      0.10        10\n",
      "      Polish       0.59      0.50      0.54        26\n",
      "  Portuguese       0.33      0.11      0.17         9\n",
      "     Russian       0.86      0.84      0.85       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.51      0.38      0.44        50\n",
      "  Vietnamese       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.75      2196\n",
      "   macro avg       0.60      0.44      0.47      2196\n",
      "weighted avg       0.74      0.75      0.73      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(rnn_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "lstm_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.LSTM,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.908684  [    0/ 8784]\n",
      "loss: 1.817289  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.537341, Avg loss: 1.629798 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.460007  [    0/ 8784]\n",
      "loss: 1.196895  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.601548, Avg loss: 1.364094 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.310088  [    0/ 8784]\n",
      "loss: 1.463891  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.650273, Avg loss: 1.209218 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.372692  [    0/ 8784]\n",
      "loss: 1.153307  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.682605, Avg loss: 1.096049 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.933651  [    0/ 8784]\n",
      "loss: 0.957198  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.705373, Avg loss: 1.024245 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.919880  [    0/ 8784]\n",
      "loss: 0.989278  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.708561, Avg loss: 0.981927 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.620268  [    0/ 8784]\n",
      "loss: 0.828350  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.722222, Avg loss: 0.956686 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.689240  [    0/ 8784]\n",
      "loss: 0.742189  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.723588, Avg loss: 0.942551 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.632372  [    0/ 8784]\n",
      "loss: 0.757831  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.735883, Avg loss: 0.935546 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.760626  [    0/ 8784]\n",
      "loss: 0.609592  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.739526, Avg loss: 0.903424 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.809643  [    0/ 8784]\n",
      "loss: 0.666741  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.735883, Avg loss: 0.934456 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.638307  [    0/ 8784]\n",
      "loss: 0.529187  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.739982, Avg loss: 0.925298 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.550252  [    0/ 8784]\n",
      "loss: 0.562175  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.742259, Avg loss: 0.911577 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.520475  [    0/ 8784]\n",
      "loss: 0.727367  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.743625, Avg loss: 0.907508 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.484846  [    0/ 8784]\n",
      "loss: 0.729332  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.745902, Avg loss: 0.924102 \n",
      "\n",
      "CPU times: user 11.3 s, sys: 1.1 s, total: 12.4 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=lstm_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.93      1.00      0.97       340\n",
      "     Chinese       0.67      0.74      0.70        38\n",
      "       Czech       0.51      0.20      0.29        96\n",
      "       Dutch       0.69      0.35      0.47        51\n",
      "     English       0.65      0.89      0.76       573\n",
      "      French       0.10      0.03      0.04        39\n",
      "      German       0.67      0.38      0.48       121\n",
      "       Greek       0.71      0.50      0.59        34\n",
      "       Irish       0.92      0.30      0.45        37\n",
      "     Italian       0.67      0.73      0.70       128\n",
      "    Japanese       0.87      0.82      0.84       156\n",
      "      Korean       0.22      0.20      0.21        10\n",
      "      Polish       0.62      0.38      0.48        26\n",
      "  Portuguese       1.00      0.00      0.00         9\n",
      "     Russian       0.82      0.86      0.84       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.49      0.38      0.43        50\n",
      "  Vietnamese       0.50      0.08      0.13        13\n",
      "\n",
      "    accuracy                           0.75      2196\n",
      "   macro avg       0.61      0.44      0.46      2196\n",
      "weighted avg       0.73      0.75      0.72      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(lstm_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "gru_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.GRU,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.899589  [    0/ 8784]\n",
      "loss: 1.802641  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.544171, Avg loss: 1.556026 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.625537  [    0/ 8784]\n",
      "loss: 1.396525  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.618397, Avg loss: 1.300508 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.280757  [    0/ 8784]\n",
      "loss: 1.290818  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.666211, Avg loss: 1.151116 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.177101  [    0/ 8784]\n",
      "loss: 1.215153  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.680328, Avg loss: 1.065233 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.980553  [    0/ 8784]\n",
      "loss: 0.903879  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.700820, Avg loss: 1.014038 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.875414  [    0/ 8784]\n",
      "loss: 0.799713  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.717668, Avg loss: 0.994450 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.949876  [    0/ 8784]\n",
      "loss: 0.987676  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.720401, Avg loss: 0.967431 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.790546  [    0/ 8784]\n",
      "loss: 0.930677  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728597, Avg loss: 0.944903 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.665739  [    0/ 8784]\n",
      "loss: 0.659209  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.736794, Avg loss: 0.908897 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.694373  [    0/ 8784]\n",
      "loss: 0.797988  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.741803, Avg loss: 0.915884 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.535014  [    0/ 8784]\n",
      "loss: 0.763799  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.735428, Avg loss: 0.917209 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.659247  [    0/ 8784]\n",
      "loss: 0.608453  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.745902, Avg loss: 0.890730 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.491198  [    0/ 8784]\n",
      "loss: 0.767515  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.745446, Avg loss: 0.919303 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.529140  [    0/ 8784]\n",
      "loss: 0.450555  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.747723, Avg loss: 0.913973 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.590804  [    0/ 8784]\n",
      "loss: 0.539918  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.749545, Avg loss: 0.906096 \n",
      "\n",
      "CPU times: user 11.6 s, sys: 1.08 s, total: 12.7 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=gru_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.95      1.00      0.97       340\n",
      "     Chinese       0.72      0.68      0.70        38\n",
      "       Czech       0.44      0.28      0.34        96\n",
      "       Dutch       0.84      0.41      0.55        51\n",
      "     English       0.67      0.86      0.75       573\n",
      "      French       0.17      0.05      0.08        39\n",
      "      German       0.52      0.46      0.49       121\n",
      "       Greek       0.72      0.53      0.61        34\n",
      "       Irish       0.86      0.32      0.47        37\n",
      "     Italian       0.65      0.70      0.68       128\n",
      "    Japanese       0.86      0.86      0.86       156\n",
      "      Korean       0.27      0.30      0.29        10\n",
      "      Polish       0.73      0.31      0.43        26\n",
      "  Portuguese       0.00      0.00      0.00         9\n",
      "     Russian       0.85      0.86      0.85       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.54      0.42      0.47        50\n",
      "  Vietnamese       0.50      0.15      0.24        13\n",
      "\n",
      "    accuracy                           0.75      2196\n",
      "   macro avg       0.57      0.46      0.49      2196\n",
      "weighted avg       0.73      0.75      0.73      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(gru_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6YBam_3t-fO"
   },
   "source": [
    "1.3 Загрузите предобученные эмбеддинги (https://disk.yandex.ru/d/BHuT2tEXr_yBOQ?w=1) в модуль `nn.Embedding` и обучите модели из 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7kf990U9Do-"
   },
   "source": [
    "## 2. Классификация новостей на основе заголовка\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/FN-EgWGIpyjLxQ?w=1\n",
    "<br>Эмбеддинги: https://nlp.stanford.edu/projects/glove/ (находите ссылку на архив\n",
    "glove.6B.zip, в нем несколько файлов с эмбеддингами слов, выбираете один из файлов в\n",
    "архиве)\n",
    "<br><br>2.1 Загрузите набор данных train.csv. Выполните предобработку столбца Title\n",
    "<br><br>2.2 На основе этих данных создайте датасет NewsDataset . Не забудьте добавить\n",
    "специальные токены <PAD> для дополнения последовательностей до нужной длины и\n",
    "<UNK> для корректной обработке ранее не встречавшихся токенов. В данной задаче\n",
    "рассматривайте отдельные слова как токены. Разбейте датасет на обучающее и\n",
    "валидационное множество.\n",
    "<br><br>2.3 Создайте модель для классификации, используя слой nn.Embedding и слой nn.RNN .\n",
    "эмбеддинги инициализируйте случайным образом\n",
    "не забудьте указать аргумент padding_idx для nn.Embedding\n",
    "<br><br>2.4 Переобучите модель, заменив слой nn.RNN на nn.LSTM и nn.GRU . Сравните качество\n",
    "на тестовой выборке. Результаты сведите в таблицу (модель/метрика качества на\n",
    "тестовом множестве).\n",
    "<br><br>2.5 Выполните пункты 2.3 и 2.4, используя предобученные эмбеддинги Glove.\n",
    "Прокомментируйте результат.\n",
    "Эмбеддинги из скачанного файла загрузите в виде двумерного тензора\n",
    "pretrained_embeddings .\n",
    "Обратите внимание, что номер строки в этом тензоре должен соответствовать\n",
    "токену (слову), имеющему такой индекс в вашем словаре.\n",
    "для слов, которых нет в файле с эмбеддингами, инициализуйте эмбеддинг\n",
    "случайным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPopRjm9A6la5QJRG/PWjfN",
   "collapsed_sections": [],
   "name": "blank__07_rnn_1_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
