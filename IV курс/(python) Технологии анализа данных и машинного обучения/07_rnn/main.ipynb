{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5669,
     "status": "ok",
     "timestamp": 1619632510103,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "zKMq7dp2W15Y",
    "outputId": "ce2273c5-6a96-4216-9d88-fbee51bf5ff0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\"\n",
    "\n",
    "\n",
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        lr_scheduler=None,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "        if test_dataloader:\n",
    "            loss, acc = test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm-QilGISxkt"
   },
   "source": [
    "## 1. Классификация фамилий (RNN)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/frNchuaBQVLxyA?w=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdPr92i6k-If"
   },
   "source": [
    "1.1 Используя класс `nn.RNNCell` (абстракцию для отдельного временного шага RNN), реализуйте простейшую рекуррентную сеть Элмана в виде класса `RNN`. Используя созданный класс `RNN`, решите задачу классификации фамилий. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "ir6UUkl6l4tp"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, hx: torch.Tensor = None):\n",
    "        batch_size, sequence_size, _ = inputs.size()\n",
    "        inputs = inputs.permute(1, 0, 2)  # для nn.RNNCell batch_size должен быть на 2-ой месте\n",
    "\n",
    "        if hx is None:\n",
    "            # так же скрытое состояние инициализируется в nn.RNN\n",
    "            hx = torch.zeros(batch_size, self.hidden_size, dtype=inputs.dtype, device=inputs.device)\n",
    "        else:\n",
    "            # 1-ая размерность равная 1 для совместимости с nn.RNN\n",
    "            hx = hx.squeeze(0)  # избавляемся от 1-ой размерности равной 1\n",
    "\n",
    "        hidden = []\n",
    "        for i in range(sequence_size):\n",
    "            hx = self.rnn_cell(inputs[i], hx)\n",
    "            hidden.append(hx)\n",
    "\n",
    "        hidden = torch.stack(hidden)\n",
    "        hx = hidden[-1].unsqueeze(0)\n",
    "        return hidden.permute(1, 0, 2), hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка реализации RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "input_size, hidden_size = 4, 5\n",
    "inputs = torch.randn(2, 3, input_size)\n",
    "hx = torch.randn(1, 2, hidden_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "my_rnn = RNN(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "true_rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([1, 2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[[ 0.6515,  0.5430,  0.4023,  0.6325, -0.6068],\n          [ 0.9149, -0.1088,  0.6385, -0.7387,  0.7532],\n          [-0.6936,  0.5123, -0.2784, -0.5693, -0.0055]],\n \n         [[ 0.1954,  0.6152,  0.2958, -0.8005,  0.8074],\n          [-0.4577,  0.7566,  0.2972, -0.8834,  0.1265],\n          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n        grad_fn=<PermuteBackward0>),\n tensor([[[-0.6936,  0.5123, -0.2784, -0.5693, -0.0055],\n          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n        grad_fn=<UnsqueezeBackward0>))"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rnn(inputs, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.6515,  0.5430,  0.4023,  0.6325, -0.6068],\n          [ 0.9149, -0.1088,  0.6385, -0.7387,  0.7532],\n          [-0.6936,  0.5123, -0.2784, -0.5693, -0.0055]],\n \n         [[ 0.1954,  0.6152,  0.2958, -0.8005,  0.8074],\n          [-0.4577,  0.7566,  0.2972, -0.8834,  0.1265],\n          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n        grad_fn=<TransposeBackward1>),\n tensor([[[-0.6936,  0.5123, -0.2784, -0.5693, -0.0055],\n          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n        grad_fn=<StackBackward0>))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_rnn(inputs, hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesRNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.rnn = RNN(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x, hx = self.rnn(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesVocab:\n",
    "    pad = \"<PAD>\"\n",
    "\n",
    "    def __init__(self, surnames: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for w in map(str.lower, surnames):\n",
    "            uniques.update(w)\n",
    "            max_len = max(len(w), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, *uniques]\n",
    "        self.max_len = max_len\n",
    "        self.ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in word]\n",
    "        indices += [self.ch2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.pad], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class SurnamesDataset(Dataset):\n",
    "    df: pd.DataFrame\n",
    "    surnames: t.List[str]\n",
    "    vocab: SurnamesVocab\n",
    "    labeler: LabelEncoder\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path):\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "        self.surnames = self.df[\"surname\"].tolist()\n",
    "        self.vocab = SurnamesVocab(self.surnames)\n",
    "        size = self.vocab.encode(self.surnames[0].lower()).size()\n",
    "        data = torch.vstack([self.vocab.encode(w.lower()) for w in self.surnames])\n",
    "        self.data = data.view(len(self.surnames), *size)\n",
    "\n",
    "        self.labeler = LabelEncoder()\n",
    "        targets = self.labeler.fit_transform(self.df[\"nationality\"])\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        return self.vocab.encode(word)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "10980"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_dataset = SurnamesDataset(DATA_DIR / \"surnames.csv\")\n",
    "len(surnames_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8784 2196\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_surnames_dataset, test_surnames_dataset = train_test_split(surnames_dataset, train_part=0.8)\n",
    "print(len(train_surnames_dataset), len(test_surnames_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handmade RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "handmade_rnn_net = SurnamesRNNClassifier(\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=128,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(handmade_rnn_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.878279  [    0/ 8784]\n",
      "loss: 1.349192  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.635246, Avg loss: 1.264551 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.322870  [    0/ 8784]\n",
      "loss: 1.030583  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.700364, Avg loss: 1.033390 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.802031  [    0/ 8784]\n",
      "loss: 1.007586  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.724044, Avg loss: 0.946684 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.785045  [    0/ 8784]\n",
      "loss: 0.951692  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.737705, Avg loss: 0.891038 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.708065  [    0/ 8784]\n",
      "loss: 0.741949  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.741348, Avg loss: 0.868706 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.566563  [    0/ 8784]\n",
      "loss: 0.636736  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.751821, Avg loss: 0.831084 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.800768  [    0/ 8784]\n",
      "loss: 0.649259  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.751366, Avg loss: 0.830599 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.523792  [    0/ 8784]\n",
      "loss: 0.496888  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.757741, Avg loss: 0.827746 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.701128  [    0/ 8784]\n",
      "loss: 0.618194  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.758652, Avg loss: 0.842623 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.539719  [    0/ 8784]\n",
      "loss: 0.366725  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.770492, Avg loss: 0.827514 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.382926  [    0/ 8784]\n",
      "loss: 0.519315  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.763206, Avg loss: 0.821716 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.396629  [    0/ 8784]\n",
      "loss: 0.560020  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.763661, Avg loss: 0.869226 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.412334  [    0/ 8784]\n",
      "loss: 0.377753  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.763661, Avg loss: 0.874624 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.295505  [    0/ 8784]\n",
      "loss: 0.338182  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.759107, Avg loss: 0.844998 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.260803  [    0/ 8784]\n",
      "loss: 0.395218  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.763661, Avg loss: 0.858614 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.285025  [    0/ 8784]\n",
      "loss: 0.516236  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.768670, Avg loss: 0.911433 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.344251  [    0/ 8784]\n",
      "loss: 0.464063  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.762750, Avg loss: 0.890921 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.343956  [    0/ 8784]\n",
      "loss: 0.370165  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.765027, Avg loss: 0.926461 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.278008  [    0/ 8784]\n",
      "loss: 0.349103  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.768215, Avg loss: 0.941665 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.326343  [    0/ 8784]\n",
      "loss: 0.317084  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.761384, Avg loss: 0.949485 \n",
      "\n",
      "CPU times: total: 23 s\n",
      "Wall time: 36.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=handmade_rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.96      1.00      0.98       340\n",
      "     Chinese       0.76      0.66      0.70        38\n",
      "       Czech       0.64      0.31      0.42        96\n",
      "       Dutch       0.76      0.43      0.55        51\n",
      "     English       0.73      0.86      0.79       573\n",
      "      French       0.19      0.13      0.15        39\n",
      "      German       0.53      0.56      0.55       121\n",
      "       Greek       0.72      0.68      0.70        34\n",
      "       Irish       0.61      0.38      0.47        37\n",
      "     Italian       0.67      0.76      0.71       128\n",
      "    Japanese       0.85      0.83      0.84       156\n",
      "      Korean       0.38      0.30      0.33        10\n",
      "      Polish       0.58      0.54      0.56        26\n",
      "  Portuguese       0.00      0.00      0.00         9\n",
      "     Russian       0.85      0.85      0.85       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.37      0.38      0.38        50\n",
      "  Vietnamese       0.25      0.08      0.12        13\n",
      "\n",
      "    accuracy                           0.76      2196\n",
      "   macro avg       0.55      0.49      0.51      2196\n",
      "weighted avg       0.75      0.76      0.75      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(handmade_rnn_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2MIErKTo9aO"
   },
   "source": [
    "1.2 Замените модуль `RNN` из 1.1 на модули `nn.RNN`, `nn.LSTM` и `nn.GRU` (не забудьте указать аргумент `batch_first=True`). Сравните результаты работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesAutobotRNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.hx, self.cx = None, None\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if isinstance(self.rnn, (nn.RNN, nn.GRU)):\n",
    "            x, hx = self.rnn(x, self.hx)\n",
    "            self.hx = hx.detach()\n",
    "        else:\n",
    "            if self.hx is not None and self.cx is not None:\n",
    "                hx_cx = (self.hx, self.cx)\n",
    "            else:\n",
    "                hx_cx = None\n",
    "            x, (hx, cx) = self.rnn(x, hx_cx)\n",
    "            self.cx = cx.detach()\n",
    "            self.hx = hx.detach()\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "rnn_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.RNN,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=128,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.872500  [    0/ 8784]\n",
      "loss: 1.533388  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.584699, Avg loss: 1.441185 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.513549  [    0/ 8784]\n",
      "loss: 1.125278  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.644809, Avg loss: 1.203117 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.963235  [    0/ 8784]\n",
      "loss: 1.213355  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.676230, Avg loss: 1.101182 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.992673  [    0/ 8784]\n",
      "loss: 1.117367  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.701275, Avg loss: 1.032854 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.899363  [    0/ 8784]\n",
      "loss: 0.877292  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.702641, Avg loss: 1.012262 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.748350  [    0/ 8784]\n",
      "loss: 0.720974  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.725410, Avg loss: 0.950959 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.795079  [    0/ 8784]\n",
      "loss: 0.730063  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.726776, Avg loss: 0.943356 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.739941  [    0/ 8784]\n",
      "loss: 0.689762  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.731330, Avg loss: 0.935855 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.782739  [    0/ 8784]\n",
      "loss: 0.614020  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.723588, Avg loss: 0.936809 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.690164  [    0/ 8784]\n",
      "loss: 0.429780  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.730874, Avg loss: 0.947116 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.524979  [    0/ 8784]\n",
      "loss: 0.645177  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.737250, Avg loss: 0.917634 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.506539  [    0/ 8784]\n",
      "loss: 0.651226  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.742259, Avg loss: 0.930473 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.506816  [    0/ 8784]\n",
      "loss: 0.598392  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.741803, Avg loss: 0.938181 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.352596  [    0/ 8784]\n",
      "loss: 0.449015  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.744080, Avg loss: 0.926476 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.371228  [    0/ 8784]\n",
      "loss: 0.587430  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.750455, Avg loss: 0.938029 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.454866  [    0/ 8784]\n",
      "loss: 0.682317  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.745902, Avg loss: 0.950525 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.362429  [    0/ 8784]\n",
      "loss: 0.561755  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.741348, Avg loss: 0.957931 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.416293  [    0/ 8784]\n",
      "loss: 0.476131  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.736339, Avg loss: 0.989433 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.382178  [    0/ 8784]\n",
      "loss: 0.455865  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.739526, Avg loss: 0.987968 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.440805  [    0/ 8784]\n",
      "loss: 0.461406  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.745446, Avg loss: 0.966325 \n",
      "\n",
      "CPU times: total: 7.83 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.96      1.00      0.98       340\n",
      "     Chinese       0.70      0.68      0.69        38\n",
      "       Czech       0.50      0.22      0.30        96\n",
      "       Dutch       0.67      0.35      0.46        51\n",
      "     English       0.67      0.87      0.76       573\n",
      "      French       0.21      0.10      0.14        39\n",
      "      German       0.60      0.42      0.50       121\n",
      "       Greek       0.67      0.47      0.55        34\n",
      "       Irish       0.69      0.30      0.42        37\n",
      "     Italian       0.61      0.70      0.65       128\n",
      "    Japanese       0.83      0.84      0.84       156\n",
      "      Korean       0.21      0.30      0.25        10\n",
      "      Polish       0.67      0.46      0.55        26\n",
      "  Portuguese       0.25      0.11      0.15         9\n",
      "     Russian       0.86      0.85      0.86       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.47      0.38      0.42        50\n",
      "  Vietnamese       0.60      0.23      0.33        13\n",
      "\n",
      "    accuracy                           0.75      2196\n",
      "   macro avg       0.56      0.46      0.49      2196\n",
      "weighted avg       0.73      0.75      0.73      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(rnn_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "lstm_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.LSTM,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=128,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.889345  [    0/ 8784]\n",
      "loss: 1.869502  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.548270, Avg loss: 1.580504 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.611331  [    0/ 8784]\n",
      "loss: 1.340435  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.615665, Avg loss: 1.319704 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.414919  [    0/ 8784]\n",
      "loss: 1.113623  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.661658, Avg loss: 1.163483 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.102015  [    0/ 8784]\n",
      "loss: 0.984063  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.692623, Avg loss: 1.071542 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.862351  [    0/ 8784]\n",
      "loss: 0.802106  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.698087, Avg loss: 1.012793 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.773141  [    0/ 8784]\n",
      "loss: 0.899641  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.708561, Avg loss: 0.983065 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.737374  [    0/ 8784]\n",
      "loss: 0.900465  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.720401, Avg loss: 0.953772 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.807869  [    0/ 8784]\n",
      "loss: 0.862732  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.729508, Avg loss: 0.931089 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.749683  [    0/ 8784]\n",
      "loss: 0.736305  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.731785, Avg loss: 0.921047 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.892501  [    0/ 8784]\n",
      "loss: 0.742717  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.729964, Avg loss: 0.908871 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.622978  [    0/ 8784]\n",
      "loss: 0.804298  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.742714, Avg loss: 0.911400 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.512634  [    0/ 8784]\n",
      "loss: 0.738553  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.735883, Avg loss: 0.912740 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.620675  [    0/ 8784]\n",
      "loss: 0.621850  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.747723, Avg loss: 0.901882 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.416282  [    0/ 8784]\n",
      "loss: 0.703560  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.746357, Avg loss: 0.918515 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.495053  [    0/ 8784]\n",
      "loss: 0.557735  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.737705, Avg loss: 0.900658 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.443569  [    0/ 8784]\n",
      "loss: 0.536822  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.739982, Avg loss: 0.907012 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.558914  [    0/ 8784]\n",
      "loss: 0.594620  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.740437, Avg loss: 0.911507 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.462774  [    0/ 8784]\n",
      "loss: 0.526828  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.747268, Avg loss: 0.927200 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.419323  [    0/ 8784]\n",
      "loss: 0.544441  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.742714, Avg loss: 0.922044 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.338585  [    0/ 8784]\n",
      "loss: 0.537348  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.750000, Avg loss: 0.934470 \n",
      "\n",
      "CPU times: total: 8.59 s\n",
      "Wall time: 9.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=lstm_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.98      1.00      0.99       340\n",
      "     Chinese       0.66      0.76      0.71        38\n",
      "       Czech       0.46      0.19      0.27        96\n",
      "       Dutch       0.95      0.39      0.56        51\n",
      "     English       0.67      0.88      0.76       573\n",
      "      French       0.11      0.05      0.07        39\n",
      "      German       0.57      0.45      0.50       121\n",
      "       Greek       0.58      0.53      0.55        34\n",
      "       Irish       0.73      0.30      0.42        37\n",
      "     Italian       0.70      0.68      0.69       128\n",
      "    Japanese       0.88      0.85      0.87       156\n",
      "      Korean       0.10      0.10      0.10        10\n",
      "      Polish       0.56      0.38      0.45        26\n",
      "  Portuguese       0.00      0.00      0.00         9\n",
      "     Russian       0.80      0.88      0.84       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.56      0.30      0.39        50\n",
      "  Vietnamese       1.00      0.08      0.14        13\n",
      "\n",
      "    accuracy                           0.75      2196\n",
      "   macro avg       0.57      0.43      0.46      2196\n",
      "weighted avg       0.73      0.75      0.73      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(lstm_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "gru_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.GRU,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=128,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.877352  [    0/ 8784]\n",
      "loss: 1.645322  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.557377, Avg loss: 1.544307 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.330766  [    0/ 8784]\n",
      "loss: 1.426452  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.638889, Avg loss: 1.266454 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.129196  [    0/ 8784]\n",
      "loss: 1.019272  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.677140, Avg loss: 1.118819 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.941416  [    0/ 8784]\n",
      "loss: 1.017978  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.704463, Avg loss: 1.054566 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.103538  [    0/ 8784]\n",
      "loss: 0.977748  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.705373, Avg loss: 1.010324 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.826999  [    0/ 8784]\n",
      "loss: 0.795982  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.722222, Avg loss: 0.977933 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.745100  [    0/ 8784]\n",
      "loss: 0.742804  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.725410, Avg loss: 0.961646 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.859863  [    0/ 8784]\n",
      "loss: 0.748707  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728142, Avg loss: 0.948528 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.702631  [    0/ 8784]\n",
      "loss: 0.748546  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.734517, Avg loss: 0.944284 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.734824  [    0/ 8784]\n",
      "loss: 0.702680  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.740437, Avg loss: 0.937105 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.423050  [    0/ 8784]\n",
      "loss: 0.652384  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.732240, Avg loss: 0.955752 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.593650  [    0/ 8784]\n",
      "loss: 0.481825  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.740437, Avg loss: 0.939871 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.524544  [    0/ 8784]\n",
      "loss: 0.577538  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.744080, Avg loss: 0.949009 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.555465  [    0/ 8784]\n",
      "loss: 0.583883  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.743625, Avg loss: 0.936561 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.466024  [    0/ 8784]\n",
      "loss: 0.520850  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.744080, Avg loss: 0.941384 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.298009  [    0/ 8784]\n",
      "loss: 0.459616  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.743169, Avg loss: 0.960036 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.463510  [    0/ 8784]\n",
      "loss: 0.498225  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.750000, Avg loss: 0.954091 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.417855  [    0/ 8784]\n",
      "loss: 0.510466  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.736339, Avg loss: 1.000862 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.472661  [    0/ 8784]\n",
      "loss: 0.571978  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.743625, Avg loss: 0.997904 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.327925  [    0/ 8784]\n",
      "loss: 0.404242  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.741348, Avg loss: 1.001601 \n",
      "\n",
      "CPU times: total: 8.14 s\n",
      "Wall time: 8.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=gru_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.98      1.00      0.99       340\n",
      "     Chinese       0.81      0.66      0.72        38\n",
      "       Czech       0.53      0.28      0.37        96\n",
      "       Dutch       0.68      0.37      0.48        51\n",
      "     English       0.66      0.87      0.75       573\n",
      "      French       0.09      0.05      0.07        39\n",
      "      German       0.56      0.37      0.45       121\n",
      "       Greek       0.65      0.50      0.57        34\n",
      "       Irish       0.69      0.30      0.42        37\n",
      "     Italian       0.64      0.70      0.67       128\n",
      "    Japanese       0.83      0.86      0.84       156\n",
      "      Korean       0.27      0.40      0.32        10\n",
      "      Polish       0.79      0.42      0.55        26\n",
      "  Portuguese       0.00      0.00      0.00         9\n",
      "     Russian       0.84      0.84      0.84       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.44      0.30      0.36        50\n",
      "  Vietnamese       0.20      0.08      0.11        13\n",
      "\n",
      "    accuracy                           0.74      2196\n",
      "   macro avg       0.54      0.44      0.47      2196\n",
      "weighted avg       0.72      0.74      0.72      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(gru_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6YBam_3t-fO"
   },
   "source": [
    "1.3 Загрузите предобученные эмбеддинги (https://disk.yandex.ru/d/BHuT2tEXr_yBOQ?w=1) в модуль `nn.Embedding` и обучите модели из 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class SurnamesDecepticonRNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding: nn.Embedding,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hx, self.cx = None, None\n",
    "        self.rnn = rnn_cls(input_size=self.embedding.embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.hx, self.cx = None, None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if isinstance(self.rnn, (nn.RNN, nn.GRU)):\n",
    "            x, hx = self.rnn(x, self.hx)\n",
    "            self.hx = hx.detach()\n",
    "        else:\n",
    "            if self.hx is not None and self.cx is not None:\n",
    "                hx_cx = (self.hx, self.cx)\n",
    "            else:\n",
    "                hx_cx = None\n",
    "            x, (hx, cx) = self.rnn(x, hx_cx)\n",
    "            self.hx = hx.detach()\n",
    "            self.cx = cx.detach()\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(56, 50, padding_idx=0)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "embedding_weights = pd.read_csv(\n",
    "    DATA_DIR / \"glove.6B/glove.6B.50d.txt\",\n",
    "    sep=\" \",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    index_col=0,\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "weights = torch.ones(len(surnames_dataset.vocab), embedding_weights.shape[1], dtype=torch.float32)\n",
    "torch.nn.init.normal_(weights)\n",
    "\n",
    "for i, ch in enumerate(surnames_dataset.vocab.alphabet):\n",
    "    try:\n",
    "        weights[i] = torch.from_numpy(embedding_weights.loc[ch].to_numpy())\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(weights, padding_idx=0)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "rnn_net = SurnamesDecepticonRNNClassifier(\n",
    "    embedding=embedding,\n",
    "    rnn_cls=nn.RNN,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.962828  [    0/ 8784]\n",
      "loss: 2.089234  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.457650, Avg loss: 1.828808 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 2.027457  [    0/ 8784]\n",
      "loss: 1.705763  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.514572, Avg loss: 1.608777 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.491407  [    0/ 8784]\n",
      "loss: 1.658731  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.583333, Avg loss: 1.445631 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.733590  [    0/ 8784]\n",
      "loss: 1.394378  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.624772, Avg loss: 1.338745 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.283463  [    0/ 8784]\n",
      "loss: 1.377465  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.649362, Avg loss: 1.237860 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.059728  [    0/ 8784]\n",
      "loss: 1.211117  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.666667, Avg loss: 1.172205 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.949637  [    0/ 8784]\n",
      "loss: 0.980100  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.672131, Avg loss: 1.136335 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 1.133729  [    0/ 8784]\n",
      "loss: 1.104972  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.694444, Avg loss: 1.074095 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.957082  [    0/ 8784]\n",
      "loss: 0.927779  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.694444, Avg loss: 1.035266 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.919465  [    0/ 8784]\n",
      "loss: 1.018663  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.694900, Avg loss: 1.027094 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.795371  [    0/ 8784]\n",
      "loss: 0.941847  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.705373, Avg loss: 0.990295 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.786345  [    0/ 8784]\n",
      "loss: 1.045871  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.711749, Avg loss: 0.986539 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.814334  [    0/ 8784]\n",
      "loss: 0.728579  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.714481, Avg loss: 0.967964 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.740811  [    0/ 8784]\n",
      "loss: 0.758854  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.716302, Avg loss: 0.992922 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.721323  [    0/ 8784]\n",
      "loss: 0.581528  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.719945, Avg loss: 0.986993 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.941842  [    0/ 8784]\n",
      "loss: 0.821005  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.727231, Avg loss: 0.963472 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.707657  [    0/ 8784]\n",
      "loss: 0.532470  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728142, Avg loss: 0.958629 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.665380  [    0/ 8784]\n",
      "loss: 0.752147  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728597, Avg loss: 0.963359 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.736245  [    0/ 8784]\n",
      "loss: 0.705699  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.729508, Avg loss: 0.988401 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.592021  [    0/ 8784]\n",
      "loss: 0.663322  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.739982, Avg loss: 0.966545 \n",
      "\n",
      "CPU times: total: 7.61 s\n",
      "Wall time: 8.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.93      1.00      0.96       340\n",
      "     Chinese       0.54      0.84      0.66        38\n",
      "       Czech       0.62      0.16      0.25        96\n",
      "       Dutch       0.67      0.27      0.39        51\n",
      "     English       0.66      0.90      0.76       573\n",
      "      French       0.07      0.03      0.04        39\n",
      "      German       0.66      0.32      0.43       121\n",
      "       Greek       0.58      0.56      0.57        34\n",
      "       Irish       0.78      0.19      0.30        37\n",
      "     Italian       0.59      0.84      0.69       128\n",
      "    Japanese       0.81      0.81      0.81       156\n",
      "      Korean       0.40      0.20      0.27        10\n",
      "      Polish       0.57      0.31      0.40        26\n",
      "  Portuguese       1.00      0.11      0.20         9\n",
      "     Russian       0.87      0.84      0.85       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.58      0.28      0.38        50\n",
      "  Vietnamese       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.74      2196\n",
      "   macro avg       0.57      0.43      0.44      2196\n",
      "weighted avg       0.73      0.74      0.71      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(rnn_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "lstm_net = SurnamesDecepticonRNNClassifier(\n",
    "    embedding=embedding,\n",
    "    rnn_cls=nn.LSTM,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.906416  [    0/ 8784]\n",
      "loss: 2.187701  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.401184, Avg loss: 1.993795 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.975016  [    0/ 8784]\n",
      "loss: 1.826383  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.509563, Avg loss: 1.667807 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.756627  [    0/ 8784]\n",
      "loss: 1.681357  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.571038, Avg loss: 1.499368 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.506961  [    0/ 8784]\n",
      "loss: 1.350298  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.604281, Avg loss: 1.382028 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.546560  [    0/ 8784]\n",
      "loss: 1.390815  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.642987, Avg loss: 1.267897 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.314082  [    0/ 8784]\n",
      "loss: 1.232135  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.657559, Avg loss: 1.195764 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.177783  [    0/ 8784]\n",
      "loss: 0.847786  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.683060, Avg loss: 1.124817 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.799474  [    0/ 8784]\n",
      "loss: 1.165190  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.693078, Avg loss: 1.076553 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.842330  [    0/ 8784]\n",
      "loss: 0.836927  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.694444, Avg loss: 1.051924 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.995772  [    0/ 8784]\n",
      "loss: 0.815759  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.702186, Avg loss: 1.017822 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 1.007090  [    0/ 8784]\n",
      "loss: 0.873369  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.713115, Avg loss: 0.986735 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.783623  [    0/ 8784]\n",
      "loss: 0.768251  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.718579, Avg loss: 0.956234 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.800633  [    0/ 8784]\n",
      "loss: 0.724172  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.712659, Avg loss: 0.965949 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.767988  [    0/ 8784]\n",
      "loss: 0.741242  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.722222, Avg loss: 0.960968 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.568051  [    0/ 8784]\n",
      "loss: 0.823369  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728142, Avg loss: 0.957745 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.732256  [    0/ 8784]\n",
      "loss: 0.664876  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.734062, Avg loss: 0.934882 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.705313  [    0/ 8784]\n",
      "loss: 0.612533  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.731330, Avg loss: 0.931371 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.490744  [    0/ 8784]\n",
      "loss: 0.748959  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.724499, Avg loss: 0.929397 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.638272  [    0/ 8784]\n",
      "loss: 0.623626  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.737705, Avg loss: 0.918547 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.593918  [    0/ 8784]\n",
      "loss: 0.614802  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.741348, Avg loss: 0.924391 \n",
      "\n",
      "CPU times: total: 9.33 s\n",
      "Wall time: 9.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=lstm_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.96      1.00      0.98       340\n",
      "     Chinese       0.67      0.82      0.74        38\n",
      "       Czech       0.45      0.21      0.29        96\n",
      "       Dutch       0.74      0.33      0.46        51\n",
      "     English       0.65      0.91      0.76       573\n",
      "      French       0.20      0.03      0.05        39\n",
      "      German       0.53      0.36      0.43       121\n",
      "       Greek       0.58      0.44      0.50        34\n",
      "       Irish       0.86      0.16      0.27        37\n",
      "     Italian       0.66      0.74      0.70       128\n",
      "    Japanese       0.76      0.89      0.82       156\n",
      "      Korean       0.29      0.20      0.24        10\n",
      "      Polish       0.55      0.23      0.32        26\n",
      "  Portuguese       1.00      0.11      0.20         9\n",
      "     Russian       0.86      0.83      0.84       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.59      0.20      0.30        50\n",
      "  Vietnamese       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.74      2196\n",
      "   macro avg       0.57      0.41      0.44      2196\n",
      "weighted avg       0.72      0.74      0.71      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(lstm_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.GRU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "gru_net = SurnamesDecepticonRNNClassifier(\n",
    "    embedding=embedding,\n",
    "    rnn_cls=nn.GRU,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.878209  [    0/ 8784]\n",
      "loss: 1.986637  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.458561, Avg loss: 1.834729 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.671564  [    0/ 8784]\n",
      "loss: 1.591380  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.542350, Avg loss: 1.581688 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.571778  [    0/ 8784]\n",
      "loss: 1.334489  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.597905, Avg loss: 1.412697 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.261335  [    0/ 8784]\n",
      "loss: 1.502182  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.624317, Avg loss: 1.304546 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.172264  [    0/ 8784]\n",
      "loss: 1.117940  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.643443, Avg loss: 1.241262 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.310606  [    0/ 8784]\n",
      "loss: 1.070718  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.663024, Avg loss: 1.178665 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 1.219958  [    0/ 8784]\n",
      "loss: 1.070196  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.688525, Avg loss: 1.107373 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.936526  [    0/ 8784]\n",
      "loss: 1.072124  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.690346, Avg loss: 1.092534 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.801502  [    0/ 8784]\n",
      "loss: 0.856269  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.698998, Avg loss: 1.051424 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.725636  [    0/ 8784]\n",
      "loss: 0.794493  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.708106, Avg loss: 1.025588 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.779037  [    0/ 8784]\n",
      "loss: 0.867163  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.706284, Avg loss: 0.993129 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.887834  [    0/ 8784]\n",
      "loss: 0.812461  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.717213, Avg loss: 0.973030 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.763634  [    0/ 8784]\n",
      "loss: 0.822854  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728142, Avg loss: 0.962304 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.723115  [    0/ 8784]\n",
      "loss: 0.784506  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.726321, Avg loss: 0.959522 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.586038  [    0/ 8784]\n",
      "loss: 0.801427  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.722222, Avg loss: 0.982552 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.818922  [    0/ 8784]\n",
      "loss: 0.774694  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.723133, Avg loss: 0.985262 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.625815  [    0/ 8784]\n",
      "loss: 0.618605  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.725865, Avg loss: 0.981119 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.731664  [    0/ 8784]\n",
      "loss: 0.727750  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.735883, Avg loss: 0.932908 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.482618  [    0/ 8784]\n",
      "loss: 0.727197  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728142, Avg loss: 0.972024 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.616392  [    0/ 8784]\n",
      "loss: 0.709586  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.738616, Avg loss: 0.957200 \n",
      "\n",
      "CPU times: total: 7.91 s\n",
      "Wall time: 8.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=gru_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.93      1.00      0.96       340\n",
      "     Chinese       0.71      0.76      0.73        38\n",
      "       Czech       0.51      0.23      0.32        96\n",
      "       Dutch       0.71      0.29      0.42        51\n",
      "     English       0.65      0.90      0.76       573\n",
      "      French       0.10      0.03      0.04        39\n",
      "      German       0.56      0.40      0.47       121\n",
      "       Greek       0.62      0.44      0.52        34\n",
      "       Irish       0.75      0.32      0.45        37\n",
      "     Italian       0.67      0.70      0.68       128\n",
      "    Japanese       0.84      0.82      0.83       156\n",
      "      Korean       0.25      0.20      0.22        10\n",
      "      Polish       0.40      0.15      0.22        26\n",
      "  Portuguese       1.00      0.11      0.20         9\n",
      "     Russian       0.83      0.84      0.84       458\n",
      "    Scottish       1.00      0.00      0.00        17\n",
      "     Spanish       0.44      0.28      0.34        50\n",
      "  Vietnamese       0.50      0.15      0.24        13\n",
      "\n",
      "    accuracy                           0.74      2196\n",
      "   macro avg       0.64      0.42      0.46      2196\n",
      "weighted avg       0.72      0.74      0.71      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(gru_net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7kf990U9Do-"
   },
   "source": [
    "## 2. Классификация новостей на основе заголовка\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/FN-EgWGIpyjLxQ?w=1\n",
    "\n",
    "Эмбеддинги: https://nlp.stanford.edu/projects/glove/ (находите ссылку на архив\n",
    "glove.6B.zip, в нем несколько файлов с эмбеддингами слов, выбираете один из файлов в\n",
    "архиве)\n",
    "\n",
    "2.1 Загрузите набор данных train.csv. Выполните предобработку столбца Title\n",
    "\n",
    "2.2 На основе этих данных создайте датасет NewsDataset . Не забудьте добавить\n",
    "специальные токены `<PAD>` для дополнения последовательностей до нужной длины и\n",
    "`<UNK>` для корректной обработке ранее не встречавшихся токенов. В данной задаче\n",
    "рассматривайте отдельные слова как токены. Разбейте датасет на обучающее и\n",
    "валидационное множество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "PATTERN = re.compile(r\"[^a-z]\", flags=re.MULTILINE)\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def simple_preprocess_news_title(title: str) -> str:\n",
    "    return title.lower()\n",
    "\n",
    "\n",
    "def complex_preprocess_news_title(\n",
    "        title: str,\n",
    "        lemmatizer_or_stemmer: t.Callable[[str], str] = None,\n",
    "        min_word_len: int = 0,\n",
    ") -> str:\n",
    "    title = simple_preprocess_news_title(title)\n",
    "    title = PATTERN.sub(\" \", title)\n",
    "\n",
    "    words = []\n",
    "    for word in nltk.word_tokenize(title):\n",
    "        if word not in STOPWORDS and len(word) >= min_word_len:\n",
    "            if not lemmatizer_or_stemmer:\n",
    "                words.append(word)\n",
    "                continue\n",
    "            word = lemmatizer_or_stemmer(word)\n",
    "            if word not in STOPWORDS and len(word) >= min_word_len:\n",
    "                words.append(word)\n",
    "\n",
    "    return \" \".join(words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def get_pos(word: str) -> str:\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "_wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def wordnet_lemmatizer(token: str) -> str:\n",
    "    return _wordnet_lemmatizer.lemmatize(token, pos=get_pos(token))\n",
    "\n",
    "\n",
    "_snowball_stemmer = nltk.SnowballStemmer(language=\"english\")\n",
    "\n",
    "\n",
    "def snowball_stemmer(token: str) -> str:\n",
    "    return _snowball_stemmer.stem(token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class NewsVocab:\n",
    "    pad = \"<PAD>\"\n",
    "    unknown = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, news_titles: t.List[str], max_len: int = 0):\n",
    "        uniques = set()\n",
    "        for title in news_titles:\n",
    "            words = nltk.word_tokenize(title)\n",
    "            uniques.update(words)\n",
    "            max_len = max(len(words), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, self.unknown, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        w2i = {w: i for i, w in enumerate(self.alphabet)}\n",
    "        unknown_idx = w2i[self.unknown]\n",
    "        self.w2i = defaultdict(lambda: unknown_idx, w2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, review: str) -> torch.Tensor:\n",
    "        indices = [self.w2i[w] for w in nltk.word_tokenize(review)]\n",
    "        indices += [self.w2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.w2i[self.pad], as_tuple=True)[0]  # noqa\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \" \".join(self.alphabet[i] for i in indices)\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    df: pd.DataFrame\n",
    "    titles: t.List[str]\n",
    "    classes: t.List[int]\n",
    "    vocab: NewsVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path, preprocess: t.Callable[[str], str], title_max_len: int = 0):\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "        self.titles = self.df[\"Title\"].apply(preprocess).tolist()\n",
    "        self.vocab = NewsVocab(self.titles, max_len=title_max_len)\n",
    "\n",
    "        self.data = torch.vstack([self.vocab.encode(w.lower()) for w in self.titles])\n",
    "        self.targets = torch.tensor(self.df[\"Class Index\"], dtype=torch.long) - 1\n",
    "        self.classes = self.targets.unique().tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    def encode(self, title: str) -> torch.Tensor:\n",
    "        return self.vocab.encode(title)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "(120000, 7600)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_news_title(title: str) -> str:\n",
    "    return complex_preprocess_news_title(title, lemmatizer_or_stemmer=snowball_stemmer, min_word_len=3)\n",
    "\n",
    "\n",
    "train_news_dataset = NewsDataset(\n",
    "    DATA_DIR / \"news/train.csv\",\n",
    "    preprocess_news_title,\n",
    ")\n",
    "test_news_dataset = NewsDataset(\n",
    "    DATA_DIR / \"news/test.csv\",\n",
    "    preprocess_news_title,\n",
    "    title_max_len=train_news_dataset.vocab.max_len,\n",
    ")\n",
    "len(train_news_dataset), len(test_news_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "train_news_dataloader = DataLoader(train_news_dataset, batch_size=256, shuffle=True)\n",
    "test_news_dataloader = DataLoader(test_news_dataset, batch_size=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3 Создайте модель для классификации, используя слой nn.Embedding и слой nn.RNN.\n",
    "эмбеддинги инициализируйте случайным образом не забудьте указать аргумент padding_idx для nn.Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding: nn.Embedding,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hx, self.cx = None, None\n",
    "        self.rnn = rnn_cls(input_size=self.embedding.embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.hx, self.cx = None, None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if isinstance(self.rnn, (nn.RNN, nn.GRU)):\n",
    "            x, hx = self.rnn(x, self.hx)\n",
    "            self.hx = hx.detach()\n",
    "        else:\n",
    "            if self.hx is not None and self.cx is not None:\n",
    "                hx_cx = (self.hx, self.cx)\n",
    "            else:\n",
    "                hx_cx = None\n",
    "            x, (hx, cx) = self.rnn(x, hx_cx)\n",
    "            self.hx = hx.detach()\n",
    "            self.cx = cx.detach()\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "news_rnn_net = NewsClassifier(\n",
    "    embedding=nn.Embedding(num_embeddings=len(train_news_dataset.vocab), embedding_dim=64, padding_idx=0),\n",
    "    rnn_cls=nn.RNN,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=train_news_dataset.vocab.max_len,\n",
    "    num_classes=len(train_news_dataset.classes),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_rnn_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 1.358896  [    0/120000]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.21 GiB already allocated; 0 bytes free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:1\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mcommon_train\u001B[1;34m(model, loss_fn, optimizer, train_dataloader, epochs, test_dataloader, lr_scheduler, verbose, device)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m test_dataloader:\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(dataloader, model, loss_fn, optimizer, verbose, device)\u001B[0m\n\u001B[0;32m     54\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, y)\n\u001B[0;32m     56\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 57\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     60\u001B[0m avg_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n",
      "File \u001B[1;32mD:\\IT\\Coding\\GitHub Projects\\My-University\\py-venvs\\ml-venv\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\IT\\Coding\\GitHub Projects\\My-University\\py-venvs\\ml-venv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.21 GiB already allocated; 0 bytes free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=5,\n",
    "    model=news_rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_news_dataloader,\n",
    "    test_dataloader=test_news_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4 Переобучите модель, заменив слой nn.RNN на nn.LSTM и nn.GRU . Сравните качество\n",
    "на тестовой выборке. Результаты сведите в таблицу (модель/метрика качества на тестовом множестве)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "news_lstm_net = NewsClassifier(\n",
    "    embedding=nn.Embedding(num_embeddings=len(train_news_dataset.vocab), embedding_dim=64, padding_idx=0),\n",
    "    rnn_cls=nn.LSTM,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=train_news_dataset.vocab.max_len,\n",
    "    num_classes=len(train_news_dataset.classes),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_lstm_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=5,\n",
    "    model=news_lstm_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_news_dataloader,\n",
    "    test_dataloader=test_news_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "news_gru_net = NewsClassifier(\n",
    "    embedding=nn.Embedding(num_embeddings=len(train_news_dataset.vocab), embedding_dim=64, padding_idx=0),\n",
    "    rnn_cls=nn.GRU,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=train_news_dataset.vocab.max_len,\n",
    "    num_classes=len(train_news_dataset.classes),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_gru_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=5,\n",
    "    model=news_gru_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_news_dataloader,\n",
    "    test_dataloader=test_news_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5 Выполните пункты 2.3 и 2.4, используя предобученные эмбеддинги Glove.\n",
    "Прокомментируйте результат.\n",
    "Эмбеддинги из скачанного файла загрузите в виде двумерного тензора pretrained_embeddings.\n",
    "Обратите внимание, что номер строки в этом тензоре должен соответствовать\n",
    "токену (слову), имеющему такой индекс в вашем словаре.\n",
    "для слов, которых нет в файле с эмбеддингами, инициализуйте эмбеддинг\n",
    "случайным образом"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/21963 [00:00<05:05, 71.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 8024/21963 [01:10<01:56, 119.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20012/21963 [03:05<00:22, 85.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21963/21963 [03:25<00:00, 107.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(21963, 100, padding_idx=0)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "embedding_weights = pd.read_csv(\n",
    "    DATA_DIR / \"glove.6B/glove.6B.100d.txt\",\n",
    "    sep=\" \",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    index_col=0,\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "weights = torch.empty(len(train_news_dataset.vocab), embedding_weights.shape[1], dtype=torch.float32)\n",
    "torch.nn.init.normal_(weights)\n",
    "\n",
    "for i, w in tqdm(enumerate(train_news_dataset.vocab.alphabet), total=len(train_news_dataset.vocab.alphabet)):\n",
    "    try:\n",
    "        weights[i] = torch.from_numpy(embedding_weights.loc[w].to_numpy())\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(weights, padding_idx=0)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "news_rnn_pretrained_net = NewsClassifier(\n",
    "    embedding=embedding,\n",
    "    rnn_cls=nn.RNN,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=train_news_dataset.vocab.max_len,\n",
    "    num_classes=len(train_news_dataset.classes),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_rnn_pretrained_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 1.456971  [    0/120000]\n",
      "loss: 1.387290  [64000/120000]\n",
      "Test Error: \n",
      " Accuracy: 0.262895, Avg loss: 1.385705 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.369181  [    0/120000]\n",
      "loss: 1.341796  [64000/120000]\n",
      "Test Error: \n",
      " Accuracy: 0.271711, Avg loss: 1.384669 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.349925  [    0/120000]\n",
      "loss: 1.319169  [64000/120000]\n",
      "Test Error: \n",
      " Accuracy: 0.266316, Avg loss: 1.386917 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.322543  [    0/120000]\n",
      "loss: 1.282620  [64000/120000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=5,\n",
    "    model=news_rnn_pretrained_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_news_dataloader,\n",
    "    test_dataloader=test_news_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "news_lstm_pretrained_net = NewsClassifier(\n",
    "    embedding=embedding,\n",
    "    rnn_cls=nn.LSTM,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=train_news_dataset.vocab.max_len,\n",
    "    num_classes=len(train_news_dataset.classes),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_lstm_pretrained_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=5,\n",
    "    model=news_lstm_pretrained_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_news_dataloader,\n",
    "    test_dataloader=test_news_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "news_gru_pretrained_net = NewsClassifier(\n",
    "    embedding=embedding,\n",
    "    rnn_cls=nn.GRU,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=train_news_dataset.vocab.max_len,\n",
    "    num_classes=len(train_news_dataset.classes),\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_gru_pretrained_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=5,\n",
    "    model=news_gru_pretrained_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_news_dataloader,\n",
    "    test_dataloader=test_news_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Результаты сведите в таблицу (модель/метрика качества на тестовом множестве)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_pivot_table(\n",
    "        models: t.List[t.Tuple[str, NewsClassifier]],\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> pd.DataFrame:\n",
    "    general_report = {}\n",
    "    for model in models:\n",
    "        report = {}\n",
    "        y_test, y_pred = get_y_test_y_pred(model, test_dataloader, device)\n",
    "        ms = metrics.classification_report(y_test, y_pred, zero_division=True, output_dict=True)\n",
    "        report[\"accuracy\"] = ms[\"accuracy\"]\n",
    "        report[\"precision (w avg)\"] = ms[\"weighted avg\"][\"precision\"]\n",
    "        report[\"recall (w avg)\"] = ms[\"weighted avg\"][\"recall\"]\n",
    "        report[\"f1-score (w avg)\"] = ms[\"weighted avg\"][\"f1-score\"]\n",
    "        general_report[model.rnn.__class__.__name__] = report\n",
    "    return pd.DataFrame(general_report)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pivot_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [88]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m report \u001B[38;5;241m=\u001B[39m \u001B[43mmake_pivot_table\u001B[49m(\n\u001B[0;32m      2\u001B[0m     [\n\u001B[0;32m      3\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRNN\u001B[39m\u001B[38;5;124m\"\u001B[39m, news_rnn_net),\n\u001B[0;32m      4\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLSTM\u001B[39m\u001B[38;5;124m\"\u001B[39m, news_lstm_net),\n\u001B[0;32m      5\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGRU\u001B[39m\u001B[38;5;124m\"\u001B[39m, news_gru_net),\n\u001B[0;32m      6\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRNN (pretrained)\u001B[39m\u001B[38;5;124m\"\u001B[39m, news_rnn_pretrained_net),\n\u001B[0;32m      7\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLSTM (pretrained)\u001B[39m\u001B[38;5;124m\"\u001B[39m, news_lstm_pretrained_net),\n\u001B[0;32m      8\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGRU (pretrained)\u001B[39m\u001B[38;5;124m\"\u001B[39m, news_gru_pretrained_net),\n\u001B[0;32m      9\u001B[0m     ],\n\u001B[0;32m     10\u001B[0m     test_news_dataloader,\n\u001B[0;32m     11\u001B[0m     DEVICE,\n\u001B[0;32m     12\u001B[0m )\n\u001B[0;32m     13\u001B[0m report\n",
      "\u001B[1;31mNameError\u001B[0m: name 'make_pivot_table' is not defined"
     ]
    }
   ],
   "source": [
    "report = make_pivot_table(\n",
    "    [\n",
    "        (\"RNN\", news_rnn_net),\n",
    "        (\"LSTM\", news_lstm_net),\n",
    "        (\"GRU\", news_gru_net),\n",
    "        (\"RNN (pretrained)\", news_rnn_pretrained_net),\n",
    "        (\"LSTM (pretrained)\", news_lstm_pretrained_net),\n",
    "        (\"GRU (pretrained)\", news_gru_pretrained_net),\n",
    "    ],\n",
    "    test_news_dataloader,\n",
    "    DEVICE,\n",
    ")\n",
    "report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report.plot.bar();"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPopRjm9A6la5QJRG/PWjfN",
   "collapsed_sections": [],
   "name": "blank__07_rnn_1_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
