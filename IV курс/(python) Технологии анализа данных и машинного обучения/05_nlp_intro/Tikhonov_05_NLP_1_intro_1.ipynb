{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XtFQP3RNll3c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7026,
     "status": "ok",
     "timestamp": 1618423490594,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "443CZCLp0_sE",
    "outputId": "ddda53b2-a593-4d13-ee48-cfed23e613b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\super\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqDHq_AEjRZ1"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaki7efDpmXo"
   },
   "source": [
    "1.1 Операции по предобработке:\n",
    "* токенизация\n",
    "* стемминг / лемматизация\n",
    "* удаление стоп-слов\n",
    "* удаление пунктуации\n",
    "* приведение к нижнему регистру\n",
    "* любые другие операции над текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nHRy4jpYphEr"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lMMzGhq0ikz1"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUhfertRtXE5"
   },
   "source": [
    "Реализовать функцию `preprocess_text(text: str)`, которая:\n",
    "* приводит строку к нижнему регистру\n",
    "* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "pat = re.compile(r\"[^a-z.!?]\", flags=re.MULTILINE)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = set(\".!?\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # нижний регистр и только разделители предложений\n",
    "    text = pat.sub(\" \", text.lower())\n",
    "\n",
    "    sentences = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        words = []\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            # не стоп слово и не разделитель предложение\n",
    "            if word not in stop_words and word not in punctuation:\n",
    "                stemma = stemmer.stem(word)\n",
    "                # отбросить слишком короткие стеммы - положительно сказывается на точности модели\n",
    "                if stemma not in stop_words and len(stemma) > 2:\n",
    "                    words.append(stemma)\n",
    "        sentences.append(\" \".join(words))\n",
    "\n",
    "    # что бы сохранить возможность разбивать на предложения достаточно \".\"\n",
    "    text = \". \".join(sentences)\n",
    "    # на всякий случай\n",
    "    text = re.sub(r\"[!?]\", \".\", text, flags=re.MULTILINE)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'select prefer run instal command. stabl repres current test support version pytorch. note libtorch avail'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Dt1ssIqckC"
   },
   "source": [
    "1.2 Представление текстовых данных при помощи бинарного кодирования\n",
    "\n",
    "\n",
    "Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ = preprocess_text(text)\n",
    "\n",
    "vocab = set(nltk.word_tokenize(text_))\n",
    "vocab.remove(\".\")\n",
    "w2i = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "sent = nltk.sent_tokenize(text_)[0]\n",
    "sent = sent.replace(\".\", \"\")\n",
    "indices = [w2i[w] for w in nltk.word_tokenize(sent)]\n",
    "vector = torch.zeros(len(vocab))\n",
    "vector[indices] = 1\n",
    "vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2Nz_zcgw3N4"
   },
   "source": [
    "## 2. Классификация фамилий по национальности\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French\n2      Kore     English\n3     Koury      Arabic\n4    Lebzak     Russian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/surnames.csv\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "    surname nationality  target\n0  Woodford     English       4\n1      Coté      French       5\n2      Kore     English       4\n3     Koury      Arabic       0\n4    Lebzak     Russian      14",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "df[\"target\"] = enc.fit_transform(df[\"nationality\"])\n",
    "print(f\"classes: {len(enc.classes_)}\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        self.idx_to_token = {i: t for i, t in enumerate({ch for w in data for ch in w.lower()})}\n",
    "        self.token_to_idx = {t: i for i, t in self.idx_to_token.items()}\n",
    "        self.vocab_len = len(self.idx_to_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "55"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(df[\"surname\"])\n",
    "vocab.vocab_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab: Vocab):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)\"\"\"\n",
    "        v = torch.zeros(self.vocab.vocab_len)\n",
    "        v[[self.vocab.token_to_idx[ch] for ch in surname.lower()]] = 1\n",
    "        return v\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vectorize(self.X[idx]), self.y[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "10980"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SurnamesDataset(\n",
    "    X=df[\"surname\"].tolist(),\n",
    "    y=torch.tensor(df[\"target\"], dtype=torch.long),\n",
    "    vocab=vocab,\n",
    ")\n",
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(8784, 2196)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = round(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "len(train_dataset), len(test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super(SurnamesClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(300, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.classifier(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model = SurnamesClassifier(vocab.vocab_len, vocab.vocab_len)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=4.019810\n",
      "[ 2400/ 8784] loss=2.142408\n",
      "[ 4800/ 8784] loss=2.530181\n",
      "[ 7200/ 8784] loss=1.795364\n",
      "Validate: accuracy=0.525046, avg_loss=1.554981\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.302734\n",
      "[ 2400/ 8784] loss=1.052091\n",
      "[ 4800/ 8784] loss=1.559641\n",
      "[ 7200/ 8784] loss=0.535729\n",
      "Validate: accuracy=0.553734, avg_loss=1.451229\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.500295\n",
      "[ 2400/ 8784] loss=1.125522\n",
      "[ 4800/ 8784] loss=1.585158\n",
      "[ 7200/ 8784] loss=1.483449\n",
      "Validate: accuracy=0.571949, avg_loss=1.381282\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=2.311538\n",
      "[ 2400/ 8784] loss=1.647652\n",
      "[ 4800/ 8784] loss=1.132809\n",
      "[ 7200/ 8784] loss=1.630430\n",
      "Validate: accuracy=0.583333, avg_loss=1.338217\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.176329\n",
      "[ 2400/ 8784] loss=2.259466\n",
      "[ 4800/ 8784] loss=0.986127\n",
      "[ 7200/ 8784] loss=1.145083\n",
      "Validate: accuracy=0.588798, avg_loss=1.320260\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.412281\n",
      "[ 2400/ 8784] loss=0.960923\n",
      "[ 4800/ 8784] loss=1.341316\n",
      "[ 7200/ 8784] loss=1.406827\n",
      "Validate: accuracy=0.605647, avg_loss=1.276944\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=0.726800\n",
      "[ 2400/ 8784] loss=1.400991\n",
      "[ 4800/ 8784] loss=0.775602\n",
      "[ 7200/ 8784] loss=2.128188\n",
      "Validate: accuracy=0.607468, avg_loss=1.258456\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=0.644953\n",
      "[ 2400/ 8784] loss=0.436922\n",
      "[ 4800/ 8784] loss=1.378889\n",
      "[ 7200/ 8784] loss=1.805901\n",
      "Validate: accuracy=0.616120, avg_loss=1.248776\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=0.554993\n",
      "[ 2400/ 8784] loss=1.453959\n",
      "[ 4800/ 8784] loss=0.914179\n",
      "[ 7200/ 8784] loss=0.899557\n",
      "Validate: accuracy=0.620219, avg_loss=1.238630\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.361069\n",
      "[ 2400/ 8784] loss=0.825714\n",
      "[ 4800/ 8784] loss=1.173997\n",
      "[ 7200/ 8784] loss=1.295552\n",
      "Validate: accuracy=0.620674, avg_loss=1.232302\n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.614185\n",
      "[ 2400/ 8784] loss=1.322818\n",
      "[ 4800/ 8784] loss=1.382327\n",
      "[ 7200/ 8784] loss=0.432412\n",
      "Validate: accuracy=0.617486, avg_loss=1.231755\n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=0.589037\n",
      "[ 2400/ 8784] loss=0.952885\n",
      "[ 4800/ 8784] loss=1.031723\n",
      "[ 7200/ 8784] loss=0.750241\n",
      "Validate: accuracy=0.627505, avg_loss=1.206069\n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.259615\n",
      "[ 2400/ 8784] loss=0.897233\n",
      "[ 4800/ 8784] loss=1.426218\n",
      "[ 7200/ 8784] loss=0.748086\n",
      "Validate: accuracy=0.626594, avg_loss=1.195330\n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=2.070745\n",
      "[ 2400/ 8784] loss=1.239500\n",
      "[ 4800/ 8784] loss=1.797934\n",
      "[ 7200/ 8784] loss=0.993617\n",
      "Validate: accuracy=0.627049, avg_loss=1.186374\n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "[    0/ 8784] loss=1.275664\n",
      "[ 2400/ 8784] loss=1.309713\n",
      "[ 4800/ 8784] loss=1.145484\n",
      "[ 7200/ 8784] loss=1.293550\n",
      "Validate: accuracy=0.622495, avg_loss=1.179533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "\n",
    "    model.train()\n",
    "    size = len(train_dataloader.dataset)\n",
    "    num_batches = len(train_dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"[{batch * len(x):>5d}/{size:>5d}] loss={loss:6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in test_dataloader:\n",
    "        pred = model(x)\n",
    "        avg_loss += criterion(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Validate: accuracy={accuracy:5f}, avg_loss={avg_loss:5f}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.80      1.00      0.89       325\n",
      "     Chinese       0.43      0.44      0.43        45\n",
      "       Czech       0.60      0.10      0.17        88\n",
      "       Dutch       0.00      0.00      0.00        39\n",
      "     English       0.52      0.82      0.63       581\n",
      "      French       0.25      0.02      0.04        50\n",
      "      German       0.61      0.22      0.33       122\n",
      "       Greek       0.48      0.24      0.32        42\n",
      "       Irish       0.67      0.29      0.41        41\n",
      "     Italian       0.45      0.31      0.37       126\n",
      "    Japanese       0.62      0.54      0.58       159\n",
      "      Korean       0.50      0.17      0.25        12\n",
      "      Polish       0.82      0.41      0.55        22\n",
      "  Portuguese       1.00      0.00      0.00        13\n",
      "     Russian       0.75      0.76      0.76       448\n",
      "    Scottish       1.00      0.00      0.00        19\n",
      "     Spanish       0.43      0.20      0.28        44\n",
      "  Vietnamese       1.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.62      2196\n",
      "   macro avg       0.61      0.31      0.33      2196\n",
      "weighted avg       0.62      0.62      0.58      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_test, y_pred = [], []\n",
    "for x, y in test_dataloader:\n",
    "    pred = model(x).argmax(1)\n",
    "    y_test.append(y)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "y_test, y_pred = torch.hstack(y_test), torch.hstack(y_pred)\n",
    "print(classification_report(y_test, y_pred, target_names=enc.classes_, zero_division=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kUkSZkDqxNYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konovalova   -   Russian (0.81), Czech (0.15), English (0.03)\n",
      "Tikhonov     -   Russian (0.98), Czech (0.01), English (0.01)\n",
      "Bilyukin     -   Russian (0.96), English (0.03), Czech (0.01)\n",
      "Titov        -   Russian (0.58), Italian (0.15), English (0.15)\n",
      "Ratoshnyuk   -   Russian (0.94), Greek (0.03), Japanese (0.01)\n",
      "Voronina     -   Italian (0.47), Russian (0.26), English (0.11)\n",
      "Petrov       -   Russian (0.44), English (0.33), French (0.07)\n",
      "Kamenchuk    -   Russian (0.44), Czech (0.18), German (0.10)\n",
      "Katamadze    -   Russian (0.35), Polish (0.23), Czech (0.19)\n"
     ]
    }
   ],
   "source": [
    "students = [\n",
    "    \"Konovalova\",\n",
    "    \"Tikhonov\",\n",
    "    \"Bilyukin\",\n",
    "    \"Titov\",\n",
    "    \"Ratoshnyuk\",\n",
    "    \"Voronina\",\n",
    "    \"Petrov\",\n",
    "    \"Kamenchuk\",\n",
    "    \"Katamadze\",\n",
    "]\n",
    "for surname in students:\n",
    "    x = dataset.vectorize(surname)\n",
    "    pred = model(x.unsqueeze(0))  # батч из одного элемента\n",
    "    pred_proba, pred_label_indices = torch.softmax(pred, dim=1).topk(3, dim=1)\n",
    "    pred_labels = enc.inverse_transform(pred_label_indices.squeeze())\n",
    "\n",
    "    predicts = \", \".join([\n",
    "        f\"{label} ({prob:.2f})\"\n",
    "        for (label, prob) in zip(pred_labels, pred_proba.squeeze())\n",
    "    ])\n",
    "    print(f\"{surname:<10}   -   {predicts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLmDB3fJtVox"
   },
   "source": [
    "## 3. Классификация обзоров ресторанов\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n",
    "\n",
    "3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n",
    "\n",
    "3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n",
    "\n",
    "3.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "3.4 Реализовать класс `Vocab` (токен = слово)\n",
    "\n",
    "3.5 Реализовать класс `ReviewDataset`\n",
    "\n",
    "3.6 Обучить классификатор\n",
    "\n",
    "3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560000\n",
      "56000\n"
     ]
    },
    {
     "data": {
      "text/plain": "   0                                                  1\n0  2  Call me crazy, but I really enjoyed this place...\n1  1  HORRIBLE, HORRIBLE, HORRIBLE ONLINE CUSTOMER S...\n2  2  The staff were extremely helpful in answering ...\n3  2  The steak tartare is fantastic!  I'd come back...\n4  2                    Everything I could want for $3.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Call me crazy, but I really enjoyed this place...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>HORRIBLE, HORRIBLE, HORRIBLE ONLINE CUSTOMER S...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>The staff were extremely helpful in answering ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>The steak tartare is fantastic!  I'd come back...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Everything I could want for $3.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/yelp/raw_train.csv\", header=None)\n",
    "print(len(df))\n",
    "df = df.sample(frac=0.1, ignore_index=True, random_state=0)\n",
    "print(len(df))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "   0                                                  1\n0  1  call crazi realli enjoy place get ton food dec...\n1  0  horribl horribl horribl onlin custom servic or...\n2  1  staff extrem help answer question took time ma...\n3  1  steak tartar fantast come back vega dine great...\n4  1                                 everyth could want",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>call crazi realli enjoy place get ton food dec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>horribl horribl horribl onlin custom servic or...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>staff extrem help answer question took time ma...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>steak tartar fantast come back vega dine great...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>everyth could want</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "\n",
    "df[0] = enc.fit_transform(df[0])\n",
    "df[1] = df[1].apply(lambda t: preprocess_text(t).replace(\".\", \"\"))\n",
    "print(f\"classes: {len(enc.classes_)}\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        unique = {w for text in data for w in nltk.word_tokenize(text)}\n",
    "        if \".\" in unique:\n",
    "            unique.remove(\".\")\n",
    "        self.idx_to_token = list(unique)\n",
    "\n",
    "        self.token_to_idx = {t: i for i, t in enumerate(self.idx_to_token)}\n",
    "\n",
    "        self.vocab_len = len(self.idx_to_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "50659"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(df[1])\n",
    "vocab.vocab_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab: Vocab):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\"Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)\"\"\"\n",
    "        vec = torch.zeros(self.vocab.vocab_len)\n",
    "        indices = [self.vocab.token_to_idx[w] for w in nltk.word_tokenize(review)]\n",
    "        vec[indices] = 1\n",
    "        return vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vectorize(self.X[idx]), self.y[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "56000"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ReviewDataset(\n",
    "    X=df[1].tolist(),\n",
    "    y=torch.tensor(df[0], dtype=torch.long),\n",
    "    vocab=vocab,\n",
    ")\n",
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(44800, 11200)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = round(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "len(train_dataset), len(test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_lCTSKZgu68K"
   },
   "outputs": [],
   "source": [
    "class ReviewsClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super(ReviewsClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(200, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.classifier(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "model = ReviewsClassifier(vocab.vocab_len, vocab.vocab_len)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "[    0/44800] loss=10.858390\n",
      "[12800/44800] loss=0.655565\n",
      "[25600/44800] loss=0.301907\n",
      "[38400/44800] loss=0.200185\n",
      "Validate: accuracy=0.903036, avg_loss=0.283251\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "[    0/44800] loss=0.310345\n",
      "[12800/44800] loss=0.296770\n",
      "[25600/44800] loss=0.203643\n",
      "[38400/44800] loss=0.188113\n",
      "Validate: accuracy=0.909554, avg_loss=0.241475\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "[    0/44800] loss=0.094054\n",
      "[12800/44800] loss=0.248649\n",
      "[25600/44800] loss=0.089277\n",
      "[38400/44800] loss=0.071432\n",
      "Validate: accuracy=0.911339, avg_loss=0.236651\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "[    0/44800] loss=0.106096\n",
      "[12800/44800] loss=0.118433\n",
      "[25600/44800] loss=0.151436\n",
      "[38400/44800] loss=0.115770\n",
      "Validate: accuracy=0.907679, avg_loss=0.241892\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "[    0/44800] loss=0.071021\n",
      "[12800/44800] loss=0.141998\n",
      "[25600/44800] loss=0.080506\n",
      "[38400/44800] loss=0.093734\n",
      "Validate: accuracy=0.906518, avg_loss=0.256744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "\n",
    "    model.train()\n",
    "    size = len(train_dataloader.dataset)\n",
    "    num_batches = len(train_dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % 200 == 0:\n",
    "            print(f\"[{batch * len(x):>5d}/{size:>5d}] loss={loss:6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "        avg_loss, correct = 0, 0\n",
    "\n",
    "        for x, y in test_dataloader:\n",
    "            pred = model(x)\n",
    "            avg_loss += criterion(pred, y)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        avg_loss /= num_batches\n",
    "        accuracy = correct / size\n",
    "        print(f\"Validate: accuracy={accuracy:5f}, avg_loss={avg_loss:5f}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WXLmCDvcvRmb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      5622\n",
      "           1       0.91      0.90      0.91      5578\n",
      "\n",
      "    accuracy                           0.91     11200\n",
      "   macro avg       0.91      0.91      0.91     11200\n",
      "weighted avg       0.91      0.91      0.91     11200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_test, y_pred = [], []\n",
    "for x, y in test_dataloader:\n",
    "    pred = model(x).argmax(1)\n",
    "    y_test.append(y)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "y_test, y_pred = torch.hstack(y_test), torch.hstack(y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad  | bad  (0.99) | This is the most terrible place I've ever been to. Untidy and sloppy staff. Low-quality products.\n",
      "\n",
      "good | good (0.97) | A wonderful place. I will recommend it to my relatives and friends.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    (\"This is the most terrible place I've ever been to. Untidy and sloppy staff. Low-quality products.\", 1),\n",
    "    (\"A wonderful place. I will recommend it to my relatives and friends.\", 2),\n",
    "]\n",
    "\n",
    "translate = {1: \"bad\", 2: \"good\"}\n",
    "\n",
    "for review, target in reviews:\n",
    "    x = dataset.vectorize(preprocess_text(review).replace(\".\", \"\"))\n",
    "\n",
    "    pred = model(x.unsqueeze(0))  # батч из одного элемента\n",
    "    pred_proba, pred_label_idx = torch.softmax(pred, 1).max(dim=1)\n",
    "    pred_label = enc.inverse_transform([pred_label_idx.item()])\n",
    "\n",
    "    print(\n",
    "        f\"{translate[target]:^4} | {translate[pred_label.item()]:^4} ({pred_proba.item():.2f}) | {review:<60}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJ70DQQj2X/XaG2BMq6jy8",
   "collapsed_sections": [],
   "name": "blank__05_NLP_1_intro.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
